### data training ###
#
# all labeled photos have been uploaded to the link below on my google drive.
# after the data training finishes, the best weights model will be downloaded in order to use it
# with the local machine environment that allows for live cam capture.


from ultralytics import yolo


data_yaml = '/content/drive/mydrive/smartcamassistant/data.yaml'

model = yolo('yolov5s.pt')  # or yolov5n.pt for a tiny model
model.train(data=data_yaml, epochs=30, imgsz=640)



# download the best weights
from google.colab import files
files.download('runs/detect/train/weights/best.pt')



import time
import cv2
import pyttsx3
import numpy as np
from ultralytics import yolo

# ─── config ─────────────────────────────────────────────────────────────────────
camera_index = 0       # 0 for the built-in cam, 1 is for the usb-cam
weights_path = "best.pt"

# ─── open camera ─────────────────────────────────────────────────────────────────
cap = cv2.videocapture(camera_index, cv2.cap_msmf)
if not cap.isopened():
    cap = cv2.videocapture(camera_index, cv2.cap_dshow)
if not cap.isopened():
    raise runtimeerror(f"cannot open camera at index {camera_index}")

cap.set(cv2.cap_prop_frame_width, 1280)
cap.set(cv2.cap_prop_frame_height, 720)

# ─── load model ───────────────────────────────────────────────────────────────────
model = yolo(weights_path)

# ─── text-to-speech setup ─────────────────────────────────────────────────────────
tts = pyttsx3.init()
tts.setproperty("rate", tts.getproperty("rate") - 50)

# state variables for delayed announcement
_pending_color = none
_pending_time  = 0
_last_announced = none
delay_seconds   = 2.0

def announce(color):
    tts.say({
        'red':    "it is a red light, you need to stop.",
        'yellow': "slow down, it is a yellow light.",
        'green':  "it is a green light, keep driving."
    }[color])
    tts.runandwait()


# ─── main loop ───────────────────────────────────────────────────────────────────
try:
    while true:
        ret, frame = cap.read()
        if not ret:
            break

        # 1) inference
        results = model(frame)[0]

        # 2) find detected colors
        colors = set()
        for det in results.boxes.data.cpu().numpy():
            x1, y1, x2, y2, conf, cls = det
            x1, y1, x2, y2 = map(int, (x1, y1, x2, y2))
            color = model.names[int(cls)]
            colors.add(color)

            # draw
            cv2.rectangle(frame, (x1, y1), (x2, y2), (255,255,255), 2)
            cv2.puttext(frame, color, (x1, y1 - 10),
                        cv2.font_hershey_simplex, 0.9, (255,255,255), 2)

        # 3) determine highest-priority color (or none)
        new_color = none
        if "red"    in colors: new_color = "red"
        elif "yellow" in colors: new_color = "yellow"
        elif "green"  in colors: new_color = "green"

        # 4) schedule a delayed announcement if it changed
        if new_color is not none and new_color != _last_announced:
            if new_color != _pending_color:
                _pending_color = new_color
                _pending_time  = time.time()

        # 5) check if it's time to speak
        if _pending_color is not none:
            if time.time() - _pending_time >= delay_seconds:
                announce(_pending_color)
                _pending_color = none

        # 6) show the frame
        cv2.imshow("smartcam assistant", frame)
        if cv2.waitkey(1) & 0xff == ord("q"):
            break

finally:
    cap.release()
    cv2.destroyallwindows()



