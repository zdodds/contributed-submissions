# Welcome to cs35, hw1pr0 ! #### Homework 1, problem 0 is a "reading-and-response problem"
+ There is usually one of these each week
+ It often reflects on the programming portion of that week...
+ **Response**    We ask you to compose a response to each article. It can be short (4-5 sentences is great), and the goal is to engage with the article, as well as to incorporate your own thoughts and experiences into your response. 
  + Include your response in the cell at the bottom of this notebook
  + Then, submit this notebook `hw1pr0.ipynb` to the submission site

<br>

+ To get familiar with Gradescope, feel free to submit this notebook, for the moment, _without_ the reading-response 
  + It's always possible to re-submit a file to Gradescope.
  + Gradescope keeps all submitted versions; the graders see the last version submitted

<br>

<hr>### The reading

Similar in spirit to cs5's reading/response assignments, each week there will be a short article overlapping that week's topics (sometimes broadly, other times narrowly). 

This week has "1+" readings. 

<br>

The "1" of "1+" comes from a short Nature article that describes a "credit-blame asymmetry" caused by AI, i.e., LLMs.  That is, LLMs make it more difficult to feel that creditable work is being accomplished -- but they do not change the standards for blame.  This is timely, seeing as this class -- and so much else -- is about using and exploring AI.  First, the article:

+ The article:     [Generative AI entails a creditâ€“blame asymmetry](https://www.nature.com/articles/s42256-023-00653-1)  &nbsp;&nbsp;   [pdf](https://drive.google.com/file/d/1rgIBOjld0N6QZpkI6G4Vm4YZAFVyYQe_/view?usp=drive_link)  &nbsp;&nbsp;     <font size="-2">(Oxford's  <a href="https://www.ox.ac.uk/news/2023-05-05-tackling-ethical-dilemma-responsibility-large-language-models">overview</a>  of the article)</font>

<br>

The prompt below will ask if you sense the "asymmetry" the article claims. But, first, the "+" of "1+":

The "+" of "1+" is the first two sections, 1.1 and 1.2, of the [ACM's Code of Ethics and Professional Conduct](https://ethics.acm.org/). The ACM is the Association for Computing Machinery and  is the world's largest professional-computing group. In 2018, the ACM released "the Code," its broad ethical guide for all computing work:  

+ Read sections 1.1 and 1.2  of "[the Code](https://ethics.acm.org/)."  More, of course, if you'd like...

<br><hr><br>
#### The prompt

As each week, this "problem 0" asks you to compose a one-paragraph reflection (~5 sentences, give or take), with the goal of bridging your personal experiences with the ideas in the articles. 

This week's prompts:

<br>

(a) The Nature article - and its overview - suggests that society is "justified in holding persons accountable for deliberate or careless errors in generated text if they put such text to use in ways that negatively impact others"  [<font color="DodgerBlue">the "blame" side</font>] and also that society "might not think people deserve credit for text generated without much skill and effort, such as ChatGPT-generated exam papers" [<font color="DodgerBlue">the "credit" side</font>].  <font color="Coral">Do you agree with neither, either, or both of these two "sides" of the article's "credit-blame asymmetry"? In a sentence or two, elaborate how/why</font>

(b) The ACM Code of Ethics, especially sections 1.1 (<i>Contribute...</i>) and 1.2 (<i>Avoid harm</i>), mirror this credit-blame dichotomy.  With these principles in mind, <font color="Coral">do you feel that, in particular, AI systems are ethics-promoting? detracting? neutral? amplifying what's already there?</font>  What changes to norms or expectations would you like to see around AI systems, credit for creative work, and blame for misuse? 

<br>

Here, and in general, your response should incorporate ideas from the article(s), connecting them with your own thoughts, takes, and experiences. 

And - you can always add detours, if you'd like ...

<br><hr><br>#### The Response
(a) I believe in both sides of the credit blame asymmetry. I think this asymmetry comes from consideration of the wider context of LLM generated content, particularly as it pertains to impact scaling. The cases where LLM causes harm (widespread disinformation, IP theft, miseducation) often carry far more weight than when LLM contributes to good. 

(b) I think AI systems lean towards ethically detracting. All it really does is expedite processes that would otherwise be achievable, but take more time and effort to complete. In terms of harm, this means LLMs allow actions that harm to be completed much more easily, allowing harms to be propagated further and faster than before. As for actions that do good, it certainly may have benefits towards creating more "good" faster, like for example expediting novel research with large scale benefits, but making that process easier and removing the critical thinking behind the process seems like a fast pass to wider intellectual degeneration. It's disheartening to see peers be unable to write without LLM, among other necessary skills for making positive contribution to the world. This is my current standpoint, but it is definitely subject to change. I feel like people could've absolutely felt similarly to how I do towards AI at the advent of the internet, but as someone whose grown up with the internet it doesn't feel like I have less skills than the people who came before me, just that I'm allowed to do more with the expediting aid of the internet.

I do think rules should be established around AI systems. I wish I knew how best to establish these rules, and what rules to establish, but I do know that the growth of AI has led to a lot of harm in different avenues. One that I particularly am interested in is the use of AI in art. On one hand, AI has killed a lot of creative jobs (marketing, graphic design, etc.) and has been used to steal certain artist's styles to mass produce replicas of their work. On the other hand, AI creates a lot of interesting ideas about creation, what is visually possible, and it can aid artists in minor ways to allow them to be more prolific. How to limit AI to being purely a creative aid, and not a tool of replacement is a very tough problem though, and I'm not entirely sure how it would come about. I don't think there are any strict limitations developers can implement wihtin their programming without limiting the AI in ways that decrease its ability to be used in constructive ways. That leaves proper use of AI as an unenforceable matter of "honor code", which is easily bypassed by a bad actor. So I'm not sure...... Maybe someone smarter than me can figure this out.... Or someone who is getting paid to think about these issues all day....<br><br>
<hr>
<br>

Additional thoughts on LLMs: not necessary for cs35.

In fact, we will explore more about the technical workings of LLMs around weeks 8-9.

+ [LLMs don't have a coherent model of the world](https://www.lesswrong.com/posts/wkws2WgraeN8AYJjv/llms-don-t-have-a-coherent-model-of-the-world-what-it-means) &nbsp; Interesting...
+ [Human Immortality using LLMs](https://danielmiessler.com/p/human-immortality-using-llms) &nbsp; Is that word warranted? ...
+ [LLM Programs](https://mpost.io/llm-programs-the-new-path-to-fine-tuning-neural-models-in-complex-situations/) &nbsp; An overview article
+ [Large language model programs](https://arxiv.org/abs/2305.05364) &nbsp; An Arxiv article
