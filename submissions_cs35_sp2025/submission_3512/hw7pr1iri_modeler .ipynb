{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "###   hw7pr1iris_modeler \n",
    "\n",
    "Here, you'll epxlore iris clasification <font color=\"Coral\">and regression</font> via NNets\n",
    "\n",
    "<font color=\"Coral\"> _Regression_ </font> - estimating a floating-point value - is NNets' natural operation.\n",
    "\n",
    "<br>\n",
    "\n",
    "_Classification_ - estimating a discrete label - is done by creating discrete variables (we can't let the library treat a categorical variable as if it were continuous). <br> We'll start there, since it's a variation of our past workflows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# hw7pr1iris_modeler:  NNETS! \n",
    "#\n",
    "#   including _both_ clasification + regression for iris modeling\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries...\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in our data\n",
    "\n",
    "previously cleaned..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_cleaned.csv : file read into a pandas dataframe.\n"
     ]
    }
   ],
   "source": [
    "# let's read in our flower data...\n",
    "# \n",
    "# iris_cleaned.csv should be in this folder\n",
    "# \n",
    "filename = 'iris_cleaned.csv' # neighborhoods\n",
    "df_tidy = pd.read_csv(filename)      # encoding = \"utf-8\", \"latin1\"\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_tidy.shape is (141, 6)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 141 entries, 0 to 140\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   sepallen  141 non-null    float64\n",
      " 1   sepalwid  141 non-null    float64\n",
      " 2   petallen  141 non-null    float64\n",
      " 3   petalwid  141 non-null    float64\n",
      " 4   irisname  141 non-null    object \n",
      " 5   irisnum   141 non-null    int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 7.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisname</th>\n",
       "      <th>irisnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid   irisname  irisnum\n",
       "0         4.6       3.6       1.0       0.2     setosa        0\n",
       "1         4.3       3.0       1.1       0.1     setosa        0\n",
       "2         5.0       3.2       1.2       0.2     setosa        0\n",
       "3         5.8       4.0       1.2       0.2     setosa        0\n",
       "4         4.4       3.0       1.3       0.2     setosa        0\n",
       "..        ...       ...       ...       ...        ...      ...\n",
       "136       7.9       3.8       6.4       2.0  virginica        2\n",
       "137       7.6       3.0       6.6       2.1  virginica        2\n",
       "138       7.7       3.8       6.7       2.2  virginica        2\n",
       "139       7.7       2.8       6.7       2.0  virginica        2\n",
       "140       7.7       2.6       6.9       2.3  virginica        2\n",
       "\n",
       "[141 rows x 6 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# different version vary on how to see all rows (adapt to suit your system!)\n",
    "#\n",
    "print(f\"df_tidy.shape is {df_tidy.shape}\\n\")\n",
    "df_tidy.info()  # prints column information\n",
    "df_tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>irisnum</th>\n",
       "      <th>is_setosa</th>\n",
       "      <th>is_versicolor</th>\n",
       "      <th>is_virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid  irisnum  is_setosa  \\\n",
       "0         4.6       3.6       1.0       0.2        0       True   \n",
       "1         4.3       3.0       1.1       0.1        0       True   \n",
       "2         5.0       3.2       1.2       0.2        0       True   \n",
       "3         5.8       4.0       1.2       0.2        0       True   \n",
       "4         4.4       3.0       1.3       0.2        0       True   \n",
       "..        ...       ...       ...       ...      ...        ...   \n",
       "136       7.9       3.8       6.4       2.0        2      False   \n",
       "137       7.6       3.0       6.6       2.1        2      False   \n",
       "138       7.7       3.8       6.7       2.2        2      False   \n",
       "139       7.7       2.8       6.7       2.0        2      False   \n",
       "140       7.7       2.6       6.9       2.3        2      False   \n",
       "\n",
       "     is_versicolor  is_virginica  \n",
       "0            False         False  \n",
       "1            False         False  \n",
       "2            False         False  \n",
       "3            False         False  \n",
       "4            False         False  \n",
       "..             ...           ...  \n",
       "136          False          True  \n",
       "137          False          True  \n",
       "138          False          True  \n",
       "139          False          True  \n",
       "140          False          True  \n",
       "\n",
       "[141 rows x 8 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# we need to make sure the categories are handed as categories!\n",
    "#    scikit-learn calls these \"dummy\" variables:\n",
    "\n",
    "df_tidy_cat = pd.get_dummies(data=df_tidy,prefix=\"is\",columns=['irisname'])\n",
    "df_tidy_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallen</th>\n",
       "      <th>sepalwid</th>\n",
       "      <th>petallen</th>\n",
       "      <th>petalwid</th>\n",
       "      <th>is_setosa</th>\n",
       "      <th>is_versicolor</th>\n",
       "      <th>is_virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallen  sepalwid  petallen  petalwid  is_setosa  is_versicolor  \\\n",
       "0         4.6       3.6       1.0       0.2       True          False   \n",
       "1         4.3       3.0       1.1       0.1       True          False   \n",
       "2         5.0       3.2       1.2       0.2       True          False   \n",
       "3         5.8       4.0       1.2       0.2       True          False   \n",
       "4         4.4       3.0       1.3       0.2       True          False   \n",
       "..        ...       ...       ...       ...        ...            ...   \n",
       "136       7.9       3.8       6.4       2.0      False          False   \n",
       "137       7.6       3.0       6.6       2.1      False          False   \n",
       "138       7.7       3.8       6.7       2.2      False          False   \n",
       "139       7.7       2.8       6.7       2.0      False          False   \n",
       "140       7.7       2.6       6.9       2.3      False          False   \n",
       "\n",
       "     is_virginica  \n",
       "0           False  \n",
       "1           False  \n",
       "2           False  \n",
       "3           False  \n",
       "4           False  \n",
       "..            ...  \n",
       "136          True  \n",
       "137          True  \n",
       "138          True  \n",
       "139          True  \n",
       "140          True  \n",
       "\n",
       "[141 rows x 7 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# We've removed the irisname column, and with our \"\" \n",
    "# we don't need the irisnum of the columns need to be numeric, we'll drop irisname\n",
    "ROW = 0\n",
    "COLUMN = 1\n",
    "df_model1 = df_tidy_cat.drop('irisnum', axis=COLUMN )\n",
    "df_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS is Index(['sepallen', 'sepalwid', 'petallen', 'petalwid', 'is_setosa',\n",
      "       'is_versicolor', 'is_virginica'],\n",
      "      dtype='object')\n",
      "\n",
      "COLUMNS[0] is sepallen\n",
      "\n",
      "COL_INDEX is {'sepallen': 0, 'sepalwid': 1, 'petallen': 2, 'petalwid': 3, 'is_setosa': 4, 'is_versicolor': 5, 'is_virginica': 6}\n",
      "\n",
      "\n",
      "setosa maps to 0\n",
      "versicolor maps to 1\n",
      "virginica maps to 2\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# once we have all the columns we want, let's create an index of their names...\n",
    "\n",
    "#\n",
    "# Let's make sure we have all of our helpful variables in one place \n",
    "#       To be adapted if we drop/add more columns...\n",
    "#\n",
    "\n",
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df_model1.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\\n\\n\")\n",
    "\n",
    "\n",
    "#\n",
    "# and our \"species\" names\n",
    "#\n",
    "\n",
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['setosa','versicolor','virginica']   # int to str\n",
    "SPECIES_INDEX = {'setosa':0,'versicolor':1,'virginica':2}  # str to int\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {SPECIES_INDEX[name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We _could_ reweight our columns...  DON'T DO THIS!\n",
    "# For example, if petalwid were \"worth\" 20x more than the others?\n",
    "# \n",
    "\n",
    "# df_model1['petalwid'] *= 20\n",
    "# df_model1\n",
    "\n",
    "#\n",
    "# But, with NNets, the whole goal of the network is to _adaptively_ weight each feature...\n",
    "#\n",
    "#      That's really all the network is doing... !?!!\n",
    "#      Let's see it in action:\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.6 1.  0.2 1.  0.  0. ]\n",
      " [4.3 3.  1.1 0.1 1.  0.  0. ]\n",
      " [5.  3.2 1.2 0.2 1.  0.  0. ]\n",
      " [5.8 4.  1.2 0.2 1.  0.  0. ]\n",
      " [4.4 3.  1.3 0.2 1.  0.  0. ]\n",
      " [4.4 3.2 1.3 0.2 1.  0.  0. ]\n",
      " [4.5 2.3 1.3 0.3 1.  0.  0. ]\n",
      " [4.7 3.2 1.3 0.2 1.  0.  0. ]\n",
      " [5.  3.5 1.3 0.3 1.  0.  0. ]\n",
      " [5.4 3.9 1.3 0.4 1.  0.  0. ]\n",
      " [5.5 3.5 1.3 0.2 1.  0.  0. ]\n",
      " [4.4 2.9 1.4 0.2 1.  0.  0. ]\n",
      " [4.6 3.4 1.4 0.3 1.  0.  0. ]\n",
      " [4.6 3.2 1.4 0.2 1.  0.  0. ]\n",
      " [4.8 3.  1.4 0.1 1.  0.  0. ]\n",
      " [4.8 3.  1.4 0.3 1.  0.  0. ]\n",
      " [4.9 3.  1.4 0.2 1.  0.  0. ]\n",
      " [5.  3.6 1.4 0.2 1.  0.  0. ]\n",
      " [5.  3.3 1.4 0.2 1.  0.  0. ]\n",
      " [5.1 3.5 1.4 0.2 1.  0.  0. ]\n",
      " [5.1 3.5 1.4 0.3 1.  0.  0. ]\n",
      " [5.2 3.4 1.4 0.2 1.  0.  0. ]\n",
      " [5.5 4.2 1.4 0.2 1.  0.  0. ]\n",
      " [4.6 3.1 1.5 0.2 1.  0.  0. ]\n",
      " [4.9 3.1 1.5 0.1 1.  0.  0. ]\n",
      " [4.9 3.1 1.5 0.1 1.  0.  0. ]\n",
      " [4.9 3.1 1.5 0.1 1.  0.  0. ]\n",
      " [5.  3.4 1.5 0.2 1.  0.  0. ]\n",
      " [5.1 3.8 1.5 0.3 1.  0.  0. ]\n",
      " [5.1 3.7 1.5 0.4 1.  0.  0. ]\n",
      " [5.1 3.4 1.5 0.2 1.  0.  0. ]\n",
      " [5.2 3.5 1.5 0.2 1.  0.  0. ]\n",
      " [5.3 3.7 1.5 0.2 1.  0.  0. ]\n",
      " [5.4 3.7 1.5 0.2 1.  0.  0. ]\n",
      " [5.7 4.4 1.5 0.4 1.  0.  0. ]\n",
      " [4.7 3.2 1.6 0.2 1.  0.  0. ]\n",
      " [4.8 3.4 1.6 0.2 1.  0.  0. ]\n",
      " [5.  3.  1.6 0.2 1.  0.  0. ]\n",
      " [5.  3.4 1.6 0.4 1.  0.  0. ]\n",
      " [5.  3.5 1.6 0.6 1.  0.  0. ]\n",
      " [5.1 3.8 1.6 0.2 1.  0.  0. ]\n",
      " [5.1 3.3 1.7 0.5 1.  0.  0. ]\n",
      " [5.4 3.9 1.7 0.4 1.  0.  0. ]\n",
      " [5.4 3.4 1.7 0.2 1.  0.  0. ]\n",
      " [5.7 3.8 1.7 0.3 1.  0.  0. ]\n",
      " [4.8 3.4 1.9 0.2 1.  0.  0. ]\n",
      " [5.1 3.8 1.9 0.4 1.  0.  0. ]\n",
      " [7.  3.2 4.7 1.4 0.  1.  0. ]\n",
      " [4.9 2.4 3.3 1.  0.  1.  0. ]\n",
      " [5.  2.3 3.3 1.  0.  1.  0. ]\n",
      " [5.  2.  3.5 1.  0.  1.  0. ]\n",
      " [5.7 2.6 3.5 1.  0.  1.  0. ]\n",
      " [5.6 2.9 3.6 1.3 0.  1.  0. ]\n",
      " [5.5 2.4 3.7 1.  0.  1.  0. ]\n",
      " [5.5 2.4 3.8 1.1 0.  1.  0. ]\n",
      " [5.2 2.7 3.9 1.4 0.  1.  0. ]\n",
      " [5.6 2.5 3.9 1.1 0.  1.  0. ]\n",
      " [5.8 2.7 3.9 1.2 0.  1.  0. ]\n",
      " [5.5 2.3 4.  1.3 0.  1.  0. ]\n",
      " [5.5 2.5 4.  1.3 0.  1.  0. ]\n",
      " [5.8 2.6 4.  1.2 0.  1.  0. ]\n",
      " [6.  2.2 4.  1.  0.  1.  0. ]\n",
      " [6.1 2.8 4.  1.3 0.  1.  0. ]\n",
      " [5.6 3.  4.1 1.3 0.  1.  0. ]\n",
      " [5.8 2.7 4.1 1.  0.  1.  0. ]\n",
      " [5.6 2.7 4.2 1.3 0.  1.  0. ]\n",
      " [5.7 3.  4.2 1.2 0.  1.  0. ]\n",
      " [5.9 3.  4.2 1.5 0.  1.  0. ]\n",
      " [6.4 2.9 4.3 1.3 0.  1.  0. ]\n",
      " [5.5 2.6 4.4 1.2 0.  1.  0. ]\n",
      " [6.3 2.3 4.4 1.3 0.  1.  0. ]\n",
      " [6.6 3.  4.4 1.4 0.  1.  0. ]\n",
      " [6.7 3.1 4.4 1.4 0.  1.  0. ]\n",
      " [5.4 3.  4.5 1.5 0.  1.  0. ]\n",
      " [5.6 3.  4.5 1.5 0.  1.  0. ]\n",
      " [5.7 2.8 4.5 1.3 0.  1.  0. ]\n",
      " [6.  2.9 4.5 1.5 0.  1.  0. ]\n",
      " [6.  3.4 4.5 1.6 0.  1.  0. ]\n",
      " [6.2 2.2 4.5 1.5 0.  1.  0. ]\n",
      " [6.4 3.2 4.5 1.5 0.  1.  0. ]\n",
      " [6.1 3.  4.6 1.4 0.  1.  0. ]\n",
      " [6.5 2.8 4.6 1.5 0.  1.  0. ]\n",
      " [6.6 2.9 4.6 1.3 0.  1.  0. ]\n",
      " [6.1 2.9 4.7 1.4 0.  1.  0. ]\n",
      " [6.1 2.8 4.7 1.2 0.  1.  0. ]\n",
      " [6.3 3.3 4.7 1.6 0.  1.  0. ]\n",
      " [6.7 3.1 4.7 1.5 0.  1.  0. ]\n",
      " [5.9 3.2 4.8 1.8 0.  1.  0. ]\n",
      " [6.8 2.8 4.8 1.4 0.  1.  0. ]\n",
      " [6.3 2.5 4.9 1.5 0.  1.  0. ]\n",
      " [6.9 3.1 4.9 1.5 0.  1.  0. ]\n",
      " [6.7 3.  5.  1.7 0.  1.  0. ]\n",
      " [6.  2.7 5.1 1.6 0.  1.  0. ]\n",
      " [4.9 2.5 4.5 1.7 0.  0.  1. ]\n",
      " [6.  3.  4.8 1.8 0.  0.  1. ]\n",
      " [6.2 2.8 4.8 1.8 0.  0.  1. ]\n",
      " [5.6 2.8 4.9 2.  0.  0.  1. ]\n",
      " [6.1 3.  4.9 1.8 0.  0.  1. ]\n",
      " [6.3 2.7 4.9 1.8 0.  0.  1. ]\n",
      " [5.7 2.5 5.  2.  0.  0.  1. ]\n",
      " [6.  2.2 5.  1.5 0.  0.  1. ]\n",
      " [6.3 2.5 5.  1.9 0.  0.  1. ]\n",
      " [5.8 2.8 5.1 2.4 0.  0.  1. ]\n",
      " [5.8 2.7 5.1 1.9 0.  0.  1. ]\n",
      " [5.9 3.  5.1 1.8 0.  0.  1. ]\n",
      " [6.3 2.8 5.1 1.5 0.  0.  1. ]\n",
      " [6.5 3.2 5.1 2.  0.  0.  1. ]\n",
      " [6.9 3.1 5.1 2.3 0.  0.  1. ]\n",
      " [6.5 3.  5.2 2.  0.  0.  1. ]\n",
      " [6.7 3.  5.2 2.3 0.  0.  1. ]\n",
      " [6.4 2.7 5.3 1.9 0.  0.  1. ]\n",
      " [6.4 3.2 5.3 2.3 0.  0.  1. ]\n",
      " [6.2 3.4 5.4 2.3 0.  0.  1. ]\n",
      " [6.9 3.1 5.4 2.1 0.  0.  1. ]\n",
      " [6.4 3.1 5.5 1.8 0.  0.  1. ]\n",
      " [6.5 3.  5.5 1.8 0.  0.  1. ]\n",
      " [6.8 3.  5.5 2.1 0.  0.  1. ]\n",
      " [6.1 2.6 5.6 1.4 0.  0.  1. ]\n",
      " [6.3 2.9 5.6 1.8 0.  0.  1. ]\n",
      " [6.3 3.4 5.6 2.4 0.  0.  1. ]\n",
      " [6.4 2.8 5.6 2.1 0.  0.  1. ]\n",
      " [6.4 2.8 5.6 2.2 0.  0.  1. ]\n",
      " [6.7 3.1 5.6 2.4 0.  0.  1. ]\n",
      " [6.7 3.3 5.7 2.1 0.  0.  1. ]\n",
      " [6.7 3.3 5.7 2.5 0.  0.  1. ]\n",
      " [6.9 3.2 5.7 2.3 0.  0.  1. ]\n",
      " [6.5 3.  5.8 2.2 0.  0.  1. ]\n",
      " [6.7 2.5 5.8 1.8 0.  0.  1. ]\n",
      " [7.2 3.  5.8 1.6 0.  0.  1. ]\n",
      " [6.8 3.2 5.9 2.3 0.  0.  1. ]\n",
      " [7.1 3.  5.9 2.1 0.  0.  1. ]\n",
      " [7.2 3.2 6.  1.8 0.  0.  1. ]\n",
      " [7.2 3.6 6.1 2.5 0.  0.  1. ]\n",
      " [7.4 2.8 6.1 1.9 0.  0.  1. ]\n",
      " [7.7 3.  6.1 2.3 0.  0.  1. ]\n",
      " [7.3 2.9 6.3 1.8 0.  0.  1. ]\n",
      " [7.9 3.8 6.4 2.  0.  0.  1. ]\n",
      " [7.6 3.  6.6 2.1 0.  0.  1. ]\n",
      " [7.7 3.8 6.7 2.2 0.  0.  1. ]\n",
      " [7.7 2.8 6.7 2.  0.  0.  1. ]\n",
      " [7.7 2.6 6.9 2.3 0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#\n",
    "A = df_model1.to_numpy()   \n",
    "A = A.astype('float64')    # many types:  www.tutorialspoint.com/numpy/numpy_data_types.htm\n",
    "print(A[:,:])\n",
    "\n",
    "# print(A)  # the five rows above is probably enough...   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset has 141 rows and 7 cols\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# nice to have NUM_ROWS and NUM_COLS around\n",
    "#\n",
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have full access to individual data elements\n",
    "\n",
    "Checking on our observations and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower #42 is [5.4 3.9 1.7 0.4 1.  0.  0. ]\n",
      "  Its sepallen is 5.4\n",
      "  Its sepalwid is 3.9\n",
      "  Its petallen is 1.7\n",
      "  Its petalwid is 0.4\n",
      "  Its is_setosa is 1.0\n",
      "  Its is_versicolor is 0.0\n",
      "  Its is_virginica is 0.0\n",
      "\n",
      "  and its species name: virginica\n"
     ]
    }
   ],
   "source": [
    "# let's use all of our variables, to reinforce that we have\n",
    "# (1) names...\n",
    "# (2) access and control...\n",
    "\n",
    "# choose a row index, n:\n",
    "n = 42\n",
    "print(f\"flower #{n} is {A[n]}\")\n",
    "species = \"\"\n",
    "\n",
    "for i in range(len(COLUMNS)):\n",
    "    colname = COLUMNS[i]\n",
    "    value = A[n][i]\n",
    "    print(f\"  Its {colname} is {value}\")\n",
    "    if \"setosa\" in colname: species = \"setosa\"\n",
    "    elif \"versicolor\" in colname: species = \"versicolor\"\n",
    "    elif \"virginica\" in colname: species = \"virginica\"\n",
    "\n",
    "print()\n",
    "print(f\"  and its species name: {species}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining X_all (our features) and y_all (our target to predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data definitions +++\n",
      "\n",
      "y_all (just the labels/species, first few rows) are \n",
      " [[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "X_all (just the features, first few rows) are \n",
      " [[4.6 3.6 1.  0.2]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [4.4 3.  1.3 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of data definitions +++\\n\")\n",
    "\n",
    "#\n",
    "# we could do this at the data-frame level, too!\n",
    "#\n",
    "\n",
    "X_all = A[:,0:4]   # X (features) ... is all rows, columns 0, 1, 2, 3\n",
    "y_all = A[:,4:]    # y (labels) ... is all rows, columns 4, 5, and 6\n",
    "\n",
    "print(f\"y_all (just the labels/species, first few rows) are \\n {y_all[0:5]}\")\n",
    "print()\n",
    "print(f\"X_all (just the features, first few rows) are \\n {X_all[0:5]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scrambled labels/species are \n",
      " [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "The corresponding data rows are \n",
      " [[4.9 3.  1.4 0.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.7 3.  5.2 2.3]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we can scramble the data, to remove (potential) dependence on its ordering: \n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(f\"The scrambled labels/species are \\n {y_all[0:5]}\")\n",
    "print()\n",
    "print(f\"The corresponding data rows are \\n {X_all[0:5]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate into training and testing\n",
    "+ X_train, y_train will be used for model-building\n",
    "+ X_test, y_test will be used for testing, later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 112 rows;  testing with 29 rows\n",
      "\n",
      "+++ Testing +++   Held-out data... (testing data: 29)\n",
      "\n",
      "y_test: [[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "X_test (few rows): [[6.2 2.2 4.5 1.5]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [7.6 3.  6.6 2.1]]\n",
      "\n",
      "\n",
      "+++ Training +++   Data used for modeling... (training data: 112)\n",
      "\n",
      "y_train: [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "X_train (few rows): [[4.8 3.  1.4 0.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [7.2 3.2 6.  1.8]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"+++ Testing +++   Held-out data... (testing data: {len(y_test)})\\n\")\n",
    "print(f\"y_test: {y_test[0:5,:]}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print(\"\\n\")\n",
    "print(f\"+++ Training +++   Data used for modeling... (training data: {len(y_train)})\\n\")\n",
    "print(f\"y_train: {y_train[0:5,:]}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNets often need a ***Scaler***\n",
    "\n",
    "This scales all of the data so that it's of a similar size, e.g., -1 to 1\n",
    "\n",
    "Notice that this cell produces\n",
    "+ X_train_scaled &nbsp; (the scaled training data)\n",
    "   + y_train_scaled is the same as y_train &nbsp; (no scaling needed for the target)\n",
    "+ X_test_scaled &nbsp; (the scaled testing data)\n",
    "   + y_test_scaled is the same as y_test &nbsp; (no scaling needed for the target)\n",
    "\n",
    "After this, we just use the scaled data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#\n",
    "# do we want to use a Scaler?\n",
    "#\n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train.copy()  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test.copy()  # not using the scaler\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition-building for the Scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  ->  pred   des  \n",
      "                                 [-1.24 -0.13 -1.33 -1.18] ->    ?    [1. 0. 0.]           \n",
      "                                 [ 0.19 -1.92  0.71  0.42] ->    ?    [0. 0. 1.]           \n",
      "                                 [-0.41 -1.02  0.37  0.02] ->    ?    [0. 1. 0.]           \n",
      "                                 [-1.    0.76 -1.27 -1.32] ->    ?    [1. 0. 0.]           \n",
      "                                     [1.62 0.32 1.28 0.82] ->    ?    [0. 0. 1.]           \n",
      "\n",
      "                                                    input  ->  pred   des  \n",
      "                                         [4.8 3.  1.4 0.3] ->    ?    [1. 0. 0.]           \n",
      "                                         [6.  2.2 5.  1.5] ->    ?    [0. 0. 1.]           \n",
      "                                         [5.5 2.6 4.4 1.2] ->    ?    [0. 1. 0.]           \n",
      "                                         [5.  3.4 1.5 0.2] ->    ?    [1. 0. 0.]           \n",
      "                                         [7.2 3.2 6.  1.8] ->    ?    [0. 0. 1.]           \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's create a table for showing our data and its predictions...\n",
    "#\n",
    "def ascii_table(X,y,scaler_to_invert=None):\n",
    "    \"\"\" print a table of inputs and outputs \"\"\"\n",
    "    np.set_printoptions(precision=2)  # Let's use less precision\n",
    "    if scaler_to_invert == None:  # don't use the scaler\n",
    "        X = X\n",
    "    else:\n",
    "        X = scaler_to_invert.inverse_transform(X)\n",
    "    print(f\"{'input ':>58s} -> {'pred':^7s} {'des':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        # whoa! serious f-string formatting:\n",
    "        print(f\"{str(X[i,0:4]):>58s} -> {'?':^7s} {str(y[i]):<21s}\")   # !s is str ...\n",
    "    print()\n",
    "    \n",
    "# to show the table with the scaled data:\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],None)\n",
    "\n",
    "# to show the table with the original data:\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],scaler_to_invert=scaler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ML models is no problem\n",
    "\n",
    "that is, if the library does it for us! &nbsp;&nbsp; ðŸ˜ƒ ðŸ§¶ â›„ï¸ ðŸŒŠ ðŸªº ðŸ¦” :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 2.27041274\n",
      "Iteration 2, loss = 1.92073327\n",
      "Iteration 3, loss = 1.80970034\n",
      "Iteration 4, loss = 1.70841495\n",
      "Iteration 5, loss = 1.49959573\n",
      "Iteration 6, loss = 1.21454963\n",
      "Iteration 7, loss = 0.91652817\n",
      "Iteration 8, loss = 0.72200035\n",
      "Iteration 9, loss = 0.67981873\n",
      "Iteration 10, loss = 0.67070645\n",
      "Iteration 11, loss = 0.60001402\n",
      "Iteration 12, loss = 0.49955865\n",
      "Iteration 13, loss = 0.41958199\n",
      "Iteration 14, loss = 0.38064682\n",
      "Iteration 15, loss = 0.35349545\n",
      "Iteration 16, loss = 0.32620388\n",
      "Iteration 17, loss = 0.29492476\n",
      "Iteration 18, loss = 0.25831591\n",
      "Iteration 19, loss = 0.21136983\n",
      "Iteration 20, loss = 0.16183592\n",
      "Iteration 21, loss = 0.12729065\n",
      "Iteration 22, loss = 0.11460348\n",
      "Iteration 23, loss = 0.11112534\n",
      "Iteration 24, loss = 0.10346977\n",
      "Iteration 25, loss = 0.10110075\n",
      "Iteration 26, loss = 0.10056551\n",
      "Iteration 27, loss = 0.09549214\n",
      "Iteration 28, loss = 0.09249417\n",
      "Iteration 29, loss = 0.09124377\n",
      "Iteration 30, loss = 0.08488683\n",
      "Iteration 31, loss = 0.07990561\n",
      "Iteration 32, loss = 0.07606211\n",
      "Iteration 33, loss = 0.06994124\n",
      "Iteration 34, loss = 0.06740962\n",
      "Iteration 35, loss = 0.06660627\n",
      "Iteration 36, loss = 0.06502874\n",
      "Iteration 37, loss = 0.06640826\n",
      "Iteration 38, loss = 0.06600220\n",
      "Iteration 39, loss = 0.06438869\n",
      "Iteration 40, loss = 0.06315696\n",
      "Iteration 41, loss = 0.05945936\n",
      "Iteration 42, loss = 0.05713886\n",
      "Iteration 43, loss = 0.05484637\n",
      "Iteration 44, loss = 0.05324093\n",
      "Iteration 45, loss = 0.05293408\n",
      "Iteration 46, loss = 0.05163591\n",
      "Iteration 47, loss = 0.05102082\n",
      "Iteration 48, loss = 0.04919339\n",
      "Iteration 49, loss = 0.04779147\n",
      "Iteration 50, loss = 0.04645408\n",
      "Iteration 51, loss = 0.04534799\n",
      "Iteration 52, loss = 0.04474769\n",
      "Iteration 53, loss = 0.04360140\n",
      "Iteration 54, loss = 0.04269385\n",
      "Iteration 55, loss = 0.04110527\n",
      "Iteration 56, loss = 0.04013539\n",
      "Iteration 57, loss = 0.03902344\n",
      "Iteration 58, loss = 0.03862844\n",
      "Iteration 59, loss = 0.03788022\n",
      "Iteration 60, loss = 0.03731011\n",
      "Iteration 61, loss = 0.03626779\n",
      "Iteration 62, loss = 0.03555698\n",
      "Iteration 63, loss = 0.03478971\n",
      "Iteration 64, loss = 0.03437413\n",
      "Iteration 65, loss = 0.03385004\n",
      "Iteration 66, loss = 0.03322266\n",
      "Iteration 67, loss = 0.03267990\n",
      "Iteration 68, loss = 0.03212083\n",
      "Iteration 69, loss = 0.03187470\n",
      "Iteration 70, loss = 0.03149269\n",
      "Iteration 71, loss = 0.03107603\n",
      "Iteration 72, loss = 0.03061893\n",
      "Iteration 73, loss = 0.03013322\n",
      "Iteration 74, loss = 0.02982920\n",
      "Iteration 75, loss = 0.02944764\n",
      "Iteration 76, loss = 0.02898267\n",
      "Iteration 77, loss = 0.02857631\n",
      "Iteration 78, loss = 0.02817704\n",
      "Iteration 79, loss = 0.02783227\n",
      "Iteration 80, loss = 0.02748924\n",
      "Iteration 81, loss = 0.02706034\n",
      "Iteration 82, loss = 0.02666200\n",
      "Iteration 83, loss = 0.02633870\n",
      "Iteration 84, loss = 0.02597208\n",
      "Iteration 85, loss = 0.02554473\n",
      "Iteration 86, loss = 0.02515346\n",
      "Iteration 87, loss = 0.02480489\n",
      "Iteration 88, loss = 0.02442888\n",
      "Iteration 89, loss = 0.02402484\n",
      "Iteration 90, loss = 0.02364886\n",
      "Iteration 91, loss = 0.02330861\n",
      "Iteration 92, loss = 0.02295696\n",
      "Iteration 93, loss = 0.02258002\n",
      "Iteration 94, loss = 0.02221916\n",
      "Iteration 95, loss = 0.02188571\n",
      "Iteration 96, loss = 0.02155155\n",
      "Iteration 97, loss = 0.02121251\n",
      "Iteration 98, loss = 0.02088740\n",
      "Iteration 99, loss = 0.02057364\n",
      "Iteration 100, loss = 0.02025406\n",
      "Iteration 101, loss = 0.01994475\n",
      "Iteration 102, loss = 0.01966235\n",
      "Iteration 103, loss = 0.01941366\n",
      "Iteration 104, loss = 0.01923322\n",
      "Iteration 105, loss = 0.01926580\n",
      "Iteration 106, loss = 0.01983482\n",
      "Iteration 107, loss = 0.02169558\n",
      "Iteration 108, loss = 0.02456960\n",
      "Iteration 109, loss = 0.02555623\n",
      "Iteration 110, loss = 0.01976599\n",
      "Iteration 111, loss = 0.01769748\n",
      "Iteration 112, loss = 0.02167716\n",
      "Iteration 113, loss = 0.02052120\n",
      "Iteration 114, loss = 0.01674181\n",
      "Iteration 115, loss = 0.01768527\n",
      "Iteration 116, loss = 0.01889649\n",
      "Iteration 117, loss = 0.01647934\n",
      "Iteration 118, loss = 0.01631393\n",
      "Iteration 119, loss = 0.01767344\n",
      "Iteration 120, loss = 0.01625359\n",
      "Iteration 121, loss = 0.01520392\n",
      "Iteration 122, loss = 0.01641886\n",
      "Iteration 123, loss = 0.01589921\n",
      "Iteration 124, loss = 0.01456448\n",
      "Iteration 125, loss = 0.01536036\n",
      "Iteration 126, loss = 0.01532355\n",
      "Iteration 127, loss = 0.01405057\n",
      "Iteration 128, loss = 0.01431809\n",
      "Iteration 129, loss = 0.01471652\n",
      "Iteration 130, loss = 0.01372227\n",
      "Iteration 131, loss = 0.01343666\n",
      "Iteration 132, loss = 0.01388182\n",
      "Iteration 133, loss = 0.01332321\n",
      "Iteration 134, loss = 0.01277545\n",
      "Iteration 135, loss = 0.01301469\n",
      "Iteration 136, loss = 0.01290533\n",
      "Iteration 137, loss = 0.01234454\n",
      "Iteration 138, loss = 0.01220951\n",
      "Iteration 139, loss = 0.01231186\n",
      "Iteration 140, loss = 0.01201178\n",
      "Iteration 141, loss = 0.01163604\n",
      "Iteration 142, loss = 0.01162965\n",
      "Iteration 143, loss = 0.01161154\n",
      "Iteration 144, loss = 0.01128448\n",
      "Iteration 145, loss = 0.01102717\n",
      "Iteration 146, loss = 0.01100718\n",
      "Iteration 147, loss = 0.01091369\n",
      "Iteration 148, loss = 0.01065111\n",
      "Iteration 149, loss = 0.01043838\n",
      "Iteration 150, loss = 0.01037652\n",
      "Iteration 151, loss = 0.01029186\n",
      "Iteration 152, loss = 0.01007996\n",
      "Iteration 153, loss = 0.00988027\n",
      "Iteration 154, loss = 0.00978499\n",
      "Iteration 155, loss = 0.00970440\n",
      "Iteration 156, loss = 0.00955160\n",
      "Iteration 157, loss = 0.00936320\n",
      "Iteration 158, loss = 0.00922842\n",
      "Iteration 159, loss = 0.00914116\n",
      "Iteration 160, loss = 0.00903117\n",
      "Iteration 161, loss = 0.00888298\n",
      "Iteration 162, loss = 0.00873607\n",
      "Iteration 163, loss = 0.00860818\n",
      "Iteration 164, loss = 0.00850313\n",
      "Iteration 165, loss = 0.00839827\n",
      "Iteration 166, loss = 0.00827846\n",
      "Iteration 167, loss = 0.00815142\n",
      "Iteration 168, loss = 0.00803480\n",
      "Iteration 169, loss = 0.00793164\n",
      "Iteration 170, loss = 0.00783085\n",
      "Iteration 171, loss = 0.00772320\n",
      "Iteration 172, loss = 0.00760991\n",
      "Iteration 173, loss = 0.00750073\n",
      "Iteration 174, loss = 0.00740041\n",
      "Iteration 175, loss = 0.00730482\n",
      "Iteration 176, loss = 0.00720757\n",
      "Iteration 177, loss = 0.00710654\n",
      "Iteration 178, loss = 0.00700572\n",
      "Iteration 179, loss = 0.00690933\n",
      "Iteration 180, loss = 0.00681801\n",
      "Iteration 181, loss = 0.00672862\n",
      "Iteration 182, loss = 0.00663795\n",
      "Iteration 183, loss = 0.00654635\n",
      "Iteration 184, loss = 0.00645615\n",
      "Iteration 185, loss = 0.00636932\n",
      "Iteration 186, loss = 0.00628549\n",
      "Iteration 187, loss = 0.00620273\n",
      "Iteration 188, loss = 0.00611977\n",
      "Iteration 189, loss = 0.00603696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The analog prediction error (the loss) is 0.006036958071609447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of hidden layers\n",
    "# and number of neurons!  It's in the tuple  hidden_layer_sizes:\n",
    "#\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(6,7),  \n",
    "                    # hidden_layer_sizes=(6,7)   means   4 inputs -> 6 hidden -> 7 hidden -> 3 outputs\n",
    "                    max_iter=500,      # how many times to train\n",
    "                    # activation=\"tanh\", # the \"activation function\" input -> output\n",
    "                    # solver='sgd',      # the algorithm for optimizing weights\n",
    "                    verbose=True,      # False to \"mute\" the training\n",
    "                    shuffle=True,      # reshuffle the training epochs?\n",
    "                    random_state=None, # set for reproduceability\n",
    "                    learning_rate_init=.1,       # learning rate: the amt of error to backpropagate!\n",
    "                    learning_rate = 'adaptive')  # soften feedback as it converges\n",
    "\n",
    "# documentation:\n",
    "# scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "#     Try other network sizes / other parameters ...\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_classifier.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The analog prediction error (the loss) is {nn_classifier.loss_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      input  ->     pred         des.    \n",
      "           [6.2 2.2 4.5 1.5] ->  versicolor  versicolor     correct \n",
      "           [6.  2.7 5.1 1.6] ->  virginica   versicolor     incorrect: [8.69e-09 2.11e-08 1.00e+00]\n",
      "           [6.8 2.8 4.8 1.4] ->  versicolor  versicolor     correct \n",
      "           [6.3 3.3 4.7 1.6] ->  versicolor  versicolor     correct \n",
      "           [7.6 3.  6.6 2.1] ->  virginica   virginica      correct \n",
      "           [6.2 3.4 5.4 2.3] ->  virginica   virginica      correct \n",
      "           [6.5 3.  5.2 2. ] ->  virginica   virginica      correct \n",
      "           [6.4 3.2 5.3 2.3] ->  virginica   virginica      correct \n",
      "           [5.1 3.7 1.5 0.4] ->    setosa    setosa         correct \n",
      "           [5.5 2.4 3.8 1.1] ->  versicolor  versicolor     correct \n",
      "           [4.9 3.1 1.5 0.1] ->    setosa    setosa         correct \n",
      "           [5.5 2.4 3.7 1. ] ->  versicolor  versicolor     correct \n",
      "           [5.6 3.  4.5 1.5] ->  versicolor  versicolor     correct \n",
      "           [7.7 2.8 6.7 2. ] ->  virginica   virginica      correct \n",
      "           [5.6 2.7 4.2 1.3] ->  versicolor  versicolor     correct \n",
      "           [5.4 3.4 1.7 0.2] ->    setosa    setosa         correct \n",
      "           [5.  3.6 1.4 0.2] ->    setosa    setosa         correct \n",
      "           [6.7 3.1 5.6 2.4] ->  virginica   virginica      correct \n",
      "           [5.2 3.5 1.5 0.2] ->    setosa    setosa         correct \n",
      "           [5.6 2.9 3.6 1.3] ->  versicolor  versicolor     correct \n",
      "           [6.2 2.8 4.8 1.8] ->  virginica   virginica      correct \n",
      "           [4.9 3.1 1.5 0.1] ->    setosa    setosa         correct \n",
      "           [7.1 3.  5.9 2.1] ->  virginica   virginica      correct \n",
      "           [4.8 3.  1.4 0.1] ->    setosa    setosa         correct \n",
      "           [6.8 3.  5.5 2.1] ->  virginica   virginica      correct \n",
      "           [5.9 3.  4.2 1.5] ->  versicolor  versicolor     correct \n",
      "           [6.9 3.2 5.7 2.3] ->  virginica   virginica      correct \n",
      "           [4.4 2.9 1.4 0.2] ->    setosa    setosa         correct \n",
      "           [5.3 3.7 1.5 0.2] ->    setosa    setosa         correct \n",
      "\n",
      "correct predictions: 28 out of 29\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do on the testing data?\n",
    "#\n",
    "\n",
    "def get_species(A):\n",
    "    \"\"\" returns the species for A ~ [1 0 0] or [0 1 0] or ... \"\"\"\n",
    "    for i in range(len(SPECIES)):\n",
    "        if A[i] == 1: \n",
    "            return SPECIES[i]  # note that this \"takes the first one\"\n",
    "    return \"no species\" \n",
    "\n",
    "SEE_PROBS = False\n",
    "\n",
    "def ascii_table_for_classifier(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc)            # all predictions\n",
    "    prediction_probs = nn.predict_proba(Xsc) # all prediction probabilities\n",
    "    Xpr = scaler.inverse_transform(Xsc)      # Xpr is the \"X to print\": the unscaled data\n",
    "    # count correct\n",
    "    num_correct = 0\n",
    "    # printing\n",
    "    print(f\"{'input ':>28s} -> {'pred':^12s} {'des.':^12s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        pred_probs = str(prediction_probs[i,:])\n",
    "        desired = y[i].astype(int)\n",
    "        # print(pred, desired, pred_probs)\n",
    "        pred_species = get_species(pred)\n",
    "        des_species  = get_species(desired)\n",
    "        if pred_species != des_species: result = \"  incorrect: \" + pred_probs\n",
    "        else: result = \"  correct\" + (\": \"+pred_probs if SEE_PROBS else \"\") ; num_correct += 1\n",
    "        # Xpr = Xsc  # if you want to see the scaled versions\n",
    "        print(f\"{Xpr[i,0:4]!s:>28s} -> {pred_species:^12s} {des_species:12s} {result:^10s}\") \n",
    "    print(f\"\\ncorrect predictions: {num_correct} out of {len(y)}\")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_classifier(X_test_scaled,\n",
    "                           y_test_scaled,\n",
    "                           nn_classifier,\n",
    "                           scaler)   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the network's weights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[ Layer 0 ]]\n",
      "   has shape = (4, 6) and weights =\n",
      "[[-0.87 -0.78 -0.11 -0.18 -1.19 -0.52]\n",
      " [ 0.58 -0.4  -1.5  -0.44  0.68  0.45]\n",
      " [-1.3   2.39  0.37  3.43 -0.01 -1.84]\n",
      " [-1.21  1.78  1.25  1.08 -0.82 -1.85]]\n",
      "   with intercepts:\n",
      " [-0.37 -2.17  1.26 -2.47 -1.08  1.24]\n",
      "\n",
      "[[ Layer 1 ]]\n",
      "   has shape = (6, 7) and weights =\n",
      "[[-3.82e-01 -9.41e-01  1.87e+00 -8.81e-01 -1.56e+00  1.61e+00  4.26e-01]\n",
      " [-1.46e-05 -7.30e-01 -1.56e+00  1.39e+00 -3.18e+00 -1.18e+00  2.00e+00]\n",
      " [-1.51e-04  1.54e+00 -1.53e+00  9.72e-01  1.05e+00 -1.80e+00  1.43e-01]\n",
      " [-2.38e-01 -2.90e+00 -1.18e+00  1.82e+00 -4.53e+00 -3.07e-01  4.64e+00]\n",
      " [-1.63e-01  1.06e+00  6.45e-01  2.92e-01 -4.30e-02  1.69e-01  1.99e-01]\n",
      " [ 1.15e-04  1.28e-01  2.18e+00 -9.34e-01  6.12e-01  1.09e+00 -7.19e-01]]\n",
      "   with intercepts:\n",
      " [-1.13  0.06  0.64 -0.08  2.79 -0.4  -1.35]\n",
      "\n",
      "[[ Layer 2 ]]\n",
      "   has shape = (7, 3) and weights =\n",
      "[[-0.8   0.08  0.54]\n",
      " [-1.67  0.55 -0.23]\n",
      " [ 1.77 -1.07 -1.87]\n",
      " [-1.75 -0.29  0.29]\n",
      " [-0.63  3.12 -3.22]\n",
      " [ 1.15 -0.47 -0.68]\n",
      " [-0.63 -3.12  2.54]]\n",
      "   with intercepts:\n",
      " [-1.13  1.45 -0.63]\n",
      "\n",
      "\n",
      "\n",
      "all parameters: {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (6, 7), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 500, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': True, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We don't usually look inside the NNet, but we can: it's open-box modeling...\n",
    "#\n",
    "if True:  # do we want to see all of the parameters?\n",
    "    np.set_printoptions(precision=2)  # Let's use less precision\n",
    "    nn = nn_classifier  # less to type?\n",
    "    print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "    print(f\"\\nweights/coefficients:\\n\")\n",
    "    for i, wts in enumerate(nn.coefs_):\n",
    "        print(f\"[[ Layer {i} ]]\\n   has shape = {wts.shape} and weights =\\n{wts}\")\n",
    "        print(f\"   with intercepts:\\n {nn.intercepts_[i]}\\n\")\n",
    "    print()\n",
    "    print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict       setosa from the features [4.8, 3.1, 1.6, 0.2]  \n",
      "I predict   versicolor from the features [5.7, 2.9, 4.2, 1.3]  \n",
      "I predict    virginica from the features [5.8, 2.7, 5.1, 1.9]  \n",
      "I predict       setosa from the features [5.2, 4.1, 1.5, 0.1]  \n",
      "I predict       setosa from the features [5.4, 3.4, 1.5, 0.4]  \n",
      "I predict   versicolor from the features [5.1, 2.5, 3.0, 1.1]  \n",
      "I predict   versicolor from the features [6.2, 2.9, 4.3, 1.3]  \n",
      "I predict    virginica from the features [6.3, 3.3, 6.0, 2.5]  \n",
      "I predict   versicolor from the features [5.7, 2.8, 4.1, 1.3]  \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# final predictive model (random forests), with tuned parameters + ALL data incorporated\n",
    "#\n",
    "\n",
    "def get_species(A):\n",
    "    \"\"\" returns the species for A ~ [1 0 0] or [0 1 0] or ... \"\"\"\n",
    "    for i in range(len(SPECIES)):\n",
    "        if A[i] == 1: \n",
    "            return SPECIES[i]  # note that this \"takes the first one\"\n",
    "    return \"no species\" \n",
    "\n",
    "\n",
    "def predictive_model( Features, MODEL, SCALER ):\n",
    "    \"\"\" input: a list of four features \n",
    "                [ sepallen, sepalwid, petallen, petalwid ]\n",
    "        output: the predicted species of iris, from\n",
    "                  setosa (0), versicolor (1), virginica (2)\n",
    "    \"\"\"\n",
    "    our_features = np.asarray([Features])                 # extra brackets needed\n",
    "    scaled_features = SCALER.transform(our_features)      # we have to scale the features into \"scaled space\"\n",
    "    predicted_cat = MODEL.predict(scaled_features)        # then, the nnet can predict our \"cat\" variables\n",
    "    prediction_probs = nn.predict_proba(scaled_features) # all prediction probabilities\n",
    "    # our_features = SCALER.inverse_transform(scaled_features)  # we can convert back (optional!)\n",
    "    predicted_species = get_species(predicted_cat[0])     # (it's extra-nested) get the species name\n",
    "    return predicted_species, prediction_probs\n",
    "   \n",
    "#\n",
    "# Try it!\n",
    "# \n",
    "# Features = eval(input(\"Enter new Features: \"))\n",
    "#\n",
    "Features = [6.7,3.3,5.7,0.1]  # [5.8,2.7,4.1,1.0] [4.6,3.6,3.0,2.2] [6.7,3.3,5.7,2.1]\n",
    "\n",
    "LoF = [\n",
    "[4.8, 3.1, 1.6, 0.2 ],   # actually setosa\n",
    "[5.7, 2.9, 4.2, 1.3 ],   # actually versicolor\n",
    "[5.8, 2.7, 5.1, 1.9 ],   # actually virginica\n",
    "[5.2, 4.1, 1.5, 0.1 ],   # actually setosa\n",
    "[5.4, 3.4, 1.5, 0.4 ],   # actually setosa\n",
    "[5.1, 2.5, 3.0, 1.1 ],   # actually versicolor\n",
    "[6.2, 2.9, 4.3, 1.3 ],   # actually versicolor\n",
    "[6.3, 3.3, 6.0, 2.5 ],   # actually virginica\n",
    "[5.7, 2.8, 4.1, 1.3 ],   # actually virginica  <-- almost always wrong!\n",
    "]\n",
    "\n",
    "SEE_PROBS = False\n",
    "\n",
    "# run on each one:\n",
    "for Features in LoF:\n",
    "    MODEL = nn_classifier\n",
    "    SCALER = scaler\n",
    "    name, probs = predictive_model( Features, MODEL, SCALER )  # pass in the model, too!\n",
    "    prob_str = \"   with probs: \" + str(probs) if SEE_PROBS == True else \"\"\n",
    "    print(f\"I predict {name:>12s} from the features {Features}  {prob_str}\")    # Answers in the assignment..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### We _could_ use cross-validation to find the \"right\"/\"best\" size and shape of the NNet...\n",
    "\n",
    "<font color=\"Coral\">But, let's not...</font>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"DodgerBlue\">Instead, we'll explore two new directions:</font> \n",
    "+ regression and \n",
    "+ feature-predictability\n",
    "\n",
    "#### The next example...\n",
    "+ is one in which we estimate a botanical feature ... \n",
    "+ ... from the other four columns -- including the species!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sepallen': 0, 'sepalwid': 1, 'petallen': 2, 'petalwid': 3, 'is_setosa': 4, 'is_versicolor': 5, 'is_virginica': 6}\n",
      "\n",
      "Index(['sepallen', 'sepalwid', 'petallen', 'petalwid', 'is_setosa',\n",
      "       'is_versicolor', 'is_virginica'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# What shall we predict today?\n",
    "print(COL_INDEX)\n",
    "print()\n",
    "print(COLUMNS)\n",
    "\n",
    "# Let's first predict sepal length ('sepallen', column index 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data-assembly for feature-regression! +++\n",
      "\n",
      "y_all is \n",
      " [4.6 4.3 5.  5.8 4.4 4.4 4.5 4.7 5.  5.4 5.5 4.4 4.6 4.6 4.8 4.8 4.9 5.\n",
      " 5.  5.1 5.1 5.2 5.5 4.6 4.9 4.9 4.9 5.  5.1 5.1 5.1 5.2 5.3 5.4 5.7 4.7\n",
      " 4.8 5.  5.  5.  5.1 5.1 5.4 5.4 5.7 4.8 5.1 7.  4.9 5.  5.  5.7 5.6 5.5\n",
      " 5.5 5.2 5.6 5.8 5.5 5.5 5.8 6.  6.1 5.6 5.8 5.6 5.7 5.9 6.4 5.5 6.3 6.6\n",
      " 6.7 5.4 5.6 5.7 6.  6.  6.2 6.4 6.1 6.5 6.6 6.1 6.1 6.3 6.7 5.9 6.8 6.3\n",
      " 6.9 6.7 6.  4.9 6.  6.2 5.6 6.1 6.3 5.7 6.  6.3 5.8 5.8 5.9 6.3 6.5 6.9\n",
      " 6.5 6.7 6.4 6.4 6.2 6.9 6.4 6.5 6.8 6.1 6.3 6.3 6.4 6.4 6.7 6.7 6.7 6.9\n",
      " 6.5 6.7 7.2 6.8 7.1 7.2 7.2 7.4 7.7 7.3 7.9 7.6 7.7 7.7 7.7]\n",
      "\n",
      "X_all (the features, first few rows) is \n",
      " [[3.6 1.  0.2 1.  0.  0. ]\n",
      " [3.  1.1 0.1 1.  0.  0. ]\n",
      " [3.2 1.2 0.2 1.  0.  0. ]\n",
      " [4.  1.2 0.2 1.  0.  0. ]\n",
      " [3.  1.3 0.2 1.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we set up for a regression model that will predict 'sepallen'  (column index 0)\n",
    "#\n",
    "#   sepal length  'sepallen' (column index 0)  <-- our target for _regression_\n",
    "\n",
    "# We will use\n",
    "#\n",
    "#   sepal width   'sepalwid' (column index 1)\n",
    "#   petal length  'petallen' (column index 2)\n",
    "#   petal width   'petalwid' (column index 3)\n",
    "#   _and_\n",
    "#   species       'irisnum'  (column index 4)    # No problem - we can use this as a feature!\n",
    "\n",
    "print(\"+++ Start of data-assembly for feature-regression! +++\\n\")\n",
    "# construct the correct X_all from the columns we want\n",
    "# we use np.concatenate to combine parts of the dataset to get all-except-column 0:\n",
    "#                     exclude 0  , include 1 to the end\n",
    "X_all = np.concatenate( (A[:,0:0], A[:,1:]), axis=1)    # includes columns 1, 2, 3, and 4\n",
    "\n",
    "# if we wanted all-except-column 1:   X_all = np.concatenate( (A[:,0:1], A[:,2:]),axis=1)  # columns 0, 2, 3, and 4\n",
    "# if we wanted all-except-column 2:   X_all = np.concatenate( (A[:,0:2], A[:,3:]),axis=1)  # columns 0, 1, 3, and 4\n",
    "# if we wanted all-except-column 3:   X_all = np.concatenate( (A[:,0:3], A[:,4:]),axis=1)  # columns 0, 1, 2, and 4\n",
    "# if we wanted all-except-column 4:   X_all = np.concatenate( (A[:,0:4], A[:,5:]),axis=1)  # columns 0, 1, 2, and 3\n",
    "# (slicing is forgiving)\n",
    "\n",
    "\n",
    "y_all = A[:,0]                    # y (labels) ... is all of column 0, sepallen (sepal length) \n",
    "#                                 # change the line above to make other columns the target (y_all)\n",
    "print(f\"y_all is \\n {y_all}\")\n",
    "print() \n",
    "print(f\"X_all (the features, first few rows) is \\n {X_all[:5,:]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute!\n",
    "\n",
    "<font size=\"-2\">I like this because it adds an element of randomness to the whole process...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target values to predict: \n",
      " [5.6 6.5 6.  6.8 6.7 4.9 6.3 6.7 7.1 6.9 7.  5.6 5.  7.2 5.7 5.  6.7 4.8\n",
      " 4.6 6.5 5.1 6.4 4.5 5.4 6.4 4.4 5.9 7.7 5.4 5.  5.5 6.7 7.2 4.4 6.  6.1\n",
      " 6.4 5.1 6.2 5.1 6.7 6.  6.1 7.7 4.9 5.1 5.7 5.7 5.6 6.9 5.  6.5 6.7 6.7\n",
      " 5.5 5.4 5.2 5.  7.3 4.3 5.7 5.8 4.8 5.5 5.9 6.  4.4 5.1 4.6 5.5 7.7 6.5\n",
      " 4.9 6.  7.6 5.  6.1 6.9 4.8 5.4 5.5 6.7 5.2 4.8 5.7 4.9 6.3 6.3 5.1 7.7\n",
      " 5.8 5.  6.9 5.2 5.8 5.8 6.3 6.6 7.2 6.3 4.6 6.8 5.5 5.7 6.  5.6 6.8 5.8\n",
      " 6.5 5.6 6.6 5.5 6.1 4.7 6.1 6.4 6.2 5.4 4.9 5.  7.4 6.2 5.1 5.1 5.  4.7\n",
      " 4.6 6.1 7.9 5.9 6.3 5.  6.3 6.4 5.8 5.3 6.4 6.4 5.6 4.9 6.3]\n",
      "\n",
      "features (a few)\n",
      " [[3.  4.1 1.3 0.  1.  0. ]\n",
      " [3.2 5.1 2.  0.  0.  1. ]\n",
      " [3.4 4.5 1.6 0.  1.  0. ]\n",
      " [3.  5.5 2.1 0.  0.  1. ]\n",
      " [2.5 5.8 1.8 0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"target values to predict: \\n\",y_all)\n",
    "print(\"\\nfeatures (a few)\\n\", X_all[0:5,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and testing sets\n",
    "\n",
    "We'll stick with 80% training, 20% testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 112 rows;  testing with 29 rows\n",
      "\n",
      "Held-out data... (testing data: 29)\n",
      "y_test: [6.5 6.5 5.5 5.1 5.4 5.5 5.  5.6 6.5 6.5 6.2 6.1 5.8 5.7 6.  5.9 5.7 5.1\n",
      " 5.  7.7 6.3 5.1 5.1 5.7 5.8 5.4 5.4 6.8 5. ]\n",
      "\n",
      "X_test (few rows): [[3.  5.5 1.8 0.  0.  1. ]\n",
      " [2.8 4.6 1.5 0.  1.  0. ]\n",
      " [2.4 3.8 1.1 0.  1.  0. ]\n",
      " [3.7 1.5 0.4 1.  0.  0. ]\n",
      " [3.4 1.7 0.2 1.  0.  0. ]]\n",
      "\n",
      "Data used for modeling... (training data: 112)\n",
      "y_train: [6.8 5.9 4.7 4.7 6.7 6.  5.5 4.8 4.4 5.  6.1 6.7 4.9 5.6 5.5 6.  5.  7.4\n",
      " 6.3 5.  4.9 5.1 6.7 4.9 6.  4.6 4.6 6.1 7.2 7.2 4.9 5.2 6.3 5.8 6.3 6.3\n",
      " 7.1 7.7 5.2 6.7 7.9 5.6 7.7 5.7 6.4 4.8 4.4 5.5 5.4 6.7 4.5 4.6 6.7 4.4\n",
      " 6.9 6.9 6.6 4.8 5.3 6.6 5.  6.4 5.7 5.8 6.5 4.8 4.9 6.9 7.3 6.1 4.6 4.3\n",
      " 5.  6.9 5.  5.5 5.1 7.  5.9 5.5 6.4 6.8 5.6 6.4 6.2 7.2 6.2 5.  5.1 5.6\n",
      " 6.  5.2 6.1 6.3 5.8 5.1 5.6 7.6 6.3 6.4 6.4 6.1 5.7 5.4 5.8 6.  6.7 6.4\n",
      " 6.7 4.9 7.7 6.3]\n",
      "\n",
      "X_train (few rows): [[3.  5.5 2.1 0.  0.  1. ]\n",
      " [3.2 4.8 1.8 0.  1.  0. ]\n",
      " [3.2 1.6 0.2 1.  0.  0. ]\n",
      " [3.2 1.3 0.2 1.  0.  0. ]\n",
      " [3.3 5.7 2.5 0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's a nnet - this time, we _should_ use a <u>Scaler</u>\n",
    "\n",
    "<font size=\"-2\">Note that, for our <u>own</u> neural nets, this is performed by our senses:\n",
    "+ our sense of hearing scales sounds from whispers to shouts... \n",
    "    + ... into a single amplitude for interpretation. \n",
    "    + and, we keep the scale itself as an additional signal!\n",
    "+ similarly, our visual system scales across many orders of magnitude of brightness\n",
    "    + plus, our eyes can \"adapt\" to low light (and, less, to bright-light )\n",
    "+ less dramatically, for taste, touch, and smell...\n",
    "+ this scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  ->  pred   des  \n",
      "                                 [-0.04  0.92  1.12 -0.66] ->    ?    6.8                  \n",
      "                                 [ 0.46  0.52  0.73 -0.66] ->    ?    5.9                  \n",
      "                                 [ 0.46 -1.31 -1.38  1.51] ->    ?    4.7                  \n",
      "                                 [ 0.46 -1.49 -1.38  1.51] ->    ?    4.7                  \n",
      "                                 [ 0.71  1.03  1.65 -0.66] ->    ?    6.7                  \n",
      "\n",
      "                                                    input  ->  pred   des  \n",
      "                                         [3.  5.5 2.1 0. ] ->    ?    6.8                  \n",
      "                                         [3.2 4.8 1.8 0. ] ->    ?    5.9                  \n",
      "                                         [3.2 1.6 0.2 1. ] ->    ?    4.7                  \n",
      "                                         [3.2 1.3 0.2 1. ] ->    ?    4.7                  \n",
      "                                         [3.3 5.7 2.5 0. ] ->    ?    6.7                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "# reused from above - seeing the scaled data \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],None)\n",
    "\n",
    "# reused from above - seeing the unscaled data (inverting the scaler)\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],scaler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train the model\n",
    "\n",
    "Note that hidden_layer_sizes is an important parameter.\n",
    "+ for this hw, we're simply trying things out\n",
    "+ no need to optimize via cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 22.79464842\n",
      "Iteration 2, loss = 18.96879149\n",
      "Iteration 3, loss = 16.14703887\n",
      "Iteration 4, loss = 13.28203054\n",
      "Iteration 5, loss = 10.03898869\n",
      "Iteration 6, loss = 7.55733514\n",
      "Iteration 7, loss = 6.55390828\n",
      "Iteration 8, loss = 4.87306748\n",
      "Iteration 9, loss = 2.59979089\n",
      "Iteration 10, loss = 0.90642904\n",
      "Iteration 11, loss = 0.61300580\n",
      "Iteration 12, loss = 1.37192737\n",
      "Iteration 13, loss = 2.00269514\n",
      "Iteration 14, loss = 1.93495132\n",
      "Iteration 15, loss = 1.40554815\n",
      "Iteration 16, loss = 0.84212809\n",
      "Iteration 17, loss = 0.45585135\n",
      "Iteration 18, loss = 0.25045210\n",
      "Iteration 19, loss = 0.19168506\n",
      "Iteration 20, loss = 0.23534458\n",
      "Iteration 21, loss = 0.31700784\n",
      "Iteration 22, loss = 0.36314619\n",
      "Iteration 23, loss = 0.35154826\n",
      "Iteration 24, loss = 0.29795980\n",
      "Iteration 25, loss = 0.23005333\n",
      "Iteration 26, loss = 0.18425258\n",
      "Iteration 27, loss = 0.17633910\n",
      "Iteration 28, loss = 0.19096352\n",
      "Iteration 29, loss = 0.20061894\n",
      "Iteration 30, loss = 0.19635797\n",
      "Iteration 31, loss = 0.18758140\n",
      "Iteration 32, loss = 0.18570704\n",
      "Iteration 33, loss = 0.18618475\n",
      "Iteration 34, loss = 0.17647141\n",
      "Iteration 35, loss = 0.15739416\n",
      "Iteration 36, loss = 0.14042835\n",
      "Iteration 37, loss = 0.12995176\n",
      "Iteration 38, loss = 0.12477606\n",
      "Iteration 39, loss = 0.12283780\n",
      "Iteration 40, loss = 0.12386811\n",
      "Iteration 41, loss = 0.12744404\n",
      "Iteration 42, loss = 0.13001928\n",
      "Iteration 43, loss = 0.12830244\n",
      "Iteration 44, loss = 0.12272427\n",
      "Iteration 45, loss = 0.11625753\n",
      "Iteration 46, loss = 0.10960517\n",
      "Iteration 47, loss = 0.10366075\n",
      "Iteration 48, loss = 0.10075479\n",
      "Iteration 49, loss = 0.10210651\n",
      "Iteration 50, loss = 0.10599781\n",
      "Iteration 51, loss = 0.10862006\n",
      "Iteration 52, loss = 0.10747190\n",
      "Iteration 53, loss = 0.10310631\n",
      "Iteration 54, loss = 0.09803735\n",
      "Iteration 55, loss = 0.09437920\n",
      "Iteration 56, loss = 0.09247016\n",
      "Iteration 57, loss = 0.09166610\n",
      "Iteration 58, loss = 0.09263075\n",
      "Iteration 59, loss = 0.09403992\n",
      "Iteration 60, loss = 0.09374288\n",
      "Iteration 61, loss = 0.09182772\n",
      "Iteration 62, loss = 0.08932810\n",
      "Iteration 63, loss = 0.08708347\n",
      "Iteration 64, loss = 0.08531016\n",
      "Iteration 65, loss = 0.08419979\n",
      "Iteration 66, loss = 0.08399290\n",
      "Iteration 67, loss = 0.08433610\n",
      "Iteration 68, loss = 0.08424731\n",
      "Iteration 69, loss = 0.08348715\n",
      "Iteration 70, loss = 0.08239241\n",
      "Iteration 71, loss = 0.08107628\n",
      "Iteration 72, loss = 0.07986979\n",
      "Iteration 73, loss = 0.07902184\n",
      "Iteration 74, loss = 0.07844420\n",
      "Iteration 75, loss = 0.07799192\n",
      "Iteration 76, loss = 0.07762616\n",
      "Iteration 77, loss = 0.07729209\n",
      "Iteration 78, loss = 0.07689444\n",
      "Iteration 79, loss = 0.07646484\n",
      "Iteration 80, loss = 0.07596187\n",
      "Iteration 81, loss = 0.07539806\n",
      "Iteration 82, loss = 0.07481332\n",
      "Iteration 83, loss = 0.07429263\n",
      "Iteration 84, loss = 0.07391360\n",
      "Iteration 85, loss = 0.07366685\n",
      "Iteration 86, loss = 0.07344975\n",
      "Iteration 87, loss = 0.07319730\n",
      "Iteration 88, loss = 0.07289882\n",
      "Iteration 89, loss = 0.07256197\n",
      "Iteration 90, loss = 0.07220404\n",
      "Iteration 91, loss = 0.07185805\n",
      "Iteration 92, loss = 0.07154181\n",
      "Iteration 93, loss = 0.07124639\n",
      "Iteration 94, loss = 0.07096564\n",
      "Iteration 95, loss = 0.07070051\n",
      "Iteration 96, loss = 0.07044294\n",
      "Iteration 97, loss = 0.07018614\n",
      "Iteration 98, loss = 0.06992715\n",
      "Iteration 99, loss = 0.06965668\n",
      "Iteration 100, loss = 0.06936887\n",
      "Iteration 101, loss = 0.06906597\n",
      "Iteration 102, loss = 0.06876220\n",
      "Iteration 103, loss = 0.06846824\n",
      "Iteration 104, loss = 0.06818982\n",
      "Iteration 105, loss = 0.06792263\n",
      "Iteration 106, loss = 0.06765323\n",
      "Iteration 107, loss = 0.06736991\n",
      "Iteration 108, loss = 0.06707124\n",
      "Iteration 109, loss = 0.06676332\n",
      "Iteration 110, loss = 0.06645357\n",
      "Iteration 111, loss = 0.06614932\n",
      "Iteration 112, loss = 0.06585120\n",
      "Iteration 113, loss = 0.06555691\n",
      "Iteration 114, loss = 0.06526399\n",
      "Iteration 115, loss = 0.06496849\n",
      "Iteration 116, loss = 0.06466841\n",
      "Iteration 117, loss = 0.06436475\n",
      "Iteration 118, loss = 0.06405843\n",
      "Iteration 119, loss = 0.06375112\n",
      "Iteration 120, loss = 0.06344514\n",
      "Iteration 121, loss = 0.06314084\n",
      "Iteration 122, loss = 0.06283826\n",
      "Iteration 123, loss = 0.06253696\n",
      "Iteration 124, loss = 0.06223408\n",
      "Iteration 125, loss = 0.06192905\n",
      "Iteration 126, loss = 0.06162288\n",
      "Iteration 127, loss = 0.06131641\n",
      "Iteration 128, loss = 0.06101040\n",
      "Iteration 129, loss = 0.06070552\n",
      "Iteration 130, loss = 0.06040158\n",
      "Iteration 131, loss = 0.06009836\n",
      "Iteration 132, loss = 0.05979582\n",
      "Iteration 133, loss = 0.05949363\n",
      "Iteration 134, loss = 0.05919164\n",
      "Iteration 135, loss = 0.05888994\n",
      "Iteration 136, loss = 0.05858865\n",
      "Iteration 137, loss = 0.05828826\n",
      "Iteration 138, loss = 0.05798931\n",
      "Iteration 139, loss = 0.05769193\n",
      "Iteration 140, loss = 0.05739630\n",
      "Iteration 141, loss = 0.05710212\n",
      "Iteration 142, loss = 0.05680925\n",
      "Iteration 143, loss = 0.05651786\n",
      "Iteration 144, loss = 0.05622844\n",
      "Iteration 145, loss = 0.05594142\n",
      "Iteration 146, loss = 0.05565708\n",
      "Iteration 147, loss = 0.05537537\n",
      "Iteration 148, loss = 0.05509621\n",
      "Iteration 149, loss = 0.05481967\n",
      "Iteration 150, loss = 0.05454584\n",
      "Iteration 151, loss = 0.05427522\n",
      "Iteration 152, loss = 0.05400790\n",
      "Iteration 153, loss = 0.05374404\n",
      "Iteration 154, loss = 0.05348379\n",
      "Iteration 155, loss = 0.05322725\n",
      "Iteration 156, loss = 0.05297449\n",
      "Iteration 157, loss = 0.05272561\n",
      "Iteration 158, loss = 0.05248072\n",
      "Iteration 159, loss = 0.05223997\n",
      "Iteration 160, loss = 0.05200355\n",
      "Iteration 161, loss = 0.05177158\n",
      "Iteration 162, loss = 0.05154415\n",
      "Iteration 163, loss = 0.05132130\n",
      "Iteration 164, loss = 0.05110308\n",
      "Iteration 165, loss = 0.05088954\n",
      "Iteration 166, loss = 0.05068075\n",
      "Iteration 167, loss = 0.05047690\n",
      "Iteration 168, loss = 0.05027812\n",
      "Iteration 169, loss = 0.05008459\n",
      "Iteration 170, loss = 0.04989588\n",
      "Iteration 171, loss = 0.04971225\n",
      "Iteration 172, loss = 0.04953347\n",
      "Iteration 173, loss = 0.04935907\n",
      "Iteration 174, loss = 0.04918941\n",
      "Iteration 175, loss = 0.04902418\n",
      "Iteration 176, loss = 0.04886353\n",
      "Iteration 177, loss = 0.04870757\n",
      "Iteration 178, loss = 0.04855600\n",
      "Iteration 179, loss = 0.04840896\n",
      "Iteration 180, loss = 0.04826617\n",
      "Iteration 181, loss = 0.04812749\n",
      "Iteration 182, loss = 0.04799294\n",
      "Iteration 183, loss = 0.04786231\n",
      "Iteration 184, loss = 0.04773573\n",
      "Iteration 185, loss = 0.04761375\n",
      "Iteration 186, loss = 0.04749483\n",
      "Iteration 187, loss = 0.04738061\n",
      "Iteration 188, loss = 0.04726852\n",
      "Iteration 189, loss = 0.04716116\n",
      "Iteration 190, loss = 0.04705601\n",
      "Iteration 191, loss = 0.04695622\n",
      "Iteration 192, loss = 0.04685723\n",
      "Iteration 193, loss = 0.04676304\n",
      "Iteration 194, loss = 0.04667085\n",
      "Iteration 195, loss = 0.04658228\n",
      "Iteration 196, loss = 0.04649619\n",
      "Iteration 197, loss = 0.04641350\n",
      "Iteration 198, loss = 0.04633301\n",
      "Iteration 199, loss = 0.04625548\n",
      "Iteration 200, loss = 0.04618028\n",
      "Iteration 201, loss = 0.04610756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 0.046 \n",
      "And, its square root:         0.215 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=500,          # how many training epochs\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_:<6.3f}\")\n",
    "print(f\"And, its square root:         {nn_regressor.loss_ ** 0.5:<6.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Note that square-root of the mean-squared-error is an \"expected error\" in predicting our feature\n",
    "#\n",
    "# It's not the only estimate of expected error, but it's easier to think about than the squared error (which has the wrong units)\n",
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test!  \n",
    "\n",
    "Testing is different for regression!  \n",
    "<font size=\"-1\">\n",
    "+ it's floating-point, so it's no longer \"right\" or \"wrong\"\n",
    "+ instead, the _difference_ is what matters\n",
    "+ in fact, the <u> _absolute difference_ </u> is usually what matters: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             input  ->   pred    des.     absdiff  \n",
      "          [3.  5.5 1.8 0.  0.  1. ] ->  +6.66   +6.50       0.16   \n",
      "          [2.8 4.6 1.5 0.  1.  0. ] ->  +6.10   +6.50       0.40   \n",
      "          [2.4 3.8 1.1 0.  1.  0. ] ->  +5.51   +5.50       0.01   \n",
      "          [3.7 1.5 0.4 1.  0.  0. ] ->  +5.05   +5.10       0.05   \n",
      "          [3.4 1.7 0.2 1.  0.  0. ] ->  +4.94   +5.40       0.46   \n",
      "          [3.5 1.3 0.2 1.  0.  0. ] ->  +5.06   +5.50       0.44   \n",
      "          [3.4 1.5 0.2 1.  0.  0. ] ->  +4.96   +5.00       0.04   \n",
      "          [2.9 3.6 1.3 0.  1.  0. ] ->  +5.35   +5.60       0.25   \n",
      "          [3.  5.2 2.  0.  0.  1. ] ->  +6.33   +6.50       0.17   \n",
      "          [3.2 5.1 2.  0.  0.  1. ] ->  +6.34   +6.50       0.16   \n",
      "          [2.8 4.8 1.8 0.  0.  1. ] ->  +5.95   +6.20       0.25   \n",
      "          [3.  4.6 1.4 0.  1.  0. ] ->  +6.34   +6.10       0.24   \n",
      "          [4.  1.2 0.2 1.  0.  0. ] ->  +5.43   +5.80       0.37   \n",
      "          [3.  4.2 1.2 0.  1.  0. ] ->  +6.16   +5.70       0.46   \n",
      "          [2.9 4.5 1.5 0.  1.  0. ] ->  +6.05   +6.00       0.05   \n",
      "          [3.  5.1 1.8 0.  0.  1. ] ->  +6.31   +5.90       0.41   \n",
      "          [2.5 5.  2.  0.  0.  1. ] ->  +5.91   +5.70       0.21   \n",
      "          [3.5 1.4 0.3 1.  0.  0. ] ->  +4.98   +5.10       0.12   \n",
      "          [3.5 1.6 0.6 1.  0.  0. ] ->  +4.76   +5.00       0.24   \n",
      "          [2.6 6.9 2.3 0.  0.  1. ] ->  +7.53   +7.70       0.17   \n",
      "          [2.3 4.4 1.3 0.  1.  0. ] ->  +5.84   +6.30       0.46   \n",
      "          [3.4 1.5 0.2 1.  0.  0. ] ->  +4.96   +5.10       0.14   \n",
      "          [3.8 1.6 0.2 1.  0.  0. ] ->  +5.24   +5.10       0.14   \n",
      "          [4.4 1.5 0.4 1.  0.  0. ] ->  +5.55   +5.70       0.15   \n",
      "          [2.7 4.1 1.  0.  1.  0. ] ->  +6.12   +5.80       0.32   \n",
      "          [3.9 1.3 0.4 1.  0.  0. ] ->  +5.21   +5.40       0.19   \n",
      "          [3.7 1.5 0.2 1.  0.  0. ] ->  +5.18   +5.40       0.22   \n",
      "          [3.2 5.9 2.3 0.  0.  1. ] ->  +6.94   +6.80       0.14   \n",
      "          [3.3 1.4 0.2 1.  0.  0. ] ->  +4.90   +5.00       0.10   \n",
      "\n",
      "+++++   +++++   +++++           \n",
      "average abs diff error:   0.225 \n",
      "+++++   +++++   +++++           \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do? Try out the TEST data...\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>35s} ->  {'pred':^6s}  {'des.':^6s}   {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,:]!s:>35s} ->  {pred:<+6.2f}  {desired:<+6.2f}   {result:^10.2f}\") \n",
    "\n",
    "    print(\"\\n\" + \"+++++   +++++   +++++           \")\n",
    "    print(f\"average abs diff error:   {error/len(y):<6.3f}\")\n",
    "    print(\"+++++   +++++   +++++           \")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your task:  &nbsp;&nbsp; model ***all*** of the iris features!\n",
    "\n",
    "Which of the four floating-point iris features is\n",
    "+ _most_ predictable? &nbsp; (smallest average abs diff error)\n",
    "+ _least_ predictable? &nbsp; (largest average abs diff error)\n",
    "\n",
    "<br>\n",
    "\n",
    "As with the _feature importances_ , it's often the <u>insight</u> we value, more than creating the predictor itself..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEPALWID\n",
      "+++ Start of data-assembly for feature-regression! +++\n",
      "\n",
      "+++ Scramble the data... +++\n",
      "\n",
      "+++ 80/20 train-test split... +++\n",
      "\n",
      "training with 112 rows;  testing with 29 rows\n",
      "\n",
      "+++ Scale values... +++\n",
      "\n",
      "                                                    input  ->  pred   des  \n",
      "                                 [ 0.45  0.62  0.82 -0.73] ->    ?    2.8                  \n",
      "                                 [ 1.05  1.13  1.75 -0.73] ->    ?    3.3                  \n",
      "                                 [-1.   -1.42 -1.28  1.37] ->    ?    3.2                  \n",
      "                                 [-0.75 -1.31 -1.28  1.37] ->    ?    3.4                  \n",
      "                                 [-0.39  0.17  0.17 -0.73] ->    ?    2.3                  \n",
      "\n",
      "                                                    input  ->  pred   des  \n",
      "                                         [6.2 4.8 1.8 0. ] ->    ?    2.8                  \n",
      "                                         [6.7 5.7 2.5 0. ] ->    ?    3.3                  \n",
      "                                         [5.  1.2 0.2 1. ] ->    ?    3.2                  \n",
      "                                         [5.2 1.4 0.2 1. ] ->    ?    3.4                  \n",
      "                                         [5.5 4.  1.3 0. ] ->    ?    2.3                  \n",
      "\n",
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 5.87159751\n",
      "Iteration 2, loss = 2.47528342\n",
      "Iteration 3, loss = 0.51054426\n",
      "Iteration 4, loss = 1.05390553\n",
      "Iteration 5, loss = 1.41582044\n",
      "Iteration 6, loss = 0.74220228\n",
      "Iteration 7, loss = 0.28600811\n",
      "Iteration 8, loss = 0.30211347\n",
      "Iteration 9, loss = 0.54432922\n",
      "Iteration 10, loss = 0.55664930\n",
      "Iteration 11, loss = 0.37212486\n",
      "Iteration 12, loss = 0.24638238\n",
      "Iteration 13, loss = 0.33210917\n",
      "Iteration 14, loss = 0.34442285\n",
      "Iteration 15, loss = 0.20149859\n",
      "Iteration 16, loss = 0.09237398\n",
      "Iteration 17, loss = 0.09795778\n",
      "Iteration 18, loss = 0.16072892\n",
      "Iteration 19, loss = 0.20667433\n",
      "Iteration 20, loss = 0.20545483\n",
      "Iteration 21, loss = 0.16550838\n",
      "Iteration 22, loss = 0.11570540\n",
      "Iteration 23, loss = 0.08503362\n",
      "Iteration 24, loss = 0.08255105\n",
      "Iteration 25, loss = 0.09340193\n",
      "Iteration 26, loss = 0.10020768\n",
      "Iteration 27, loss = 0.10133451\n",
      "Iteration 28, loss = 0.10200273\n",
      "Iteration 29, loss = 0.10094736\n",
      "Iteration 30, loss = 0.09278629\n",
      "Iteration 31, loss = 0.07719011\n",
      "Iteration 32, loss = 0.06182140\n",
      "Iteration 33, loss = 0.05648343\n",
      "Iteration 34, loss = 0.06367891\n",
      "Iteration 35, loss = 0.07477176\n",
      "Iteration 36, loss = 0.07793941\n",
      "Iteration 37, loss = 0.07029087\n",
      "Iteration 38, loss = 0.05929960\n",
      "Iteration 39, loss = 0.05315552\n",
      "Iteration 40, loss = 0.05320215\n",
      "Iteration 41, loss = 0.05535031\n",
      "Iteration 42, loss = 0.05585048\n",
      "Iteration 43, loss = 0.05454604\n",
      "Iteration 44, loss = 0.05335134\n",
      "Iteration 45, loss = 0.05278574\n",
      "Iteration 46, loss = 0.05125090\n",
      "Iteration 47, loss = 0.04793121\n",
      "Iteration 48, loss = 0.04462899\n",
      "Iteration 49, loss = 0.04362240\n",
      "Iteration 50, loss = 0.04491146\n",
      "Iteration 51, loss = 0.04639739\n",
      "Iteration 52, loss = 0.04611908\n",
      "Iteration 53, loss = 0.04391647\n",
      "Iteration 54, loss = 0.04127146\n",
      "Iteration 55, loss = 0.03977263\n",
      "Iteration 56, loss = 0.03966152\n",
      "Iteration 57, loss = 0.03991087\n",
      "Iteration 58, loss = 0.03964260\n",
      "Iteration 59, loss = 0.03898647\n",
      "Iteration 60, loss = 0.03840789\n",
      "Iteration 61, loss = 0.03786442\n",
      "Iteration 62, loss = 0.03704868\n",
      "Iteration 63, loss = 0.03607774\n",
      "Iteration 64, loss = 0.03548002\n",
      "Iteration 65, loss = 0.03549020\n",
      "Iteration 66, loss = 0.03566239\n",
      "Iteration 67, loss = 0.03538546\n",
      "Iteration 68, loss = 0.03462464\n",
      "Iteration 69, loss = 0.03389105\n",
      "Iteration 70, loss = 0.03356189\n",
      "Iteration 71, loss = 0.03351300\n",
      "Iteration 72, loss = 0.03342662\n",
      "Iteration 73, loss = 0.03320781\n",
      "Iteration 74, loss = 0.03297014\n",
      "Iteration 75, loss = 0.03276100\n",
      "Iteration 76, loss = 0.03251076\n",
      "Iteration 77, loss = 0.03223223\n",
      "Iteration 78, loss = 0.03206497\n",
      "Iteration 79, loss = 0.03206365\n",
      "Iteration 80, loss = 0.03208871\n",
      "Iteration 81, loss = 0.03198359\n",
      "Iteration 82, loss = 0.03177371\n",
      "Iteration 83, loss = 0.03161066\n",
      "Iteration 84, loss = 0.03156421\n",
      "Iteration 85, loss = 0.03156151\n",
      "Iteration 86, loss = 0.03152478\n",
      "Iteration 87, loss = 0.03146590\n",
      "Iteration 88, loss = 0.03141748\n",
      "Iteration 89, loss = 0.03136352\n",
      "Iteration 90, loss = 0.03128757\n",
      "Iteration 91, loss = 0.03122437\n",
      "Iteration 92, loss = 0.03120885\n",
      "Iteration 93, loss = 0.03121187\n",
      "Iteration 94, loss = 0.03117839\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 0.031 \n",
      "And, its square root:         0.177 \n",
      "\n",
      "SEPALWID TESTING RESULTS\n",
      "                             input  ->   pred    des.     absdiff  \n",
      "          [7.  4.7 1.4 0.  1.  0. ] ->  +2.92   +3.20       0.28   \n",
      "          [7.7 6.7 2.  0.  0.  1. ] ->  +3.18   +2.80       0.38   \n",
      "          [4.8 1.9 0.2 1.  0.  0. ] ->  +3.00   +3.40       0.40   \n",
      "          [6.  4.8 1.8 0.  0.  1. ] ->  +2.71   +3.00       0.29   \n",
      "          [6.9 5.7 2.3 0.  0.  1. ] ->  +3.18   +3.20       0.02   \n",
      "          [6.2 4.5 1.5 0.  1.  0. ] ->  +2.90   +2.20       0.70   \n",
      "          [7.2 5.8 1.6 0.  0.  1. ] ->  +2.79   +3.00       0.21   \n",
      "          [5.2 1.5 0.2 1.  0.  0. ] ->  +3.59   +3.50       0.09   \n",
      "          [5.8 3.9 1.2 0.  1.  0. ] ->  +2.62   +2.70       0.08   \n",
      "          [6.7 5.7 2.1 0.  0.  1. ] ->  +3.05   +3.30       0.25   \n",
      "          [7.7 6.7 2.2 0.  0.  1. ] ->  +3.30   +3.80       0.50   \n",
      "          [6.4 4.5 1.5 0.  1.  0. ] ->  +2.91   +3.20       0.29   \n",
      "          [6.5 5.8 2.2 0.  0.  1. ] ->  +3.11   +3.00       0.11   \n",
      "          [5.8 1.2 0.2 1.  0.  0. ] ->  +4.33   +4.00       0.33   \n",
      "          [5.8 4.1 1.  0.  1.  0. ] ->  +2.52   +2.70       0.18   \n",
      "          [4.6 1.4 0.3 1.  0.  0. ] ->  +2.98   +3.40       0.42   \n",
      "          [5.  1.6 0.6 1.  0.  0. ] ->  +3.13   +3.50       0.37   \n",
      "          [5.5 1.4 0.2 1.  0.  0. ] ->  +3.94   +4.20       0.26   \n",
      "          [6.4 5.3 2.3 0.  0.  1. ] ->  +3.10   +3.20       0.10   \n",
      "          [4.3 1.1 0.1 1.  0.  0. ] ->  +2.92   +3.00       0.08   \n",
      "          [5.4 4.5 1.5 0.  1.  0. ] ->  +2.85   +3.00       0.15   \n",
      "          [4.8 1.6 0.2 1.  0.  0. ] ->  +3.14   +3.40       0.26   \n",
      "          [5.6 4.2 1.3 0.  1.  0. ] ->  +2.70   +2.70       0.00   \n",
      "          [5.8 5.1 2.4 0.  0.  1. ] ->  +3.10   +2.80       0.30   \n",
      "          [6.4 5.3 1.9 0.  0.  1. ] ->  +2.86   +2.70       0.16   \n",
      "          [5.6 3.6 1.3 0.  1.  0. ] ->  +2.62   +2.90       0.28   \n",
      "          [5.7 4.2 1.2 0.  1.  0. ] ->  +2.65   +3.00       0.35   \n",
      "          [6.  5.  1.5 0.  0.  1. ] ->  +2.55   +2.20       0.35   \n",
      "          [6.9 5.4 2.1 0.  0.  1. ] ->  +3.02   +3.10       0.08   \n",
      "\n",
      "+++++   +++++   +++++           \n",
      "average abs diff error:   0.251 \n",
      "+++++   +++++   +++++           \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "~~~~~~~~~~~~\n",
    "Sepal width\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "Here we set up for a regression model that will predict sepal width 'sepalwid' (column index 1)\n",
    "\n",
    "\"\"\"\n",
    "print(\"SEPALWID\")\n",
    "print(\"+++ Start of data-assembly for feature-regression! +++\\n\")\n",
    "# construct the correct X_all from the columns we want\n",
    "X_all = np.concatenate( (A[:,0:1], A[:,2:]),axis=1)  # includes columns 0, 2, 3, and 4\n",
    "\n",
    "y_all = A[:,1]                    # y (labels) ... is all of column 1, sepalwid\n",
    "\n",
    "# print(f\"y_all is \\n {y_all}\")\n",
    "# print() \n",
    "# print(f\"X_all (the features, first few rows) is \\n {X_all[:5,:]}\")\n",
    "# print() \n",
    "# print() \n",
    "\n",
    "\n",
    "\n",
    "# Scramble the data, to give a different TRAIN/TEST split each time...\n",
    "print(\"+++ Scramble the data... +++\\n\")\n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "# print(\"target values to predict: \\n\",y_all)\n",
    "# print(\"\\nfeatures (a few)\\n\", X_all[0:5,:])\n",
    "\n",
    "\n",
    "\n",
    "# We next separate into test data and training data ... \n",
    "print(\"+++ 80/20 train-test split... +++\\n\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "# print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "# print(f\"y_test: {y_test}\\n\")\n",
    "# print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "# print()\n",
    "# print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "# print(f\"y_train: {y_train}\\n\")\n",
    "# print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows\n",
    "\n",
    "\n",
    "\n",
    "# For NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    his is done through the \"StandardScaler\" in scikit-learn\n",
    "print(\"+++ Scale values... +++\\n\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "# reused from above - seeing the scaled data \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],None)\n",
    "\n",
    "# reused from above - seeing the unscaled data (inverting the scaler)\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],scaler)\n",
    "\n",
    "# Train\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=500,          # how many training epochs\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_:<6.3f}\")\n",
    "print(f\"And, its square root:         {nn_regressor.loss_ ** 0.5:<6.3f}\")\n",
    "print()\n",
    "\n",
    "#\n",
    "# how did it do? Try out the TEST data...\n",
    "#\n",
    "print(\"SEPALWID TESTING RESULTS\")\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>35s} ->  {'pred':^6s}  {'des.':^6s}   {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,:]!s:>35s} ->  {pred:<+6.2f}  {desired:<+6.2f}   {result:^10.2f}\") \n",
    "\n",
    "    print(\"\\n\" + \"+++++   +++++   +++++           \")\n",
    "    print(f\"average abs diff error:   {error/len(y):<6.3f}\")\n",
    "    print(\"+++++   +++++   +++++           \")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PETALLEN\n",
      "+++ Start of data-assembly for feature-regression! +++\n",
      "\n",
      "+++ Scramble the data... +++\n",
      "\n",
      "+++ 80/20 train-test split... +++\n",
      "\n",
      "training with 112 rows;  testing with 29 rows\n",
      "\n",
      "+++ Scale values... +++\n",
      "\n",
      "                                                    input  ->  pred   des  \n",
      "                                 [ 0.61 -0.79  0.92 -0.69] ->    ?    5.3                  \n",
      "                                 [ 1.57  0.33  0.79 -0.69] ->    ?    6.0                  \n",
      "                                 [-0.94  1.67 -1.22  1.45] ->    ?    1.5                  \n",
      "                                 [-0.94  1.45 -1.09  1.45] ->    ?    1.5                  \n",
      "                                 [-0.71  1.45 -1.36  1.45] ->    ?    1.5                  \n",
      "\n",
      "                                                    input  ->  pred   des  \n",
      "                                         [6.4 2.7 1.9 0. ] ->    ?    5.3                  \n",
      "                                         [7.2 3.2 1.8 0. ] ->    ?    6.0                  \n",
      "                                         [5.1 3.8 0.3 1. ] ->    ?    1.5                  \n",
      "                                         [5.1 3.7 0.4 1. ] ->    ?    1.5                  \n",
      "                                         [5.3 3.7 0.2 1. ] ->    ?    1.5                  \n",
      "\n",
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 15.60114027\n",
      "Iteration 2, loss = 10.00309549\n",
      "Iteration 3, loss = 7.04520565\n",
      "Iteration 4, loss = 4.97593295\n",
      "Iteration 5, loss = 3.06301545\n",
      "Iteration 6, loss = 1.75223267\n",
      "Iteration 7, loss = 1.44406183\n",
      "Iteration 8, loss = 1.95013362\n",
      "Iteration 9, loss = 1.69897823\n",
      "Iteration 10, loss = 0.96341482\n",
      "Iteration 11, loss = 0.46476611\n",
      "Iteration 12, loss = 0.34298196\n",
      "Iteration 13, loss = 0.40679594\n",
      "Iteration 14, loss = 0.47767233\n",
      "Iteration 15, loss = 0.49683841\n",
      "Iteration 16, loss = 0.46122882\n",
      "Iteration 17, loss = 0.37784654\n",
      "Iteration 18, loss = 0.27821884\n",
      "Iteration 19, loss = 0.19786556\n",
      "Iteration 20, loss = 0.16548780\n",
      "Iteration 21, loss = 0.18908057\n",
      "Iteration 22, loss = 0.24411911\n",
      "Iteration 23, loss = 0.28394061\n",
      "Iteration 24, loss = 0.27629438\n",
      "Iteration 25, loss = 0.22951850\n",
      "Iteration 26, loss = 0.17685397\n",
      "Iteration 27, loss = 0.14662980\n",
      "Iteration 28, loss = 0.14333676\n",
      "Iteration 29, loss = 0.15590040\n",
      "Iteration 30, loss = 0.16766417\n",
      "Iteration 31, loss = 0.16674897\n",
      "Iteration 32, loss = 0.15093233\n",
      "Iteration 33, loss = 0.12629940\n",
      "Iteration 34, loss = 0.10337468\n",
      "Iteration 35, loss = 0.09130561\n",
      "Iteration 36, loss = 0.09269819\n",
      "Iteration 37, loss = 0.10191239\n",
      "Iteration 38, loss = 0.10886804\n",
      "Iteration 39, loss = 0.10635441\n",
      "Iteration 40, loss = 0.09494596\n",
      "Iteration 41, loss = 0.08152938\n",
      "Iteration 42, loss = 0.07323171\n",
      "Iteration 43, loss = 0.07243735\n",
      "Iteration 44, loss = 0.07623778\n",
      "Iteration 45, loss = 0.07950205\n",
      "Iteration 46, loss = 0.07863492\n",
      "Iteration 47, loss = 0.07358094\n",
      "Iteration 48, loss = 0.06726472\n",
      "Iteration 49, loss = 0.06317518\n",
      "Iteration 50, loss = 0.06276872\n",
      "Iteration 51, loss = 0.06452778\n",
      "Iteration 52, loss = 0.06543479\n",
      "Iteration 53, loss = 0.06360807\n",
      "Iteration 54, loss = 0.05971903\n",
      "Iteration 55, loss = 0.05604606\n",
      "Iteration 56, loss = 0.05438806\n",
      "Iteration 57, loss = 0.05480056\n",
      "Iteration 58, loss = 0.05585511\n",
      "Iteration 59, loss = 0.05602467\n",
      "Iteration 60, loss = 0.05482125\n",
      "Iteration 61, loss = 0.05285670\n",
      "Iteration 62, loss = 0.05120488\n",
      "Iteration 63, loss = 0.05049443\n",
      "Iteration 64, loss = 0.05047315\n",
      "Iteration 65, loss = 0.05034606\n",
      "Iteration 66, loss = 0.04946545\n",
      "Iteration 67, loss = 0.04802590\n",
      "Iteration 68, loss = 0.04666288\n",
      "Iteration 69, loss = 0.04590705\n",
      "Iteration 70, loss = 0.04574985\n",
      "Iteration 71, loss = 0.04572585\n",
      "Iteration 72, loss = 0.04538409\n",
      "Iteration 73, loss = 0.04464351\n",
      "Iteration 74, loss = 0.04378265\n",
      "Iteration 75, loss = 0.04313972\n",
      "Iteration 76, loss = 0.04280677\n",
      "Iteration 77, loss = 0.04259102\n",
      "Iteration 78, loss = 0.04224314\n",
      "Iteration 79, loss = 0.04171033\n",
      "Iteration 80, loss = 0.04114882\n",
      "Iteration 81, loss = 0.04074898\n",
      "Iteration 82, loss = 0.04055368\n",
      "Iteration 83, loss = 0.04041635\n",
      "Iteration 84, loss = 0.04020494\n",
      "Iteration 85, loss = 0.03988120\n",
      "Iteration 86, loss = 0.03952793\n",
      "Iteration 87, loss = 0.03922844\n",
      "Iteration 88, loss = 0.03903417\n",
      "Iteration 89, loss = 0.03887787\n",
      "Iteration 90, loss = 0.03867728\n",
      "Iteration 91, loss = 0.03842156\n",
      "Iteration 92, loss = 0.03816897\n",
      "Iteration 93, loss = 0.03797585\n",
      "Iteration 94, loss = 0.03783910\n",
      "Iteration 95, loss = 0.03770782\n",
      "Iteration 96, loss = 0.03754078\n",
      "Iteration 97, loss = 0.03734546\n",
      "Iteration 98, loss = 0.03716066\n",
      "Iteration 99, loss = 0.03699104\n",
      "Iteration 100, loss = 0.03685082\n",
      "Iteration 101, loss = 0.03669762\n",
      "Iteration 102, loss = 0.03648564\n",
      "Iteration 103, loss = 0.03623354\n",
      "Iteration 104, loss = 0.03598671\n",
      "Iteration 105, loss = 0.03575765\n",
      "Iteration 106, loss = 0.03553819\n",
      "Iteration 107, loss = 0.03531464\n",
      "Iteration 108, loss = 0.03508585\n",
      "Iteration 109, loss = 0.03486465\n",
      "Iteration 110, loss = 0.03466362\n",
      "Iteration 111, loss = 0.03446154\n",
      "Iteration 112, loss = 0.03425129\n",
      "Iteration 113, loss = 0.03403968\n",
      "Iteration 114, loss = 0.03383034\n",
      "Iteration 115, loss = 0.03363273\n",
      "Iteration 116, loss = 0.03345134\n",
      "Iteration 117, loss = 0.03328447\n",
      "Iteration 118, loss = 0.03313773\n",
      "Iteration 119, loss = 0.03299552\n",
      "Iteration 120, loss = 0.03286200\n",
      "Iteration 121, loss = 0.03276943\n",
      "Iteration 122, loss = 0.03269219\n",
      "Iteration 123, loss = 0.03262136\n",
      "Iteration 124, loss = 0.03255466\n",
      "Iteration 125, loss = 0.03249368\n",
      "Iteration 126, loss = 0.03244025\n",
      "Iteration 127, loss = 0.03239334\n",
      "Iteration 128, loss = 0.03235085\n",
      "Iteration 129, loss = 0.03231026\n",
      "Iteration 130, loss = 0.03227204\n",
      "Iteration 131, loss = 0.03223767\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 0.032 \n",
      "And, its square root:         0.180 \n",
      "\n",
      "PETALLEN TESTING RESULTS\n",
      "                             input  ->   pred    des.     absdiff  \n",
      "          [7.6 3.  2.1 0.  0.  1. ] ->  +6.42   +6.60       0.18   \n",
      "          [6.8 3.2 2.3 0.  0.  1. ] ->  +5.65   +5.90       0.25   \n",
      "          [4.7 3.2 0.2 1.  0.  0. ] ->  +1.48   +1.30       0.18   \n",
      "          [5.2 3.4 0.2 1.  0.  0. ] ->  +1.46   +1.40       0.06   \n",
      "          [4.6 3.2 0.2 1.  0.  0. ] ->  +1.48   +1.40       0.08   \n",
      "          [6.  2.9 1.5 0.  1.  0. ] ->  +4.30   +4.50       0.20   \n",
      "          [6.7 3.3 2.5 0.  0.  1. ] ->  +5.65   +5.70       0.05   \n",
      "          [6.  3.4 1.6 0.  1.  0. ] ->  +4.48   +4.50       0.02   \n",
      "          [6.5 3.  1.8 0.  0.  1. ] ->  +5.19   +5.50       0.31   \n",
      "          [4.8 3.  0.1 1.  0.  0. ] ->  +1.46   +1.40       0.06   \n",
      "          [6.  2.2 1.  0.  1.  0. ] ->  +4.08   +4.00       0.08   \n",
      "          [6.7 2.5 1.8 0.  0.  1. ] ->  +5.60   +5.80       0.20   \n",
      "          [5.  3.4 0.2 1.  0.  0. ] ->  +1.47   +1.50       0.03   \n",
      "          [5.1 3.8 0.2 1.  0.  0. ] ->  +1.50   +1.60       0.10   \n",
      "          [4.8 3.4 0.2 1.  0.  0. ] ->  +1.49   +1.60       0.11   \n",
      "          [6.  2.7 1.6 0.  1.  0. ] ->  +4.21   +5.10       0.89   \n",
      "          [5.8 2.8 2.4 0.  0.  1. ] ->  +5.37   +5.10       0.27   \n",
      "          [5.7 2.5 2.  0.  0.  1. ] ->  +5.19   +5.00       0.19   \n",
      "          [5.8 2.7 1.  0.  1.  0. ] ->  +4.16   +4.10       0.06   \n",
      "          [4.6 3.6 0.2 1.  0.  0. ] ->  +1.52   +1.00       0.52   \n",
      "          [7.2 3.  1.6 0.  0.  1. ] ->  +5.67   +5.80       0.13   \n",
      "          [4.6 3.1 0.2 1.  0.  0. ] ->  +1.47   +1.50       0.03   \n",
      "          [6.5 3.  2.  0.  0.  1. ] ->  +5.34   +5.20       0.14   \n",
      "          [5.5 2.5 1.3 0.  1.  0. ] ->  +3.90   +4.00       0.10   \n",
      "          [6.4 3.2 1.5 0.  1.  0. ] ->  +4.62   +4.50       0.12   \n",
      "          [6.1 3.  1.4 0.  1.  0. ] ->  +4.40   +4.60       0.20   \n",
      "          [4.6 3.4 0.3 1.  0.  0. ] ->  +1.50   +1.40       0.10   \n",
      "          [6.4 2.8 2.2 0.  0.  1. ] ->  +5.55   +5.60       0.05   \n",
      "          [5.  3.4 0.4 1.  0.  0. ] ->  +1.46   +1.60       0.14   \n",
      "\n",
      "+++++   +++++   +++++           \n",
      "average abs diff error:   0.166 \n",
      "+++++   +++++   +++++           \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "~~~~~~~~~~~~\n",
    "Petal length\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "Here we set up for a regression model that will predict petal length  'petallen' (column index 2)\n",
    "\n",
    "\"\"\"\n",
    "print(\"PETALLEN\")\n",
    "print(\"+++ Start of data-assembly for feature-regression! +++\\n\")\n",
    "# construct the correct X_all\n",
    "X_all = np.concatenate( (A[:,0:2], A[:,3:]),axis=1)  # columns 0, 1, 3, and 4\n",
    "y_all = A[:,2]                    # y (labels) ... is all of column 2, petallen\n",
    "# print(f\"y_all is \\n {y_all}\")\n",
    "# print() \n",
    "# print(f\"X_all (the features, first few rows) is \\n {X_all[:5,:]}\")\n",
    "# print() \n",
    "# print() \n",
    "\n",
    "\n",
    "\n",
    "# Scramble the data, to give a different TRAIN/TEST split each time...\n",
    "print(\"+++ Scramble the data... +++\\n\")\n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "# print(\"target values to predict: \\n\",y_all)\n",
    "# print(\"\\nfeatures (a few)\\n\", X_all[0:5,:])\n",
    "\n",
    "\n",
    "\n",
    "# We next separate into test data and training data ... \n",
    "print(\"+++ 80/20 train-test split... +++\\n\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "# print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "# print(f\"y_test: {y_test}\\n\")\n",
    "# print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "# print()\n",
    "# print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "# print(f\"y_train: {y_train}\\n\")\n",
    "# print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows\n",
    "\n",
    "\n",
    "\n",
    "# For NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    his is done through the \"StandardScaler\" in scikit-learn\n",
    "print(\"+++ Scale values... +++\\n\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "# reused from above - seeing the scaled data \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],None)\n",
    "\n",
    "# reused from above - seeing the unscaled data (inverting the scaler)\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],scaler)\n",
    "\n",
    "# Train\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=500,          # how many training epochs\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_:<6.3f}\")\n",
    "print(f\"And, its square root:         {nn_regressor.loss_ ** 0.5:<6.3f}\")\n",
    "print()\n",
    "\n",
    "\n",
    "#\n",
    "# how did it do? Try out the TEST data...\n",
    "#\n",
    "print(\"PETALLEN TESTING RESULTS\")\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>35s} ->  {'pred':^6s}  {'des.':^6s}   {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,:]!s:>35s} ->  {pred:<+6.2f}  {desired:<+6.2f}   {result:^10.2f}\") \n",
    "\n",
    "    print(\"\\n\" + \"+++++   +++++   +++++           \")\n",
    "    print(f\"average abs diff error:   {error/len(y):<6.3f}\")\n",
    "    print(\"+++++   +++++   +++++           \")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PETALWID\n",
      "+++ Start of data-assembly for feature-regression! +++\n",
      "\n",
      "+++ Scramble the data... +++\n",
      "\n",
      "+++ 80/20 train-test split... +++\n",
      "\n",
      "training with 112 rows;  testing with 29 rows\n",
      "\n",
      "+++ Scale values... +++\n",
      "\n",
      "                                                    input  ->  pred   des  \n",
      "                                 [-1.74 -0.16 -1.39  1.4 ] ->    ?    0.2                  \n",
      "                                 [-0.91  1.77 -1.28  1.4 ] ->    ?    0.3                  \n",
      "                                 [-0.19 -1.13 -0.15 -0.72] ->    ?    1.0                  \n",
      "                                 [ 1.25  0.08  0.75 -0.72] ->    ?    2.3                  \n",
      "                                 [ 1.73 -0.4   1.42 -0.72] ->    ?    1.8                  \n",
      "\n",
      "                                                    input  ->  pred   des  \n",
      "                                         [4.4 3.  1.3 1. ] ->    ?    0.2                  \n",
      "                                         [5.1 3.8 1.5 1. ] ->    ?    0.3                  \n",
      "                                         [5.7 2.6 3.5 0. ] ->    ?    1.0                  \n",
      "                                         [6.9 3.1 5.1 0. ] ->    ?    2.3                  \n",
      "                                         [7.3 2.9 6.3 0. ] ->    ?    1.8                  \n",
      "\n",
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 0.22084685\n",
      "Iteration 2, loss = 0.29561801\n",
      "Iteration 3, loss = 0.07785366\n",
      "Iteration 4, loss = 0.10059118\n",
      "Iteration 5, loss = 0.12744159\n",
      "Iteration 6, loss = 0.10993697\n",
      "Iteration 7, loss = 0.06788200\n",
      "Iteration 8, loss = 0.04619644\n",
      "Iteration 9, loss = 0.06558294\n",
      "Iteration 10, loss = 0.07921711\n",
      "Iteration 11, loss = 0.05472215\n",
      "Iteration 12, loss = 0.03452094\n",
      "Iteration 13, loss = 0.03920679\n",
      "Iteration 14, loss = 0.04936087\n",
      "Iteration 15, loss = 0.04764434\n",
      "Iteration 16, loss = 0.03553132\n",
      "Iteration 17, loss = 0.02577122\n",
      "Iteration 18, loss = 0.02586876\n",
      "Iteration 19, loss = 0.02954114\n",
      "Iteration 20, loss = 0.02630118\n",
      "Iteration 21, loss = 0.01916537\n",
      "Iteration 22, loss = 0.01798799\n",
      "Iteration 23, loss = 0.02100606\n",
      "Iteration 24, loss = 0.02033994\n",
      "Iteration 25, loss = 0.01634093\n",
      "Iteration 26, loss = 0.01525461\n",
      "Iteration 27, loss = 0.01737608\n",
      "Iteration 28, loss = 0.01734060\n",
      "Iteration 29, loss = 0.01472360\n",
      "Iteration 30, loss = 0.01484521\n",
      "Iteration 31, loss = 0.01693818\n",
      "Iteration 32, loss = 0.01604709\n",
      "Iteration 33, loss = 0.01359856\n",
      "Iteration 34, loss = 0.01426243\n",
      "Iteration 35, loss = 0.01582689\n",
      "Iteration 36, loss = 0.01429399\n",
      "Iteration 37, loss = 0.01263268\n",
      "Iteration 38, loss = 0.01361571\n",
      "Iteration 39, loss = 0.01439069\n",
      "Iteration 40, loss = 0.01302719\n",
      "Iteration 41, loss = 0.01199428\n",
      "Iteration 42, loss = 0.01285182\n",
      "Iteration 43, loss = 0.01323344\n",
      "Iteration 44, loss = 0.01217333\n",
      "Iteration 45, loss = 0.01180678\n",
      "Iteration 46, loss = 0.01246432\n",
      "Iteration 47, loss = 0.01252716\n",
      "Iteration 48, loss = 0.01194043\n",
      "Iteration 49, loss = 0.01189083\n",
      "Iteration 50, loss = 0.01223362\n",
      "Iteration 51, loss = 0.01209688\n",
      "Iteration 52, loss = 0.01180091\n",
      "Iteration 53, loss = 0.01187984\n",
      "Iteration 54, loss = 0.01194093\n",
      "Iteration 55, loss = 0.01173067\n",
      "Iteration 56, loss = 0.01166059\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 0.012 \n",
      "And, its square root:         0.108 \n",
      "\n",
      "PETALWID TESTING RESULTS\n",
      "                             input  ->   pred    des.     absdiff  \n",
      "          [6.2 2.2 4.5 0.  1.  0. ] ->  +1.17   +1.50       0.33   \n",
      "          [5.4 3.7 1.5 1.  0.  0. ] ->  +0.25   +0.20       0.05   \n",
      "          [7.4 2.8 6.1 0.  0.  1. ] ->  +2.04   +1.90       0.14   \n",
      "          [6.5 3.  5.8 0.  0.  1. ] ->  +2.08   +2.20       0.12   \n",
      "          [6.9 3.1 4.9 0.  1.  0. ] ->  +1.54   +1.50       0.04   \n",
      "          [6.3 2.5 4.9 0.  1.  0. ] ->  +1.28   +1.50       0.22   \n",
      "          [4.4 3.2 1.3 1.  0.  0. ] ->  +0.25   +0.20       0.05   \n",
      "          [6.5 3.  5.2 0.  0.  1. ] ->  +2.08   +2.00       0.08   \n",
      "          [6.1 2.6 5.6 0.  0.  1. ] ->  +1.95   +1.40       0.55   \n",
      "          [7.7 3.  6.1 0.  0.  1. ] ->  +2.11   +2.30       0.19   \n",
      "          [5.  2.  3.5 0.  1.  0. ] ->  +1.04   +1.00       0.04   \n",
      "          [6.4 2.9 4.3 0.  1.  0. ] ->  +1.34   +1.30       0.04   \n",
      "          [5.8 2.8 5.1 0.  0.  1. ] ->  +2.00   +2.40       0.40   \n",
      "          [7.2 3.6 6.1 0.  0.  1. ] ->  +2.27   +2.50       0.23   \n",
      "          [5.8 2.7 3.9 0.  1.  0. ] ->  +1.24   +1.20       0.04   \n",
      "          [5.1 3.8 1.9 1.  0.  0. ] ->  +0.25   +0.40       0.15   \n",
      "          [6.  2.2 4.  0.  1.  0. ] ->  +1.12   +1.00       0.12   \n",
      "          [4.6 3.2 1.4 1.  0.  0. ] ->  +0.25   +0.20       0.05   \n",
      "          [6.1 3.  4.6 0.  1.  0. ] ->  +1.45   +1.40       0.05   \n",
      "          [5.  2.3 3.3 0.  1.  0. ] ->  +1.09   +1.00       0.09   \n",
      "          [4.6 3.4 1.4 1.  0.  0. ] ->  +0.25   +0.30       0.05   \n",
      "          [4.7 3.2 1.3 1.  0.  0. ] ->  +0.25   +0.20       0.05   \n",
      "          [5.3 3.7 1.5 1.  0.  0. ] ->  +0.25   +0.20       0.05   \n",
      "          [5.4 3.9 1.3 1.  0.  0. ] ->  +0.25   +0.40       0.15   \n",
      "          [6.1 3.  4.9 0.  0.  1. ] ->  +2.06   +1.80       0.26   \n",
      "          [6.3 2.8 5.1 0.  0.  1. ] ->  +2.01   +1.50       0.51   \n",
      "          [5.  3.5 1.3 1.  0.  0. ] ->  +0.25   +0.30       0.05   \n",
      "          [5.8 2.7 5.1 0.  0.  1. ] ->  +1.96   +1.90       0.06   \n",
      "          [6.7 3.1 4.4 0.  1.  0. ] ->  +1.45   +1.40       0.05   \n",
      "\n",
      "+++++   +++++   +++++           \n",
      "average abs diff error:   0.145 \n",
      "+++++   +++++   +++++           \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "~~~~~\n",
    "Petal width\n",
    "~~~~~\n",
    "\n",
    "Here we set up for a regression model that will predict petal width   'petalwid' (column index 3)\n",
    "\n",
    "\"\"\"\n",
    "print(\"PETALWID\")\n",
    "print(\"+++ Start of data-assembly for feature-regression! +++\\n\")\n",
    "# construct the correct X_all from the columns we want\n",
    "# we use np.concatenate to combine parts of the dataset to get all-except-column 0:\n",
    "#                     exclude 0  , include 1 to the end\n",
    "X_all = np.concatenate( (A[:,0:3], A[:,4:]),axis=1)  # columns 0, 1, 2, and 4 \n",
    "\n",
    "\n",
    "y_all = A[:,3]                    # y (labels) ... is all of column 0, sepallen (sepal length) \n",
    "#                                 # change the line above to make other columns the target (y_all)\n",
    "# print(f\"y_all is \\n {y_all}\")\n",
    "# print() \n",
    "# print(f\"X_all (the features, first few rows) is \\n {X_all[:5,:]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Scramble the data, to give a different TRAIN/TEST split each time...\n",
    "print(\"+++ Scramble the data... +++\\n\")\n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "# print(\"target values to predict: \\n\",y_all)\n",
    "# print(\"\\nfeatures (a few)\\n\", X_all[0:5,:])\n",
    "\n",
    "\n",
    "\n",
    "# We next separate into test data and training data ... \n",
    "print(\"+++ 80/20 train-test split... +++\\n\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "# print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "# print(f\"y_test: {y_test}\\n\")\n",
    "# print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "# print()\n",
    "# print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "# print(f\"y_train: {y_train}\\n\")\n",
    "# print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows\n",
    "\n",
    "\n",
    "\n",
    "# For NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    his is done through the \"StandardScaler\" in scikit-learn\n",
    "print(\"+++ Scale values... +++\\n\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "# reused from above - seeing the scaled data \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],None)\n",
    "\n",
    "# reused from above - seeing the unscaled data (inverting the scaler)\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],scaler)\n",
    "\n",
    "# Train\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=500,          # how many training epochs\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_:<6.3f}\")\n",
    "print(f\"And, its square root:         {nn_regressor.loss_ ** 0.5:<6.3f}\")\n",
    "print()\n",
    "\n",
    "#\n",
    "# how did it do? Try out the TEST data...\n",
    "#\n",
    "print(\"PETALWID TESTING RESULTS\")\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>35s} ->  {'pred':^6s}  {'des.':^6s}   {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        print(f\"{Xpr[i,:]!s:>35s} ->  {pred:<+6.2f}  {desired:<+6.2f}   {result:^10.2f}\") \n",
    "\n",
    "    print(\"\\n\" + \"+++++   +++++   +++++           \")\n",
    "    print(f\"average abs diff error:   {error/len(y):<6.3f}\")\n",
    "    print(\"+++++   +++++   +++++           \")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_regressor(X_test_scaled,\n",
    "                          y_test_scaled,\n",
    "                          nn_regressor,\n",
    "                          scaler)   # this is our own f'n, above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average absolute difference errors:\n",
    "+ Sepal length: 0.225\n",
    "+ Sepal width: 0.251\n",
    "+ Petal length: 0.166\n",
    "+ Petal width: 0.145\n",
    "\n",
    "This data implies that petal width is the most predictable, while sepal width is the least. I can't say I know anything about flowers, so maybe this makes sense!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
