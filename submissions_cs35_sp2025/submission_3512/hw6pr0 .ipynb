{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### cs35 Week6: Reading and response \n",
    "\n",
    "_On the Uses and Misuses of Models_   &nbsp;&nbsp; (hw6pr0.ipynb)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Reading for hw6...     (hw6pr0.ipynb)\n",
    "\n",
    "This week's reading has two options:\n",
    "+ [Option 1] Georgia Meyer's review of <u>Escape from Model Land</u>, which addresses the troubles of over-trusting models -- and provides a path for balancing the skepticism and promise of models' _\"knowledge\"_  \n",
    "  + [Here is the link to the original review](https://www.lse.ac.uk/DSI/Research/Blog-posts/Book-review-Escape-from-Model-Land) &nbsp;&nbsp; and &nbsp;&nbsp; [here is a local copy](https://drive.google.com/file/d/1SCuPWPyHEQ2N5eycg48pcV6Rkg8CLzSe/view?usp=drive_link), &nbsp;&nbsp; just in case <br><br>\n",
    "+ [Option 2] Kate Harbath's short history of Cambridge Analytica, perhaps the most costly - and expensive - example of _modeling misuse_\n",
    "  + [Here is the link to the original article](https://bipartisanpolicy.org/blog/cambridge-analytica-controversy/#) &nbsp;&nbsp; and &nbsp;&nbsp; [here is a local copy](https://drive.google.com/file/d/1k0DeDBH0EBdVfApY1O205FXMDbsashzj/view?usp=drive_link), &nbsp;&nbsp; just in case <br><br>\n",
    "+ [Option 1+2] Feel free to read and respond to both (optional and ec, up to +10)\n",
    "\n",
    "\n",
    "#### The prompt(s)\n",
    "\n",
    "Using the article you choose - and your own experience - what are your thoughts on the ***trustworthiness*** of the models that modern approaches can and have created?   \n",
    "\n",
    "Possible jumping-off points include your thoughts on ...\n",
    "+ (1) the responsibility (and accountability) of the humans who **design and create** models. How should the source data affect models' scope and use?  For example, CA's models used _social media scraping_ ... <br><br>\n",
    "+ (2) the responsibility (and accountability) of the humans who **deploy and use** models. To what extent does it matter what the model is a model ***of*** ? For example, CA's models were models _of people_ ... <br><br>\n",
    "+ (3) an example you've encountered where an artificially-learned model was mis-deployed. This could be a non-artificially-learned model, for that matter! Was the responsibility for mis-deployment focused/individually-based? or was it diffuse/community-based? \n",
    "\n",
    "<br>\n",
    "\n",
    "Alternative directions on the tensions between model-trust and human-trust are more than welcome!  \n",
    "\n",
    "As with each week's reading, responses should be thoughtful, but need not be CS35_Participant_2: a 4-5 sentence paragraph is wonderful.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading response\n",
    "\n",
    "I definitely agree with the \"book review: escape from model land article\", that models should always be approached with a level of distrust, such that they are never complete representations of what is going on. I think in that sense, there is a lot of responsibility towards how the model is used. A model can't be used without acknowledging its purpose and limitations, and its findings/predictions should be communicated with that same degree of doubt. I feel like the closest analogy to my life is the usage of simplified models in early science education. It can sometimes be misleading to learn heavily simplified versions of complex topics. Though that simiplifcation is unavoidable, I think that communicating the purpose of a certain kind of model, and especially emphasizing that this model does not carry over to issues beyond its purpose, are important in its deployment.\n",
    "\n",
    "As for the issues that arose from the CA situation, it certainly feels like a bad example of modelling in that somewhere in the process there was an ethical failure. Pinpointing that ethics breach is a little complicated to me. Sites like facebook can track and monitor your interactions and behaviors just by virtue of being a digital environment, so perhaps you should already know/assume that your data is being stored/used to manipulate your digital spaces. Furthermore, if you're consenting to facebook's algorithm pushing content you like (which can create political echo chambers regardless) how is that much better than your data being used by CA? I guess the main response to this is that in an age that's so reliant on digital communication, it's not realistic to just \"opt out\", and it's not ethical to have to choose between all the benefits of this new-age global town hall, or protecting yourself from being exploited for your own manipulation."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ccb4bb6bd67730c9185e6c24c983362cd7b4575b595bfae100d8d91e48f4f1e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
