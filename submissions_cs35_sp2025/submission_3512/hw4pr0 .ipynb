{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### week4 ~ <i>... and into the data we dive ...</i>  &nbsp;&nbsp; (hw4pr0.ipynb)\n",
    "\n",
    "<a href=\"https://docs.google.com/document/d/1OdQ-01Gp7XAk9vbZ61zM3CaTdGsZVWEvZxgu1Z0toSY/edit?tab=t.0\">This week's hw page</a>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Reading for hw4...     (hw4pr0.ipynb)\n",
    "\n",
    "As we transition into AI/Machine Learning for the next few weeks, this week's reading takes a more <u>policy-based</u>, rather than socially-normed, view:\n",
    "+ it's a NYTimes article from a few years ago -- nice, because it has a definite, wide-angle stance on regulating AI:\n",
    "+ [How to Regulate Artificial Intelligence](https://www.nytimes.com/2017/09/01/opinion/artificial-intelligence-regulations-rules.html)\n",
    "+ [locally-hosted pdf copy](https://drive.google.com/file/d/1EZcygQdk40J0dJTZ1vp0F20rIoQU27nH/view)\n",
    "\n",
    "Consider the author's three proposed principles, which elaborate Isaac Asimov's \"laws\":\n",
    "+ AIs must obey human society's laws\n",
    "+ AIs must disclose themselves as such\n",
    "+ AIs cannot retain confidential information (w/o permission)\n",
    "\n",
    "Choose one (or more) of these principles with which to agree or disagree, bringing in your own experience and perspective. \n",
    "\n",
    "Alternative paths and balancing acts are, as always, welcome! For example -- this article was written before conversational AI was a reality. Do you feel there are differences in the appropriate principles or guidance for regulating today's AI agents and their authors?\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading response\n",
    "I agree with the first law, and out of the three it was initially the least obvious to me. However, it makes a lot of sense, especially in connection with our exploration of credit-blame in terms of AI. I first thought of the AI as its own actor, and its inability to know right from wrong, but then I realized that that's exactly why this rule is so necessary. It's not really for regulating the AI as an entity, but the author behind the AI, and ensuring that they don't use AI illegally and place all the blame on something incapable of knowing right/wrong. It does make me wonder how to practically implement censorship of AI to combat, for example, the crime of inciting an imminent lawless action. That's a crime that's somewhat difficult to judge when it involves human actors, but to what extent does the (mis)information an AI feeds an individual responsible for the crime that individual commits? What kind of filters block inciting statements, while keeping informational content on controversial topics? Perhaps training the model only on objective, vetted information (ex: journals/papers) rather than conversational/www scraped content could minimize the contirbution of AI towards that sort of crime."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ccb4bb6bd67730c9185e6c24c983362cd7b4575b595bfae100d8d91e48f4f1e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
