<br>

###   hw7pr1iris_modeler 

Here, you'll epxlore iris clasification <font color="Coral">and regression</font> via NNets

<font color="Coral"> _Regression_ </font> - estimating a floating-point value - is NNets' natural operation.

<br>

_Classification_ - estimating a discrete label - is done by creating discrete variables (we can't let the library treat a categorical variable as if it were continuous). <br> We'll start there, since it's a variation of our past workflows:
### Reading in our data

previously cleaned...### We have full access to individual data elements

Checking on our observations and attributes### Defining X_all (our features) and y_all (our target to predict) ### Permute...### Separate into training and testing
+ X_train, y_train will be used for model-building
+ X_test, y_test will be used for testing, later### NNets often need a ***Scaler***

This scales all of the data so that it's of a similar size, e.g., -1 to 1

Notice that this cell produces
+ X_train_scaled &nbsp; (the scaled training data)
   + y_train_scaled is the same as y_train &nbsp; (no scaling needed for the target)
+ X_test_scaled &nbsp; (the scaled testing data)
   + y_test_scaled is the same as y_test &nbsp; (no scaling needed for the target)

After this, we just use the scaled data...### Intuition-building for the Scaled data### Training ML models is no problem

that is, if the library does it for us! &nbsp;&nbsp; üòÉ üß∂ ‚õÑÔ∏è üåä ü™∫ ü¶î :-)### Now, let's test!### Let's see the network's weights...### The predictive model<br>

#### We _could_ use cross-validation to find the "right"/"best" size and shape of the NNet...

<font color="Coral">But, let's not...</font>

### <font color="DodgerBlue">Instead, we'll explore two new directions:</font> 
+ regression and 
+ feature-predictability

#### The next example...
+ is one in which we estimate a botanical feature ... 
+ ... from the other four columns -- including the species!### Permute!

<font size="-2">I like this because it adds an element of randomness to the whole process...</font>### Split into training and testing sets

We'll stick with 80% training, 20% testing...### It's a nnet - this time, we _should_ use a <u>Scaler</u>

<font size="-2">Note that, for our <u>own</u> neural nets, this is performed by our senses:
+ our sense of hearing scales sounds from whispers to shouts... 
    + ... into a single amplitude for interpretation. 
    + and, we keep the scale itself as an additional signal!
+ similarly, our visual system scales across many orders of magnitude of brightness
    + plus, our eyes can "adapt" to low light (and, less, to bright-light )
+ less dramatically, for taste, touch, and smell...
+ this scaling ### Create and train the model

Note that hidden_layer_sizes is an important parameter.
+ for this hw, we're simply trying things out
+ no need to optimize via cross-validation### Let's test!  

Testing is different for regression!  
<font size="-1">
+ it's floating-point, so it's no longer "right" or "wrong"
+ instead, the _difference_ is what matters
+ in fact, the <u> _absolute difference_ </u> is usually what matters: </font>### Your task:  &nbsp;&nbsp; model ***all*** of the iris features!

Which of the four floating-point iris features is
+ _most_ predictable? &nbsp; (smallest average abs diff error)
+ _least_ predictable? &nbsp; (largest average abs diff error)

<br>

As with the _feature importances_ , it's often the <u>insight</u> we value, more than creating the predictor itself...#### Average absolute difference errors:
+ Sepal length: 0.225
+ Sepal width: 0.251
+ Petal length: 0.166
+ Petal width: 0.145

This data implies that petal width is the most predictable, while sepal width is the least. I can't say I know anything about flowers, so maybe this makes sense!