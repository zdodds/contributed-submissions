<br>

#### week4 ~ <i>... and into the data we dive ...</i>  &nbsp;&nbsp; (hw4pr0.ipynb)

<a href="https://docs.google.com/document/d/1OdQ-01Gp7XAk9vbZ61zM3CaTdGsZVWEvZxgu1Z0toSY/edit?tab=t.0">This week's hw page</a>

<hr><br>

#### Reading for hw4...     (hw4pr0.ipynb)

As we transition into AI/Machine Learning for the next few weeks, this week's reading takes a more <u>policy-based</u>, rather than socially-normed, view:
+ it's a NYTimes article from a few years ago -- nice, because it has a definite, wide-angle stance on regulating AI:
+ [How to Regulate Artificial Intelligence](https://www.nytimes.com/2017/09/01/opinion/artificial-intelligence-regulations-rules.html)
+ [locally-hosted pdf copy](https://drive.google.com/file/d/1EZcygQdk40J0dJTZ1vp0F20rIoQU27nH/view)

Consider the author's three proposed principles, which elaborate Isaac Asimov's "laws":
+ AIs must obey human society's laws
+ AIs must disclose themselves as such
+ AIs cannot retain confidential information (w/o permission)

Choose one (or more) of these principles with which to agree or disagree, bringing in your own experience and perspective. 

Alternative paths and balancing acts are, as always, welcome! For example -- this article was written before conversational AI was a reality. Do you feel there are differences in the appropriate principles or guidance for regulating today's AI agents and their authors?

<hr>#### Reading response

(Feel free to use this cell for your response.)I partially agree with the first and third statements, and I fully agree with the second.

First Statement 
“Human society’s laws” is a very broad concept. In some countries, strict censorship is part of the legal system, which can hinder AI development or usage because certain words or topics generated by AI might be deemed “threatening” under those laws. However, if we interpret this requirement to mean AIs must obey fundamental ethical and just principles, then it’s important to define clear boundaries that AI systems must not cross.

Second Statement 
I agree with this wholeheartedly. On social media, there are many AI-generated videos that aren’t labeled as AI creations. Some of these are so realistic that they can cause panic or spread misinformation. If AI content is labeled as such, people will be better informed and can critically assess the source, thereby reducing the spread of misleading information.

Third Statement 
My feelings here are mixed. I don’t fully understand how AI processes confidential information, but at this stage, it seems that giving confidential information to an AI might actually be safer than giving it to a human, since AI is (at least for now) more directly controlled, whereas human psychological factors are unpredictable. AI could also be useful for managing confidential data—hospitals, for example, might employ AI to organize patient insurance details and medical records more efficiently. However, if AI truly develop an independent agency or “mind” beyond human control some day in the future, we would need to revisit how it handles sensitive information. That possibility makes me cautious about fully entrusting confidential data to AI.