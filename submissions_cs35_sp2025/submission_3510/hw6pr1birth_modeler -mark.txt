#### Births Modeler: &nbsp; Using Decision Trees and Random Forests

+ births clasification via decision trees and random forests...
+ You're classifying "above" (above-median popularity of birthdays) vs "below" (below-median popularity of birthdays)
+ Feel free to re-use the cleaned data from previous weeks (already here)
+ Suggestion:  &nbsp; go cell-by-cell using the iris_modeler example, copying-and-adapting as you go...## Data set up...
+ Use the cleaned data from prior weeks (it should be here in this folder as births_cleaned.csv)
+ Be sure to get rid of the column with the **number of births** &nbsp;&nbsp; That makes the prediction "too easy"!
+ Instead, see how well the DT/RF techniques can predict above/below-median popularity, just based on month and day...
+ (It's quite a hard estimation problem!)## First: &nbsp; _Decision Trees_### Drop the columns above/below median and  a/b num### Nice to have variables for everything

just in case...### Convert to a numpy array### We have access to each data element 

and and all of its attributes### We define the features (X_all) and the target to be predicted (y_all)### Permute!### Create an 80/20 train/test split of the data<br>

#### Here is where the model-buidling begins in earnest...## Then: &nbsp; _Random Forests_## First: &nbsp; _Decision Trees_<br>

#### That's it! :-)

Let's see how it did...It seems that month is the feature the DT has chosenPlot the tree by matplotlib### Cross-validate to find the "best" tree-depth ### Let's see the deeper tree### Feature importances

This is a wonderful _feature_ of decision trees:### Feature importances, final version...### Building a Random Forest is as smooth as the others:

RFs need two parameters
+ the best depth (max_depth)
+ the number of trees (n_estimators)### Let's test!### Let's see one of the forest's trees...### Cross validation over _two_ parameters### Build a tuned model

... with the new parameter values### Let's test the tuned model### We build a final model...

+ using all of the data
+ using our best-tuned parameters ### Feature importances ...

... tend to be even more meaningful when tapped from Random Forests

This is because the "wisdom of the crowd" can emerge:### We can visualize parameter space

for any model:<br>
<hr>
<br>

### Final, big-picture question:  
+ What are the relative "feature importances" of the two features
  + month
  + day
+ ... at least, in terms of "birthday popularity," as we've defined it?

**Month plays a more important row than day. I guess because month gives you the general pattern while the variances in days are too unpredicatable.**

Reflect, in a sentence or two, on the values of feature importances that you found.
+ To use Erica Thompson's framing, 
+ how much trust (or weight) would you attribute to this model and its feature importances, "outside of Model Land"? 

**I may attribute half of the feature importances "outside of model land" because giving births is a very subjective thing decided by the couples themselves, and this decision can change by year due to many reasons like political reasons and economic reasons. This model can only "study" the trend in the birth variations for those years when this form is made, but it may not give a good prediction about the birth trends in the future years**