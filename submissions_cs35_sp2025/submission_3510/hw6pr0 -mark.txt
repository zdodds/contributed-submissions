<br>

### cs35 Week6: Reading and response 

_On the Uses and Misuses of Models_   &nbsp;&nbsp; (hw6pr0.ipynb)

<hr><br>

#### Reading for hw6...     (hw6pr0.ipynb)

This week's reading has two options:
+ [Option 1] Georgia Meyer's review of <u>Escape from Model Land</u>, which addresses the troubles of over-trusting models -- and provides a path for balancing the skepticism and promise of models' _"knowledge"_  
  + [Here is the link to the original review](https://www.lse.ac.uk/DSI/Research/Blog-posts/Book-review-Escape-from-Model-Land) &nbsp;&nbsp; and &nbsp;&nbsp; [here is a local copy](https://drive.google.com/file/d/1SCuPWPyHEQ2N5eycg48pcV6Rkg8CLzSe/view?usp=drive_link), &nbsp;&nbsp; just in case <br><br>
+ [Option 2] Kate Harbath's short history of Cambridge Analytica, perhaps the most costly - and expensive - example of _modeling misuse_
  + [Here is the link to the original article](https://bipartisanpolicy.org/blog/cambridge-analytica-controversy/#) &nbsp;&nbsp; and &nbsp;&nbsp; [here is a local copy](https://drive.google.com/file/d/1k0DeDBH0EBdVfApY1O205FXMDbsashzj/view?usp=drive_link), &nbsp;&nbsp; just in case <br><br>
+ [Option 1+2] Feel free to read and respond to both (optional and ec, up to +10)


#### The prompt(s)

Using the article you choose - and your own experience - what are your thoughts on the ***trustworthiness*** of the models that modern approaches can and have created?   

Possible jumping-off points include your thoughts on ...
+ (1) the responsibility (and accountability) of the humans who **design and create** models. How should the source data affect models' scope and use?  For example, CA's models used _social media scraping_ ... <br><br>

**I think that there should be more transparency for the design of the models. For example, in the example of facebook from reading 2, the users had no idea of what the model of facebook was doing, and they became the puppets of facebook unintentionally. I think the model designer should make the idea of their model publically available, even not making all the ideas tranparently. At the same time, I hope that more technicians could explain what those models are doing to ordinary people. The model should be subjected to public review so that people could discover possible privacy leakage and require the model designer or the company to fix it. The model itself or the company using this model shouldn't be some "superior" power above ordinary people.**

+ (2) the responsibility (and accountability) of the humans who **deploy and use** models. To what extent does it matter what the model is a model ***of*** ? For example, CA's models were models _of people_ ... <br><br>
+ (3) an example you've encountered where an artificially-learned model was mis-deployed. This could be a non-artificially-learned model, for that matter! Was the responsibility for mis-deployment focused/individually-based? or was it diffuse/community-based? 

<br>

Alternative directions on the tensions between model-trust and human-trust are more than welcome!  

As with each week's reading, responses should be thoughtful, but need not be CS35_Participant_2: a 4-5 sentence paragraph is wonderful.

<br>
<br>
<hr>
<br>
<br>#### Reading response

Feel free to use this cell for your response(s).

<br>
<br>





<br>
<br>

<br>
<br>

<br>
<br>

<hr>

<br>