{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "###  <font color=\"Coral\">hw8pr1digits_dreamer</font>   \n",
    "\n",
    "For this hw8pr1digits_dreamer challenge, you'll leverage how NNets are the most capable model for imputing - or \"hallucinating\" - or _generating_ missing data \n",
    "\n",
    "Here, we'll put them to the test, to <font color=\"DodgerBlue\"><b>dream</b></font> the unseen portion of a digit, only part of which is visible:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Here, we have a one-pixel predictor, to get you started..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries!\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits.csv : file read into a pandas dataframe.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix0</th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>...</th>\n",
       "      <th>pix56</th>\n",
       "      <th>pix57</th>\n",
       "      <th>pix58</th>\n",
       "      <th>pix59</th>\n",
       "      <th>pix60</th>\n",
       "      <th>pix61</th>\n",
       "      <th>pix62</th>\n",
       "      <th>pix63</th>\n",
       "      <th>actual_digit</th>\n",
       "      <th>excerpted from http://yann.lecun.com/exdb/mnist/</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1768 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pix0  pix1  pix2  pix3  pix4  pix5  pix6  pix7  pix8  pix9  ...  pix56  \\\n",
       "0        0     0     9    14     8     1     0     0     0     0  ...      0   \n",
       "1        0     0    11    12     0     0     0     0     0     2  ...      0   \n",
       "2        0     0     1     9    15    11     0     0     0     0  ...      0   \n",
       "3        0     0     0     0    14    13     1     0     0     0  ...      0   \n",
       "4        0     0     5    12     1     0     0     0     0     0  ...      0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "1763     0     0     4    10    13     6     0     0     0     1  ...      0   \n",
       "1764     0     0     6    16    13    11     1     0     0     0  ...      0   \n",
       "1765     0     0     1    11    15     1     0     0     0     0  ...      0   \n",
       "1766     0     0     2    10     7     0     0     0     0     0  ...      0   \n",
       "1767     0     0    10    14     8     1     0     0     0     2  ...      0   \n",
       "\n",
       "      pix57  pix58  pix59  pix60  pix61  pix62  pix63  actual_digit  \\\n",
       "0         0     11     16     15     11      1      0             8   \n",
       "1         0      9     12     13      3      0      0             9   \n",
       "2         0      1     10     13      3      0      0             0   \n",
       "3         0      0      1     13     16      1      0             1   \n",
       "4         0      3     11      8     13     12      4             2   \n",
       "...     ...    ...    ...    ...    ...    ...    ...           ...   \n",
       "1763      0      2     14     15      9      0      0             9   \n",
       "1764      0      6     16     14      6      0      0             0   \n",
       "1765      0      2      9     13      6      0      0             8   \n",
       "1766      0      5     12     16     12      0      0             9   \n",
       "1767      1      8     12     14     12      1      0             8   \n",
       "\n",
       "      excerpted from http://yann.lecun.com/exdb/mnist/  \n",
       "0                                                  NaN  \n",
       "1                                                  NaN  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "...                                                ...  \n",
       "1763                                               NaN  \n",
       "1764                                               NaN  \n",
       "1765                                               NaN  \n",
       "1766                                               NaN  \n",
       "1767                                               NaN  \n",
       "\n",
       "[1768 rows x 66 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's read in our digits data...\n",
    "# \n",
    "# for read_csv, use header=0 when row 0 is a header row\n",
    "# \n",
    "filename = 'digits.csv'\n",
    "df = pd.read_csv(filename, header=0)   # encoding=\"utf-8\" et al.\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1768 entries, 0 to 1767\n",
      "Data columns (total 65 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   pix0          1768 non-null   int64\n",
      " 1   pix1          1768 non-null   int64\n",
      " 2   pix2          1768 non-null   int64\n",
      " 3   pix3          1768 non-null   int64\n",
      " 4   pix4          1768 non-null   int64\n",
      " 5   pix5          1768 non-null   int64\n",
      " 6   pix6          1768 non-null   int64\n",
      " 7   pix7          1768 non-null   int64\n",
      " 8   pix8          1768 non-null   int64\n",
      " 9   pix9          1768 non-null   int64\n",
      " 10  pix10         1768 non-null   int64\n",
      " 11  pix11         1768 non-null   int64\n",
      " 12  pix12         1768 non-null   int64\n",
      " 13  pix13         1768 non-null   int64\n",
      " 14  pix14         1768 non-null   int64\n",
      " 15  pix15         1768 non-null   int64\n",
      " 16  pix16         1768 non-null   int64\n",
      " 17  pix17         1768 non-null   int64\n",
      " 18  pix18         1768 non-null   int64\n",
      " 19  pix19         1768 non-null   int64\n",
      " 20  pix20         1768 non-null   int64\n",
      " 21  pix21         1768 non-null   int64\n",
      " 22  pix22         1768 non-null   int64\n",
      " 23  pix23         1768 non-null   int64\n",
      " 24  pix24         1768 non-null   int64\n",
      " 25  pix25         1768 non-null   int64\n",
      " 26  pix26         1768 non-null   int64\n",
      " 27  pix27         1768 non-null   int64\n",
      " 28  pix28         1768 non-null   int64\n",
      " 29  pix29         1768 non-null   int64\n",
      " 30  pix30         1768 non-null   int64\n",
      " 31  pix31         1768 non-null   int64\n",
      " 32  pix32         1768 non-null   int64\n",
      " 33  pix33         1768 non-null   int64\n",
      " 34  pix34         1768 non-null   int64\n",
      " 35  pix35         1768 non-null   int64\n",
      " 36  pix36         1768 non-null   int64\n",
      " 37  pix37         1768 non-null   int64\n",
      " 38  pix38         1768 non-null   int64\n",
      " 39  pix39         1768 non-null   int64\n",
      " 40  pix40         1768 non-null   int64\n",
      " 41  pix41         1768 non-null   int64\n",
      " 42  pix42         1768 non-null   int64\n",
      " 43  pix43         1768 non-null   int64\n",
      " 44  pix44         1768 non-null   int64\n",
      " 45  pix45         1768 non-null   int64\n",
      " 46  pix46         1768 non-null   int64\n",
      " 47  pix47         1768 non-null   int64\n",
      " 48  pix48         1768 non-null   int64\n",
      " 49  pix49         1768 non-null   int64\n",
      " 50  pix50         1768 non-null   int64\n",
      " 51  pix51         1768 non-null   int64\n",
      " 52  pix52         1768 non-null   int64\n",
      " 53  pix53         1768 non-null   int64\n",
      " 54  pix54         1768 non-null   int64\n",
      " 55  pix55         1768 non-null   int64\n",
      " 56  pix56         1768 non-null   int64\n",
      " 57  pix57         1768 non-null   int64\n",
      " 58  pix58         1768 non-null   int64\n",
      " 59  pix59         1768 non-null   int64\n",
      " 60  pix60         1768 non-null   int64\n",
      " 61  pix61         1768 non-null   int64\n",
      " 62  pix62         1768 non-null   int64\n",
      " 63  pix63         1768 non-null   int64\n",
      " 64  actual_digit  1768 non-null   int64\n",
      "dtypes: int64(65)\n",
      "memory usage: 897.9 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix0</th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>...</th>\n",
       "      <th>pix56</th>\n",
       "      <th>pix57</th>\n",
       "      <th>pix58</th>\n",
       "      <th>pix59</th>\n",
       "      <th>pix60</th>\n",
       "      <th>pix61</th>\n",
       "      <th>pix62</th>\n",
       "      <th>pix63</th>\n",
       "      <th>actual_digit</th>\n",
       "      <th>excerpted from http://yann.lecun.com/exdb/mnist/</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1768 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pix0  pix1  pix2  pix3  pix4  pix5  pix6  pix7  pix8  pix9  ...  pix56  \\\n",
       "0        0     0     9    14     8     1     0     0     0     0  ...      0   \n",
       "1        0     0    11    12     0     0     0     0     0     2  ...      0   \n",
       "2        0     0     1     9    15    11     0     0     0     0  ...      0   \n",
       "3        0     0     0     0    14    13     1     0     0     0  ...      0   \n",
       "4        0     0     5    12     1     0     0     0     0     0  ...      0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "1763     0     0     4    10    13     6     0     0     0     1  ...      0   \n",
       "1764     0     0     6    16    13    11     1     0     0     0  ...      0   \n",
       "1765     0     0     1    11    15     1     0     0     0     0  ...      0   \n",
       "1766     0     0     2    10     7     0     0     0     0     0  ...      0   \n",
       "1767     0     0    10    14     8     1     0     0     0     2  ...      0   \n",
       "\n",
       "      pix57  pix58  pix59  pix60  pix61  pix62  pix63  actual_digit  \\\n",
       "0         0     11     16     15     11      1      0             8   \n",
       "1         0      9     12     13      3      0      0             9   \n",
       "2         0      1     10     13      3      0      0             0   \n",
       "3         0      0      1     13     16      1      0             1   \n",
       "4         0      3     11      8     13     12      4             2   \n",
       "...     ...    ...    ...    ...    ...    ...    ...           ...   \n",
       "1763      0      2     14     15      9      0      0             9   \n",
       "1764      0      6     16     14      6      0      0             0   \n",
       "1765      0      2      9     13      6      0      0             8   \n",
       "1766      0      5     12     16     12      0      0             9   \n",
       "1767      1      8     12     14     12      1      0             8   \n",
       "\n",
       "      excerpted from http://yann.lecun.com/exdb/mnist/  \n",
       "0                                                  NaN  \n",
       "1                                                  NaN  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "...                                                ...  \n",
       "1763                                               NaN  \n",
       "1764                                               NaN  \n",
       "1765                                               NaN  \n",
       "1766                                               NaN  \n",
       "1767                                               NaN  \n",
       "\n",
       "[1768 rows x 66 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# let's drop that last column (dropping is usually by _name_):\n",
    "#\n",
    "#   if you want a list of the column names use df.columns\n",
    "coltodrop = df.columns[65]     # get last column name (with the url)\n",
    "df_clean = df.drop(columns=[coltodrop])  # drop by name is typical\n",
    "df_clean.info()                         # should be happier!\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS: Index(['pix0', 'pix1', 'pix2', 'pix3', 'pix4', 'pix5', 'pix6', 'pix7', 'pix8',\n",
      "       'pix9', 'pix10', 'pix11', 'pix12', 'pix13', 'pix14', 'pix15', 'pix16',\n",
      "       'pix17', 'pix18', 'pix19', 'pix20', 'pix21', 'pix22', 'pix23', 'pix24',\n",
      "       'pix25', 'pix26', 'pix27', 'pix28', 'pix29', 'pix30', 'pix31', 'pix32',\n",
      "       'pix33', 'pix34', 'pix35', 'pix36', 'pix37', 'pix38', 'pix39', 'pix40',\n",
      "       'pix41', 'pix42', 'pix43', 'pix44', 'pix45', 'pix46', 'pix47', 'pix48',\n",
      "       'pix49', 'pix50', 'pix51', 'pix52', 'pix53', 'pix54', 'pix55', 'pix56',\n",
      "       'pix57', 'pix58', 'pix59', 'pix60', 'pix61', 'pix62', 'pix63',\n",
      "       'actual_digit'],\n",
      "      dtype='object')\n",
      "COL_INDEX: {'pix0': 0, 'pix1': 1, 'pix2': 2, 'pix3': 3, 'pix4': 4, 'pix5': 5, 'pix6': 6, 'pix7': 7, 'pix8': 8, 'pix9': 9, 'pix10': 10, 'pix11': 11, 'pix12': 12, 'pix13': 13, 'pix14': 14, 'pix15': 15, 'pix16': 16, 'pix17': 17, 'pix18': 18, 'pix19': 19, 'pix20': 20, 'pix21': 21, 'pix22': 22, 'pix23': 23, 'pix24': 24, 'pix25': 25, 'pix26': 26, 'pix27': 27, 'pix28': 28, 'pix29': 29, 'pix30': 30, 'pix31': 31, 'pix32': 32, 'pix33': 33, 'pix34': 34, 'pix35': 35, 'pix36': 36, 'pix37': 37, 'pix38': 38, 'pix39': 39, 'pix40': 40, 'pix41': 41, 'pix42': 42, 'pix43': 43, 'pix44': 44, 'pix45': 45, 'pix46': 46, 'pix47': 47, 'pix48': 48, 'pix49': 49, 'pix50': 50, 'pix51': 51, 'pix52': 52, 'pix53': 53, 'pix54': 54, 'pix55': 55, 'pix56': 56, 'pix57': 57, 'pix58': 58, 'pix59': 59, 'pix60': 60, 'pix61': 61, 'pix62': 62, 'pix63': 63, 'actual_digit': 64}\n",
      "SPECIES: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "SPECIES_INDEX: {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df_clean.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS: {COLUMNS}\")  \n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX: {COL_INDEX}\")\n",
    "\n",
    "# and for our \"SPECIES\"!\n",
    "SPECIES = [ str(i) for i in range(0,10) ]  # list with a string at each index (index -> string)\n",
    "SPECIES_INDEX = { s:int(s) for s in SPECIES }  # dictionary mapping from string -> index\n",
    "\n",
    "# and our \"target labels\"\n",
    "print(f\"SPECIES: {SPECIES}\")  \n",
    "print(f\"SPECIES_INDEX: {SPECIES_INDEX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A's shape is (1768, 65)\n",
      "[[ 0.  0.  9. ...  1.  0.  8.]\n",
      " [ 0.  0. 11. ...  0.  0.  9.]\n",
      " [ 0.  0.  1. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  0.  0.  8.]\n",
      " [ 0.  0.  2. ...  0.  0.  9.]\n",
      " [ 0.  0. 10. ...  1.  0.  8.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's convert our dataframe to a numpy array, named A\n",
    "#    Our ML library, scikit-learn operates entirely on numpy arrays.\n",
    "#\n",
    "A = df_clean.to_numpy()    # .values gets the numpy array\n",
    "A = A.astype('float64')  # so many:  www.tutorialspoint.com/numpy/numpy_data_types.htm\n",
    "print(f\"A's shape is {A.shape}\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# You will explore a different direction: \"hallucinating\" new data!\n",
    "#      This is sometimes called \"imputing\" missing data.\n",
    "#\n",
    "\n",
    "# First, build a regressor that\n",
    "#      + uses the first 48 pixels (6 image rows) to predict the floating-point value of pix52\n",
    "#      + we'll see how accurate it is...\n",
    "#      + then, you'll expand this process to build a regressor for _each_ pixel indexed from 48-63\n",
    "#      + and use those to \"imagine\" the bottom two rows of the digits..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# some starting code is provided here...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of regression prediction of pix52! +++\n",
      "\n",
      "y_all (just target values, pix52)   is \n",
      " [ 3.  9. 10. ... 15.  7.  8.]\n",
      "X_all (just features: 3 rows) is \n",
      " [[ 0.  0.  9. 14.  8.  1.  0.  0.  0.  0. 12. 14. 14. 12.  0.  0.  0.  0.\n",
      "   9. 10.  0. 15.  4.  0.  0.  0.  3. 16. 12. 14.  2.  0.  0.  0.  4. 16.\n",
      "  16.  2.  0.  0.  0.  3. 16.  8. 10. 13.  2.  0.]\n",
      " [ 0.  0. 11. 12.  0.  0.  0.  0.  0.  2. 16. 16. 16. 13.  0.  0.  0.  3.\n",
      "  16. 12. 10. 14.  0.  0.  0.  1. 16.  1. 12. 15.  0.  0.  0.  0. 13. 16.\n",
      "   9. 15.  2.  0.  0.  0.  0.  3.  0.  9. 11.  0.]\n",
      " [ 0.  0.  1.  9. 15. 11.  0.  0.  0.  0. 11. 16.  8. 14.  6.  0.  0.  2.\n",
      "  16. 10.  0.  9.  9.  0.  0.  1. 16.  4.  0.  8.  8.  0.  0.  4. 16.  4.\n",
      "   0.  8.  8.  0.  0.  1. 16.  5.  1. 11.  3.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# regression model that uses as input the first 48 pixels (pix0 to pix47)\n",
    "#                       and, as output, predicts the value of pix52\n",
    "#\n",
    "\n",
    "print(\"+++ Start of regression prediction of pix52! +++\\n\")\n",
    "\n",
    "X_all = A[:,0:48]  ### old: np.concatenate( (A[:,0:3], A[:,4:]),axis=1)  # horizontal concatenation\n",
    "y_all = A[:,52]    # y (labels) ... is all rows, column indexed 52 (pix52) only (actually the 53rd pixel, but ok)\n",
    "\n",
    "print(f\"y_all (just target values, pix52)   is \\n {y_all}\") \n",
    "print(f\"X_all (just features: 3 rows) is \\n {X_all[:3,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels (target)\n",
      " [16.  9.  6. ... 13. 16. 16.]\n",
      "features\n",
      " [[ 0.  0.  0. 10. 14.  0.  0.  0.  0.  0.  1. 16. 10.  0.  0.  0.  0.  0.\n",
      "  10. 16.  1.  0.  0.  0.  0.  4. 16.  8.  0.  3.  5.  0.  0. 10. 15.  0.\n",
      "   2. 15. 10.  0.  0. 12. 16. 14. 16. 13.  1.  0.]\n",
      " [ 0.  2. 11. 14. 14.  9.  0.  0.  0.  3. 10.  7. 10. 16.  3.  0.  0.  0.\n",
      "   0.  4. 13. 12.  0.  0.  0.  0.  0. 13. 15.  2.  0.  0.  0.  0.  0. 15.\n",
      "   9.  0.  0.  0.  0.  0.  0.  9. 15.  0.  0.  0.]\n",
      " [ 0.  0. 11. 16. 12.  2.  0.  0.  0.  7. 16.  6. 10. 13.  0.  0.  0.  0.\n",
      "   2.  0.  3. 16.  0.  0.  0.  0.  0.  3. 12.  9.  0.  0.  0.  0.  0. 10.\n",
      "  16. 12.  0.  0.  0.  0.  3.  0.  3. 15.  7.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(\"labels (target)\\n\",y_all)\n",
    "print(\"features\\n\", X_all[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 1414 rows;  testing with 354 rows\n",
      "\n",
      "Held-out data... (testing data: 354)\n",
      "y_test: [16. 14. 16. 10. 13. 16. 15. 16.  4. 12. 14.  3.  0.  8. 16.  3. 11. 10.\n",
      " 10. 16. 14.  8. 12. 16. 16. 15.  2.  2. 16. 11.  1. 10.  8.  1. 15.  6.\n",
      "  4.  0. 16.  4.  1.  2.  4.  1. 14.  5.  2.  3. 13.  4. 16.  7. 10.  6.\n",
      " 12.  0.  0. 12. 16. 10. 12.  2.  6.  0. 12. 16. 16. 13.  5. 10.  8. 14.\n",
      " 12.  1.  8.  3.  9. 16. 11. 16.  7.  0. 13.  2.  0. 10.  1. 10. 15.  0.\n",
      " 10.  8. 16. 16. 16. 16. 10. 16.  5. 13.  9. 14.  7.  6.  5.  7. 16. 15.\n",
      "  1. 10. 12.  1.  0. 15. 15. 15. 16. 14. 16.  2. 15.  4. 15. 10. 11. 16.\n",
      " 15. 13. 16. 15.  7.  2. 12. 16.  9. 15. 16.  2.  6. 11.  8.  5.  7. 14.\n",
      "  9.  4.  9.  9. 16. 10.  1.  1.  8.  8. 10.  4.  3. 15.  8. 14. 16.  5.\n",
      " 14. 11.  8. 11. 14.  1. 13.  8.  6.  4. 16. 14. 10. 16. 14.  9. 13.  7.\n",
      " 12.  0. 12. 10.  4. 15. 13. 13.  3.  8. 16. 16.  2. 10.  0. 15. 14. 12.\n",
      " 14.  7.  0. 10. 16. 12. 12. 13. 10.  6.  2.  0.  1. 12. 16. 16.  6. 11.\n",
      " 14.  4. 16. 13. 16. 12.  4. 15.  4.  5.  0. 16. 13.  8. 16.  6.  0. 12.\n",
      " 14.  2. 13.  0.  5. 16.  4. 14. 13.  8. 14.  0. 12.  8.  7. 15.  0.  6.\n",
      "  1. 16. 13. 16.  5.  6.  4.  8. 16. 16.  5.  1. 16.  6. 11.  0. 10.  2.\n",
      "  7.  9. 16. 16. 11. 16. 16. 10.  3. 10. 16. 12.  5.  7. 13. 15. 12.  0.\n",
      " 16.  4.  0.  2.  1.  6.  7.  5.  8.  0. 16.  7.  0.  7. 12.  8.  2.  6.\n",
      " 16. 16.  0.  9. 15.  4. 13. 15.  8. 16. 10. 16.  2. 10.  5.  4.  7.  1.\n",
      " 16.  7. 13.  2. 16. 11.  7. 15. 10.  7.  7. 13. 15. 10. 16.  7.  8. 16.\n",
      " 12.  8. 16. 16.  4. 11.  4.  9.  7.  8.  4.  8.]\n",
      "\n",
      "X_test (few rows): [[ 0.  0.  0.  3. 15.  4.  0.  0.  0.  0.  4. 16. 12.  0.  0.  0.  0.  0.\n",
      "  12. 15.  3.  4.  3.  0.  0.  7. 16.  5.  3. 15.  8.  0.  0. 13. 16. 13.\n",
      "  15. 16.  2.  0.  0. 12. 16. 16. 16. 13.  0.  0.]\n",
      " [ 0.  1.  7. 13. 16. 11.  0.  0.  0. 11. 16. 13. 15. 16.  0.  0.  0.  3.\n",
      "   8.  2. 16.  9.  0.  0.  0.  0.  0.  8. 16.  4.  0.  0.  0.  0.  0.  5.\n",
      "  16. 16.  5.  0.  0.  0.  0.  0.  3. 14. 11.  0.]\n",
      " [ 0.  0.  0.  6. 16.  6.  0.  0.  0.  0.  5. 16. 10.  0.  0.  0.  0.  2.\n",
      "  15. 14.  0.  7.  1.  0.  0.  6. 16.  3.  3. 16.  9.  0.  0. 11. 16.  8.\n",
      "  11. 16.  6.  0.  0.  3. 15. 16. 16. 15.  1.  0.]\n",
      " [ 0.  0. 15. 13. 13. 13.  0.  0.  0.  0. 16. 16. 11.  3.  0.  0.  0.  0.\n",
      "  12. 13.  0.  0.  0.  0.  0.  0.  5. 16.  3.  0.  0.  0.  0.  0.  0. 11.\n",
      "  10.  0.  0.  0.  0.  0.  0. 10. 14.  0.  0.  0.]\n",
      " [ 0.  0.  2. 15.  8.  0.  0.  0.  0.  0.  7. 14. 15.  8.  0.  0.  0.  7.\n",
      "  15.  3.  3. 15.  0.  0.  0.  6. 16.  1.  0.  9.  8.  0.  0.  4. 12.  0.\n",
      "   0.  8.  8.  0.  0.  0. 12.  3.  0. 12.  7.  0.]]\n",
      "\n",
      "Data used for modeling... (training data: 1414)\n",
      "y_train: [ 8.  1. 16. ...  4. 16.  0.]\n",
      "\n",
      "X_train (few rows): [[ 0.  0.  1. 13. 16. 10.  0.  0.  0.  1. 13. 15.  8. 16.  3.  0.  0.  8.\n",
      "  15.  3.  4. 15.  0.  0.  0.  1.  3.  0. 12.  8.  0.  0.  0.  0.  0.  4.\n",
      "  14.  1.  0.  0.  0.  0.  0. 11.  8.  0.  4.  0.]\n",
      " [ 0.  2.  6. 10. 12.  1.  0.  0.  0. 14. 13. 10.  5.  1.  0.  0.  0. 10.\n",
      "   6.  0.  0.  0.  0.  0.  0. 10. 13. 12. 12.  5.  0.  0.  0.  2.  8.  5.\n",
      "   7. 14.  8.  0.  0.  0.  0.  0.  0.  5. 12.  0.]\n",
      " [ 0.  0.  3.  8. 11. 11.  1.  0.  0.  0.  3. 16. 16. 12.  0.  0.  0.  0.\n",
      "   2. 15. 16. 12.  0.  0.  0.  0.  0. 16. 16.  7.  0.  0.  0.  0.  1. 15.\n",
      "  16. 10.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.]\n",
      " [ 0.  0.  0. 12. 14.  5.  0.  0.  0.  0.  6. 11.  4. 15.  0.  0.  0.  0.\n",
      "   8.  9.  8. 16.  3.  0.  0.  0.  3. 14. 13. 13.  4.  0.  0.  0.  0.  0.\n",
      "   0. 10.  7.  0.  0.  0.  0.  0.  0.  7.  8.  0.]\n",
      " [ 0.  0.  2. 14. 15.  3.  0.  0.  0.  0.  7. 16. 11.  0.  0.  0.  0.  0.\n",
      "  13. 15.  1.  0.  0.  0.  0.  1. 16. 11.  0.  0.  0.  0.  0.  2. 16.  9.\n",
      "   0.  0.  0.  0.  0.  2. 16. 16. 16.  9.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                input  -> pred  des. \n",
      "          [ 0.         -0.33282981 -0.88638003  0.25857762  0.97758264 -> ?     8    \n",
      "          [ 0.          2.00856997  0.17218531 -0.45368619  0.04532713 -> ?     1    \n",
      "          [ 0.         -0.33282981 -0.4629539  -0.92852873 -0.18773674 -> ?     16   \n",
      "          [ 0.         -0.33282981 -1.0980931   0.02115635  0.51145489 -> ?     1    \n",
      "          [ 0.         -0.33282981 -0.67466697  0.49599889  0.74451876 -> ?     10   \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "#    This is done through the \"StandardScaler\" in scikit-learn\n",
    "# \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here is a fully-scaled dataset:\n",
    "\n",
    "X_all_scaled = scaler.transform(X_all)\n",
    "y_all_scaled = y_all.copy()      # not scaled\n",
    "\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "def ascii_table(X,y):\n",
    "    \"\"\" print a table of binary inputs and outputs \"\"\"\n",
    "    print(f\"{'input ':>70s} -> {'pred':<5s} {'des.':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        s_to_show = str(X[i,:])\n",
    "        s_to_show = s_to_show[0:60]\n",
    "        print(f\"{s_to_show!s:>70s} -> {'?':<5s} {y[i]:<5.0f}\")   # !s is str ...\n",
    "    \n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5])\n",
    "\n",
    "#\n",
    "# Note that the zeros have become -1's\n",
    "# and the 1's have stayed 1's\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 17.67313124\n",
      "Iteration 2, loss = 9.21558546\n",
      "Iteration 3, loss = 8.00345763\n",
      "Iteration 4, loss = 7.53970897\n",
      "Iteration 5, loss = 7.12655935\n",
      "Iteration 6, loss = 6.76147482\n",
      "Iteration 7, loss = 6.72181059\n",
      "Iteration 8, loss = 7.64099130\n",
      "Iteration 9, loss = 7.12414085\n",
      "Iteration 10, loss = 6.89589748\n",
      "Iteration 11, loss = 7.41506368\n",
      "Iteration 12, loss = 7.73153138\n",
      "Iteration 13, loss = 7.34804574\n",
      "Iteration 14, loss = 7.83844268\n",
      "Iteration 15, loss = 7.73258793\n",
      "Iteration 16, loss = 7.48312501\n",
      "Iteration 17, loss = 7.15873351\n",
      "Iteration 18, loss = 7.00551924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 19, loss = 6.67122435\n",
      "Iteration 20, loss = 6.63385244\n",
      "Iteration 21, loss = 6.47014279\n",
      "Iteration 22, loss = 6.35932314\n",
      "Iteration 23, loss = 6.20578102\n",
      "Iteration 24, loss = 6.15120217\n",
      "Iteration 25, loss = 6.07056774\n",
      "Iteration 26, loss = 6.07321043\n",
      "Iteration 27, loss = 6.00790264\n",
      "Iteration 28, loss = 5.96365458\n",
      "Iteration 29, loss = 5.92259934\n",
      "Iteration 30, loss = 5.89037263\n",
      "Iteration 31, loss = 6.01338298\n",
      "Iteration 32, loss = 6.00385534\n",
      "Iteration 33, loss = 5.90724949\n",
      "Iteration 34, loss = 5.93550003\n",
      "Iteration 35, loss = 5.87214259\n",
      "Iteration 36, loss = 5.80712601\n",
      "Iteration 37, loss = 5.78154488\n",
      "Iteration 38, loss = 5.71617209\n",
      "Iteration 39, loss = 5.69816545\n",
      "Iteration 40, loss = 5.68876148\n",
      "Iteration 41, loss = 5.64872100\n",
      "Iteration 42, loss = 5.69995073\n",
      "Iteration 43, loss = 5.71428679\n",
      "Iteration 44, loss = 5.62822824\n",
      "Iteration 45, loss = 5.62438041\n",
      "Iteration 46, loss = 5.55174224\n",
      "Iteration 47, loss = 5.64650817\n",
      "Iteration 48, loss = 5.58508197\n",
      "Iteration 49, loss = 5.54166521\n",
      "Iteration 50, loss = 5.52695218\n",
      "Iteration 51, loss = 5.50710506\n",
      "Iteration 52, loss = 5.48556722\n",
      "Iteration 53, loss = 5.47250000\n",
      "Iteration 54, loss = 5.48068440\n",
      "Iteration 55, loss = 5.45858008\n",
      "Iteration 56, loss = 5.42313306\n",
      "Iteration 57, loss = 5.43058658\n",
      "Iteration 58, loss = 5.43420410\n",
      "Iteration 59, loss = 5.36684820\n",
      "Iteration 60, loss = 5.34532730\n",
      "Iteration 61, loss = 5.29498812\n",
      "Iteration 62, loss = 5.30147558\n",
      "Iteration 63, loss = 5.32775549\n",
      "Iteration 64, loss = 5.31117900\n",
      "Iteration 65, loss = 5.30165862\n",
      "Iteration 66, loss = 5.29503126\n",
      "Iteration 67, loss = 5.28091153\n",
      "Iteration 68, loss = 5.25401454\n",
      "Iteration 69, loss = 5.24117103\n",
      "Iteration 70, loss = 5.23325800\n",
      "Iteration 71, loss = 5.18089386\n",
      "Iteration 72, loss = 5.18416286\n",
      "Iteration 73, loss = 5.20957350\n",
      "Iteration 74, loss = 5.18401294\n",
      "Iteration 75, loss = 5.23397127\n",
      "Iteration 76, loss = 5.17067955\n",
      "Iteration 77, loss = 5.22961977\n",
      "Iteration 78, loss = 5.21448045\n",
      "Iteration 79, loss = 5.17391656\n",
      "Iteration 80, loss = 5.19470483\n",
      "Iteration 81, loss = 5.18187331\n",
      "Iteration 82, loss = 5.19633667\n",
      "Iteration 83, loss = 5.14869400\n",
      "Iteration 84, loss = 5.15642670\n",
      "Iteration 85, loss = 5.19918436\n",
      "Iteration 86, loss = 5.15814726\n",
      "Iteration 87, loss = 5.14432448\n",
      "Iteration 88, loss = 5.13573644\n",
      "Iteration 89, loss = 5.16500216\n",
      "Iteration 90, loss = 5.14873482\n",
      "Iteration 91, loss = 5.13702201\n",
      "Iteration 92, loss = 5.15833473\n",
      "Iteration 93, loss = 5.15219119\n",
      "Iteration 94, loss = 5.14303802\n",
      "Iteration 95, loss = 5.14686019\n",
      "Iteration 96, loss = 5.21628114\n",
      "Iteration 97, loss = 5.16466879\n",
      "Iteration 98, loss = 5.20081456\n",
      "Iteration 99, loss = 5.19061336\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 100, loss = 5.29814838\n",
      "Iteration 101, loss = 5.37669309\n",
      "Iteration 102, loss = 5.34196280\n",
      "Iteration 103, loss = 5.30816549\n",
      "Iteration 104, loss = 5.27831185\n",
      "Iteration 105, loss = 5.24608682\n",
      "Iteration 106, loss = 5.23292470\n",
      "Iteration 107, loss = 5.21954237\n",
      "Iteration 108, loss = 5.20930848\n",
      "Iteration 109, loss = 5.19960849\n",
      "Iteration 110, loss = 5.18736962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 111, loss = 5.17358115\n",
      "Iteration 112, loss = 5.17452617\n",
      "Iteration 113, loss = 5.17268685\n",
      "Iteration 114, loss = 5.17029625\n",
      "Iteration 115, loss = 5.16788901\n",
      "Iteration 116, loss = 5.16678096\n",
      "Iteration 117, loss = 5.16467324\n",
      "Iteration 118, loss = 5.16272470\n",
      "Iteration 119, loss = 5.16184263\n",
      "Iteration 120, loss = 5.16031361\n",
      "Iteration 121, loss = 5.15840048\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 122, loss = 5.15642280\n",
      "Iteration 123, loss = 5.15607761\n",
      "Iteration 124, loss = 5.15552103\n",
      "Iteration 125, loss = 5.15493762\n",
      "Iteration 126, loss = 5.15470455\n",
      "Iteration 127, loss = 5.15433503\n",
      "Iteration 128, loss = 5.15422816\n",
      "Iteration 129, loss = 5.15395276\n",
      "Iteration 130, loss = 5.15382636\n",
      "Iteration 131, loss = 5.15373374\n",
      "Iteration 132, loss = 5.15319999\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 133, loss = 5.15275503\n",
      "Iteration 134, loss = 5.15258128\n",
      "Iteration 135, loss = 5.15242495\n",
      "Iteration 136, loss = 5.15236181\n",
      "Iteration 137, loss = 5.15228928\n",
      "Iteration 138, loss = 5.15222870\n",
      "Iteration 139, loss = 5.15224205\n",
      "Iteration 140, loss = 5.15218056\n",
      "Iteration 141, loss = 5.15211466\n",
      "Iteration 142, loss = 5.15208185\n",
      "Iteration 143, loss = 5.15202286\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 144, loss = 5.15194919\n",
      "Iteration 145, loss = 5.15190142\n",
      "Iteration 146, loss = 5.15187495\n",
      "Iteration 147, loss = 5.15186312\n",
      "Iteration 148, loss = 5.15184994\n",
      "Iteration 149, loss = 5.15183638\n",
      "Iteration 150, loss = 5.15183422\n",
      "Iteration 151, loss = 5.15182585\n",
      "Iteration 152, loss = 5.15182362\n",
      "Iteration 153, loss = 5.15180554\n",
      "Iteration 154, loss = 5.15179362\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 155, loss = 5.15177598\n",
      "Iteration 156, loss = 5.15177334\n",
      "Iteration 157, loss = 5.15177086\n",
      "Iteration 158, loss = 5.15176825\n",
      "Iteration 159, loss = 5.15176608\n",
      "Iteration 160, loss = 5.15176511\n",
      "Iteration 161, loss = 5.15176385\n",
      "Iteration 162, loss = 5.15176164\n",
      "Iteration 163, loss = 5.15176202\n",
      "Iteration 164, loss = 5.15175938\n",
      "Iteration 165, loss = 5.15175798\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 166, loss = 5.15175600\n",
      "Iteration 167, loss = 5.15175433\n",
      "Iteration 168, loss = 5.15175433\n",
      "Iteration 169, loss = 5.15175385\n",
      "Iteration 170, loss = 5.15175356\n",
      "Iteration 171, loss = 5.15175339\n",
      "Iteration 172, loss = 5.15175326\n",
      "Iteration 173, loss = 5.15175261\n",
      "Iteration 174, loss = 5.15175200\n",
      "Iteration 175, loss = 5.15175171\n",
      "Iteration 176, loss = 5.15175099\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The (squared) prediction error (the loss) is 5.151750985654437\n",
      "And, its square root: 2.2697468990295895\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MLPRegressor predicts _floating-point_ outputs\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                    max_iter=342,          # how many training epochs\n",
    "                    activation=\"tanh\",     # the activation function\n",
    "                    solver='sgd',          # the optimizer\n",
    "                    verbose=True,          # do we want to watch as it trains?\n",
    "                    shuffle=True,          # shuffle each epoch?\n",
    "                    random_state=None,     # use for reproducibility\n",
    "                    learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                    learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "\n",
    "print(f\"The (squared) prediction error (the loss) is {nn_regressor.loss_}\")\n",
    "print(f\"And, its square root: {nn_regressor.loss_ ** 0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             input  ->   pred    des.    absdiff  \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.724  +16.000    1.276   \n",
      "          [ 0.00000000e+00  1.00000 ->  +7.198  +14.000    6.802   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.724  +16.000    1.276   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.662  +10.000    4.662   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.989  +13.000    5.011   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.649  +16.000    1.351   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.004  +15.000    3.996   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.645  +16.000    1.355   \n",
      "          [ 0.00000000e+00  0.00000 ->  +2.893  +4.000    1.107   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.204  +12.000    4.796   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.989  +14.000    6.011   \n",
      "          [ 0.00000000e+00  2.00000 ->  +10.514  +3.000    7.514   \n",
      "          [ 0.00000000e+00  0.00000 ->  +4.415  +0.000    4.415   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.197  +8.000    0.803   \n",
      "          [ 0.00000000e+00  0.00000 ->  +12.168  +16.000    3.832   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.371  +3.000    2.371   \n",
      "          [ 0.00000000e+00  0.00000 ->  +6.776  +11.000    4.224   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.713  +10.000    4.287   \n",
      "          [ 0.00000000e+00  0.00000 ->  +2.835  +10.000    7.165   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.249  +16.000    4.751   \n",
      "          [ 0.00000000e+00  0.00000 ->  +12.540  +14.000    1.460   \n",
      "          [ 0.  1. 12. 16. 16.  9.  ->  +5.434  +8.000    2.566   \n",
      "          [ 0.0000000e+00  1.000000 ->  +14.158  +12.000    2.158   \n",
      "          [ 0.  0. 13. 16. 16. 16.  ->  +14.372  +16.000    1.628   \n",
      "          [ 0.00000000e+00  0.00000 ->  +13.515  +16.000    2.485   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.773  +15.000    0.227   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.084  +2.000    1.084   \n",
      "          [ 0.00000000e+00  0.00000 ->  +1.611  +2.000    0.389   \n",
      "          [ 0.  0.  2. 14. 15.  5.  ->  +14.563  +16.000    1.437   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.641  +11.000    3.641   \n",
      "          [ 0.0000000e+00  0.000000 ->  +2.616  +1.000    1.616   \n",
      "          [0.0000000e+00 4.0000000e ->  +12.838  +10.000    2.838   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.659  +8.000    6.659   \n",
      "          [ 0.00000000e+00  0.00000 ->  +4.980  +1.000    3.980   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.646  +15.000    0.354   \n",
      "          [ 0.0000000e+00  0.000000 ->  +11.707  +6.000    5.707   \n",
      "          [ 0.00000000e+00  1.00000 ->  +3.005  +4.000    0.995   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.706  +0.000    5.706   \n",
      "          [ 0.  0.  0. 10. 16. 11.  ->  +14.646  +16.000    1.354   \n",
      "          [ 0.00000000e+00  0.00000 ->  +2.893  +4.000    1.107   \n",
      "          [ 0.0000000e+00  0.000000 ->  +2.619  +1.000    1.619   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.409  +2.000    3.409   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.084  +4.000    0.916   \n",
      "          [ 0.00000000e+00  0.00000 ->  +9.336  +1.000    8.336   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.665  +14.000    0.665   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.080  +5.000    1.920   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.679  +2.000    12.679  \n",
      "          [ 0.  0.  1. 11. 12.  9.  ->  +2.894  +3.000    0.106   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.650  +13.000    1.650   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.388  +4.000    7.388   \n",
      "          [ 0.00000000e+00  0.00000 ->  +12.348  +16.000    3.652   \n",
      "          [ 0.00000000e+00  0.00000 ->  +12.548  +7.000    5.548   \n",
      "          [ 0.  0.  3. 13. 16. 16.  ->  +9.341  +10.000    0.659   \n",
      "          [ 0.  0.  0.  0.  5. 15.  ->  +9.057  +6.000    3.057   \n",
      "          [ 0.00000000e+00  0.00000 ->  +13.517  +12.000    1.517   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.621  +0.000    5.621   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.989  +0.000    7.989   \n",
      "          [0.0000000e+00 0.0000000e ->  +7.989  +12.000    4.011   \n",
      "          [ 0.00000000e+00  1.00000 ->  +11.323  +16.000    4.677   \n",
      "          [0.0000000e+00 0.0000000e ->  +5.413  +10.000    4.587   \n",
      "          [ 0.00000000e+00  0.00000 ->  +10.948  +12.000    1.052   \n",
      "          [ 0.  0.  7. 12. 10.  0.  ->  +11.361  +2.000    9.361   \n",
      "          [ 0.00000000e+00  0.00000 ->  +9.147  +6.000    3.147   \n",
      "          [ 0.00000000e+00  0.00000 ->  +8.138  +0.000    8.138   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.197  +12.000    4.803   \n",
      "          [ 0.  0.  4. 12. 13.  5.  ->  +11.718  +16.000    4.282   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.664  +16.000    1.336   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.989  +13.000    5.011   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.707  +5.000    0.707   \n",
      "          [ 0.00000000e+00  3.00000 ->  +14.064  +10.000    4.064   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.084  +8.000    4.916   \n",
      "          [ 0.  0.  0.  6. 15.  1.  ->  +14.633  +14.000    0.633   \n",
      "          [ 0.00000000e+00  2.00000 ->  +7.901  +12.000    4.099   \n",
      "          [ 0.0000000e+00  0.000000 ->  +2.464  +1.000    1.464   \n",
      "          [ 0.00000000e+00  1.00000 ->  +10.969  +8.000    2.969   \n",
      "          [ 0.0000000e+00  0.000000 ->  +2.893  +3.000    0.107   \n",
      "          [ 0.  0.  4. 15. 14. 12.  ->  +14.158  +9.000    5.158   \n",
      "          [ 0.  0.  0.  0. 11. 15.  ->  +9.056  +16.000    6.944   \n",
      "          [ 0.  0.  2. 10. 16. 10.  ->  +6.737  +11.000    4.263   \n",
      "          [ 0.  0.  0.  9. 13.  0.  ->  +14.724  +16.000    1.276   \n",
      "          [ 0.0000000e+00  2.000000 ->  +2.846  +7.000    4.154   \n",
      "          [ 0.00000000e+00  1.00000 ->  +5.437  +0.000    5.437   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.003  +13.000    1.997   \n",
      "          [ 0.00000000e+00  0.00000 ->  +8.885  +2.000    6.885   \n",
      "          [ 0.0000000e+00  0.000000 ->  +7.896  +0.000    7.896   \n",
      "          [ 0.00000000e+00  0.00000 ->  +12.523  +10.000    2.523   \n",
      "          [ 0.0000000e+00  0.000000 ->  +2.893  +1.000    1.893   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.648  +10.000    4.648   \n",
      "          [ 0.0000000e+00  3.000000 ->  +9.174  +15.000    5.826   \n",
      "          [ 0.00000000e+00  3.00000 ->  +2.453  +0.000    2.453   \n",
      "          [0.0000000e+00 0.0000000e ->  +9.056  +10.000    0.944   \n",
      "          [ 0.00000000e+00  1.00000 ->  +10.491  +8.000    2.491   \n",
      "          [0.0000000e+00 1.0000000e ->  +13.694  +16.000    2.306   \n",
      "          [0.0000000e+00 0.0000000e ->  +7.989  +16.000    8.011   \n",
      "          [0.0000000e+00 0.0000000e ->  +14.435  +16.000    1.565   \n",
      "          [ 0.00000000e+00  4.00000 ->  +10.969  +16.000    5.031   \n",
      "          [ 0.0000000e+00  0.000000 ->  +11.029  +10.000    1.029   \n",
      "          [ 0.00000000e+00  2.00000 ->  +11.004  +16.000    4.996   \n",
      "          [ 0.  0. 12.  9.  9.  8.  ->  +2.542  +5.000    2.458   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.840  +13.000    1.160   \n",
      "          [0.0000000e+00 0.0000000e ->  +9.056  +9.000    0.056   \n",
      "          [0.0000000e+00 4.0000000e ->  +14.627  +14.000    0.627   \n",
      "          [ 0.00000000e+00  0.00000 ->  +8.725  +7.000    1.725   \n",
      "          [ 0.00000000e+00  2.00000 ->  +10.969  +6.000    4.969   \n",
      "          [ 0.0000000e+00  0.000000 ->  +10.160  +5.000    5.160   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.420  +7.000    4.420   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.749  +16.000    1.251   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.473  +15.000    0.527   \n",
      "          [ 0.00000000e+00  0.00000 ->  +10.968  +1.000    9.968   \n",
      "          [ 0.  0. 11. 16.  9.  8.  ->  +7.799  +10.000    2.201   \n",
      "          [ 0.  0.  8. 16. 16. 12.  ->  +14.286  +12.000    2.286   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.084  +1.000    2.084   \n",
      "          [ 0.00000000e+00  0.00000 ->  +2.893  +0.000    2.893   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.724  +15.000    0.276   \n",
      "          [ 0.  1.  5. 12. 16. 14.  ->  +5.434  +15.000    9.566   \n",
      "          [ 0.00000000e+00  5.00000 ->  +11.333  +15.000    3.667   \n",
      "          [ 0.00000000e+00  0.00000 ->  +12.611  +16.000    3.389   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.989  +14.000    6.011   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.722  +16.000    1.278   \n",
      "          [ 0.00000000e+00  0.00000 ->  +2.526  +2.000    0.526   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.649  +15.000    0.351   \n",
      "          [ 0.  0.  3.  9. 14.  7.  ->  +9.317  +4.000    5.317   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.354  +15.000    7.646   \n",
      "          [ 0.00000000e+00  0.00000 ->  +10.979  +10.000    0.979   \n",
      "          [0.0000000e+00 4.0000000e ->  +5.410  +11.000    5.590   \n",
      "          [ 0.00000000e+00  0.00000 ->  +8.052  +16.000    7.948   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.646  +15.000    0.354   \n",
      "          [ 0.  0.  9. 16.  6.  0.  ->  +7.891  +13.000    5.109   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.663  +16.000    1.337   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.646  +15.000    0.354   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.999  +7.000    0.999   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.653  +2.000    12.653  \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.650  +12.000    2.650   \n",
      "          [ 0.00000000e+00  2.00000 ->  +11.365  +16.000    4.635   \n",
      "          [0.0000000e+00 6.0000000e ->  +11.681  +9.000    2.681   \n",
      "          [0.0000000e+00 0.0000000e ->  +14.170  +15.000    0.830   \n",
      "          [ 0.  4. 14. 16. 16. 12.  ->  +14.646  +16.000    1.354   \n",
      "          [ 0.00000000e+00  0.00000 ->  +2.893  +2.000    0.893   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.983  +6.000    1.983   \n",
      "          [ 0.00000000e+00  0.00000 ->  +2.877  +11.000    8.123   \n",
      "          [ 0.  1. 14. 16. 16. 11.  ->  +7.710  +8.000    0.290   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.084  +5.000    1.916   \n",
      "          [ 0.00000000e+00  0.00000 ->  +9.397  +7.000    2.397   \n",
      "          [ 0.  0. 10.  7. 13.  9.  ->  +11.001  +14.000    2.999   \n",
      "          [ 0.  0.  0.  0.  5. 15.  ->  +13.731  +9.000    4.731   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.053  +4.000    0.947   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.662  +9.000    5.662   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.767  +9.000    1.233   \n",
      "          [ 0.  0.  2. 12. 12.  0.  ->  +14.645  +16.000    1.355   \n",
      "          [ 0.  0.  0. 15. 16. 16.  ->  +6.737  +10.000    3.263   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.593  +1.000    13.593  \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.714  +1.000    4.714   \n",
      "          [ 0.00000000e+00  5.00000 ->  +11.152  +8.000    3.152   \n",
      "          [ 0.00000000e+00  0.00000 ->  +10.993  +8.000    2.993   \n",
      "          [ 0.0000000e+00  0.000000 ->  +7.251  +10.000    2.749   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.084  +4.000    0.916   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.741  +3.000    8.741   \n",
      "          [ 0.  0. 11. 16. 16. 14.  ->  +11.024  +15.000    3.976   \n",
      "          [ 0.00000000e+00  0.00000 ->  +12.285  +8.000    4.285   \n",
      "          [ 0.  0.  0.  7. 14. 16.  ->  +9.341  +14.000    4.659   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.645  +16.000    1.355   \n",
      "          [ 0.  0.  9. 12. 12. 13.  ->  +2.873  +5.000    2.127   \n",
      "          [ 0.00000000e+00  1.00000 ->  +9.139  +14.000    4.861   \n",
      "          [ 0.  0. 10. 16. 15.  7.  ->  +14.601  +11.000    3.601   \n",
      "          [ 0.  0.  3. 14. 10.  3.  ->  +2.893  +8.000    5.107   \n",
      "          [ 0.  0.  6. 16. 16. 11.  ->  +7.935  +11.000    3.065   \n",
      "          [ 0.00000000e+00  0.00000 ->  +9.332  +14.000    4.668   \n",
      "          [ 0.  0.  5. 12. 16. 15.  ->  +2.880  +1.000    1.880   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.440  +13.000    7.560   \n",
      "          [0.0000000e+00 0.0000000e ->  +5.436  +8.000    2.564   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.989  +6.000    1.989   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.713  +4.000    1.713   \n",
      "          [ 0.0000000e+00  0.000000 ->  +9.340  +16.000    6.660   \n",
      "          [0.0000000e+00 0.0000000e ->  +2.954  +14.000    11.046  \n",
      "          [ 0.00000000e+00  0.00000 ->  +10.071  +10.000    0.071   \n",
      "          [ 0.00000000e+00  0.00000 ->  +8.353  +16.000    7.647   \n",
      "          [0.0000000e+00 0.0000000e ->  +9.056  +14.000    4.944   \n",
      "          [ 0.0000000e+00  0.000000 ->  +6.738  +9.000    2.262   \n",
      "          [ 0.00000000e+00  0.00000 ->  +10.185  +13.000    2.815   \n",
      "          [ 0.0000000e+00  0.000000 ->  +2.893  +7.000    4.107   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.724  +12.000    2.724   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.451  +0.000    5.451   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.982  +12.000    0.018   \n",
      "          [ 0.  0.  0.  9. 15.  9.  ->  +6.714  +10.000    3.286   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.989  +4.000    3.989   \n",
      "          [ 0.  0.  4. 16. 16.  8.  ->  +14.646  +15.000    0.354   \n",
      "          [ 0.  0.  0.  8. 12.  0.  ->  +10.045  +13.000    2.955   \n",
      "          [ 0.  0. 10.  9. 14. 10.  ->  +10.993  +13.000    2.007   \n",
      "          [ 0.0000000e+00  0.000000 ->  +5.540  +3.000    2.540   \n",
      "          [ 0.  0.  5. 12. 16. 10.  ->  +5.409  +8.000    2.591   \n",
      "          [ 0.00000000e+00  3.00000 ->  +10.962  +16.000    5.038   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.664  +16.000    1.336   \n",
      "          [ 0.0000000e+00  1.000000 ->  +2.893  +2.000    0.893   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.004  +10.000    1.004   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.208  +0.000    5.208   \n",
      "          [ 0.  1. 11. 16. 16. 12.  ->  +14.654  +15.000    0.346   \n",
      "          [ 0.00000000e+00  3.00000 ->  +10.969  +14.000    3.031   \n",
      "          [0.0000000e+00 0.0000000e ->  +5.707  +12.000    6.293   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.649  +14.000    0.649   \n",
      "          [0.0000000e+00 0.0000000e ->  +3.092  +7.000    3.908   \n",
      "          [ 0.00000000e+00  4.00000 ->  +10.982  +0.000    10.982  \n",
      "          [ 0.00000000e+00  0.00000 ->  +9.890  +10.000    0.110   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.645  +16.000    1.355   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.724  +12.000    2.724   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.989  +12.000    4.011   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.714  +13.000    7.286   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.361  +10.000    1.361   \n",
      "          [ 0.00000000e+00  0.00000 ->  +6.155  +6.000    0.155   \n",
      "          [ 0.0000000e+00  0.000000 ->  +2.902  +2.000    0.902   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.061  +0.000    3.061   \n",
      "          [ 0.00000000e+00  1.00000 ->  +5.085  +1.000    4.085   \n",
      "          [ 0.  3. 14.  5.  0.  0.  ->  +11.815  +12.000    0.185   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.649  +16.000    1.351   \n",
      "          [ 0.  1. 13. 16. 16. 16.  ->  +14.646  +16.000    1.354   \n",
      "          [ 0.0000000e+00  0.000000 ->  +6.741  +6.000    0.741   \n",
      "          [ 0.00000000e+00  0.00000 ->  +9.956  +11.000    1.044   \n",
      "          [ 0.  0.  0.  3. 13. 16.  ->  +14.169  +14.000    0.169   \n",
      "          [ 0.00000000e+00  0.00000 ->  +4.998  +4.000    0.998   \n",
      "          [ 0.00000000e+00  2.00000 ->  +11.332  +16.000    4.668   \n",
      "          [ 0.  0.  4. 12. 16. 16.  ->  +11.806  +13.000    1.194   \n",
      "          [ 0.  0.  0.  0. 10. 13.  ->  +10.965  +16.000    5.035   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.646  +12.000    2.646   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.796  +4.000    0.204   \n",
      "          [ 0.00000000e+00  0.00000 ->  +12.097  +15.000    2.903   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.084  +4.000    0.916   \n",
      "          [ 0.  0.  0.  0.  5. 15.  ->  +9.056  +5.000    4.056   \n",
      "          [ 0.0000000e+00  0.000000 ->  +5.409  +0.000    5.409   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.724  +16.000    1.276   \n",
      "          [ 0.00000000e+00  0.00000 ->  +12.312  +13.000    0.688   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.989  +8.000    0.011   \n",
      "          [ 0.00000000e+00  0.00000 ->  +9.095  +16.000    6.905   \n",
      "          [ 0.  0.  0.  1.  9. 14.  ->  +12.592  +6.000    6.592   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.990  +0.000    7.990   \n",
      "          [ 0.  0.  6. 14. 16. 15.  ->  +14.650  +12.000    2.650   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.649  +14.000    0.649   \n",
      "          [ 0.00000000e+00  1.00000 ->  +10.890  +2.000    8.890   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.742  +13.000    1.258   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.094  +0.000    3.094   \n",
      "          [ 0.0000000e+00  0.000000 ->  +2.943  +5.000    2.057   \n",
      "          [ 0.  0.  7. 10. 16.  9.  ->  +14.663  +16.000    1.337   \n",
      "          [ 0.00000000e+00  0.00000 ->  +9.147  +4.000    5.147   \n",
      "          [ 0.00000000e+00  1.00000 ->  +13.587  +14.000    0.413   \n",
      "          [ 0.  0.  9. 14. 16. 13.  ->  +2.842  +13.000    10.158  \n",
      "          [ 0.00000000e+00  0.00000 ->  +13.023  +8.000    5.023   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.649  +14.000    0.649   \n",
      "          [ 0.0000000e+00  0.000000 ->  +5.409  +0.000    5.409   \n",
      "          [ 0.0000000e+00  0.000000 ->  +9.657  +12.000    2.343   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.442  +8.000    0.558   \n",
      "          [ 0.00000000e+00  0.00000 ->  +4.423  +7.000    2.577   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.672  +15.000    3.328   \n",
      "          [ 0.0000000e+00  0.000000 ->  +2.893  +0.000    2.893   \n",
      "          [ 0.00000000e+00  1.00000 ->  +9.061  +6.000    3.061   \n",
      "          [ 0.00000000e+00  0.00000 ->  +10.531  +1.000    9.531   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.724  +16.000    1.276   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.010  +13.000    1.990   \n",
      "          [ 0.  0.  5. 12. 10.  4.  ->  +14.646  +16.000    1.354   \n",
      "          [ 0.  0.  3. 14. 16.  9.  ->  +2.893  +5.000    2.107   \n",
      "          [0.0000000e+00 0.0000000e ->  +5.439  +6.000    0.561   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.713  +4.000    1.713   \n",
      "          [ 0.00000000e+00  0.00000 ->  +4.611  +8.000    3.389   \n",
      "          [ 0.  0.  0.  4. 13. 13.  ->  +14.644  +16.000    1.356   \n",
      "          [ 0.  0.  0.  9. 13.  0.  ->  +13.188  +16.000    2.812   \n",
      "          [ 0.00000000e+00  0.00000 ->  +6.788  +5.000    1.788   \n",
      "          [ 0.  0.  8. 16. 13.  2.  ->  +2.893  +1.000    1.893   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.649  +16.000    1.351   \n",
      "          [ 0.  0. 10. 15. 14.  4.  ->  +5.434  +6.000    0.566   \n",
      "          [ 0.00000000e+00  0.00000 ->  +10.970  +11.000    0.030   \n",
      "          [ 0.0000000e+00  0.000000 ->  +2.894  +0.000    2.894   \n",
      "          [ 0.  0.  4. 10. 15. 16.  ->  +9.346  +10.000    0.654   \n",
      "          [ 0.00000000e+00  2.00000 ->  +10.478  +2.000    8.478   \n",
      "          [ 0.  0.  0.  0.  6. 16.  ->  +9.058  +7.000    2.058   \n",
      "          [ 0.0000000e+00  0.000000 ->  +9.347  +9.000    0.347   \n",
      "          [ 0.  0.  0.  4. 14. 14.  ->  +14.646  +16.000    1.354   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.664  +16.000    1.336   \n",
      "          [ 0.  1. 13. 14. 16. 15.  ->  +6.737  +11.000    4.263   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.651  +16.000    1.349   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.724  +16.000    1.276   \n",
      "          [0.0000000e+00 0.0000000e ->  +2.914  +10.000    7.086   \n",
      "          [ 0.00000000e+00  0.00000 ->  +2.256  +3.000    0.744   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.724  +10.000    4.724   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.711  +16.000    1.289   \n",
      "          [ 0.00000000e+00  0.00000 ->  +12.547  +12.000    0.547   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.713  +5.000    0.713   \n",
      "          [ 0.  0.  0.  0.  3. 15.  ->  +7.367  +7.000    0.367   \n",
      "          [0.0000000e+00 0.0000000e ->  +5.713  +13.000    7.287   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.649  +15.000    0.351   \n",
      "          [ 0.00000000e+00  4.00000 ->  +5.435  +12.000    6.565   \n",
      "          [ 0.00000000e+00  0.00000 ->  +4.934  +0.000    4.934   \n",
      "          [ 0.00000000e+00  5.00000 ->  +11.359  +16.000    4.641   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.004  +4.000    7.004   \n",
      "          [ 0.00000000e+00  7.00000 ->  +7.626  +0.000    7.626   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.990  +2.000    5.990   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.421  +1.000    4.421   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.713  +6.000    0.287   \n",
      "          [0.0000000e+00 0.0000000e ->  +9.056  +7.000    2.056   \n",
      "          [ 0.00000000e+00  1.00000 ->  +5.591  +5.000    0.591   \n",
      "          [ 0.00000000e+00  3.00000 ->  +10.969  +8.000    2.969   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.091  +0.000    3.091   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.646  +16.000    1.354   \n",
      "          [ 0.  0.  3. 12. 15. 14.  ->  +6.716  +7.000    0.284   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.084  +0.000    3.084   \n",
      "          [ 0.  0.  0.  8. 15.  8.  ->  +5.409  +7.000    1.591   \n",
      "          [ 0.  1. 12. 15. 16. 13.  ->  +5.695  +12.000    6.305   \n",
      "          [ 0.  0.  0.  0. 10.  0.  ->  +11.638  +8.000    3.638   \n",
      "          [ 0.0000000e+00  0.000000 ->  +2.893  +2.000    0.893   \n",
      "          [ 0.0000000e+00  0.000000 ->  +9.415  +6.000    3.415   \n",
      "          [0.0000000e+00 0.0000000e ->  +10.045  +16.000    5.955   \n",
      "          [ 0.0000000e+00  0.000000 ->  +9.056  +16.000    6.944   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.434  +0.000    5.434   \n",
      "          [ 0.00000000e+00  0.00000 ->  +4.314  +9.000    4.686   \n",
      "          [ 0.  0.  0. 14. 16. 15.  ->  +14.646  +15.000    0.354   \n",
      "          [ 0.00000000e+00  0.00000 ->  +8.671  +4.000    4.671   \n",
      "          [ 0.00000000e+00  0.00000 ->  +10.996  +13.000    2.004   \n",
      "          [ 0.  0.  0.  3. 15. 10.  ->  +8.690  +15.000    6.310   \n",
      "          [ 0.00000000e+00  3.00000 ->  +10.969  +8.000    2.969   \n",
      "          [ 0.  0.  1. 10. 15. 11.  ->  +14.646  +16.000    1.354   \n",
      "          [ 0.00000000e+00  0.00000 ->  +7.226  +10.000    2.774   \n",
      "          [ 0.  0.  8. 14. 16. 16.  ->  +9.056  +16.000    6.944   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.640  +2.000    3.640   \n",
      "          [ 0.00000000e+00  0.00000 ->  +12.547  +10.000    2.547   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.686  +5.000    0.686   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.084  +4.000    0.916   \n",
      "          [0.0000000e+00 0.0000000e ->  +2.893  +7.000    4.107   \n",
      "          [ 0.0000000e+00  0.000000 ->  +9.310  +1.000    8.310   \n",
      "          [ 0.  0.  0.  8. 15.  9.  ->  +5.462  +16.000    10.538  \n",
      "          [ 0.00000000e+00  0.00000 ->  +2.460  +7.000    4.540   \n",
      "          [ 0.00000000e+00  2.00000 ->  +11.014  +13.000    1.986   \n",
      "          [ 0.  0. 12. 16. 15.  6.  ->  +9.394  +2.000    7.394   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.664  +16.000    1.336   \n",
      "          [ 0.  0.  0.  5. 14. 12.  ->  +5.409  +11.000    5.591   \n",
      "          [ 0.0000000e+00  0.000000 ->  +9.341  +7.000    2.341   \n",
      "          [ 0.0000000e+00  2.000000 ->  +14.748  +15.000    0.252   \n",
      "          [ 0.00000000e+00  1.00000 ->  +10.969  +10.000    0.969   \n",
      "          [ 0.00000000e+00  2.00000 ->  +10.969  +7.000    3.969   \n",
      "          [ 0.00000000e+00  0.00000 ->  +4.951  +7.000    2.049   \n",
      "          [ 0.00000000e+00  0.00000 ->  +11.007  +13.000    1.993   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.724  +15.000    0.276   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.436  +10.000    4.564   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.665  +16.000    1.335   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.409  +7.000    1.591   \n",
      "          [ 0.00000000e+00  0.00000 ->  +4.980  +8.000    3.020   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.649  +16.000    1.351   \n",
      "          [ 0.00000000e+00  0.00000 ->  +9.137  +12.000    2.863   \n",
      "          [0.0000000e+00 0.0000000e ->  +5.668  +8.000    2.332   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.649  +16.000    1.351   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.646  +16.000    1.354   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.084  +4.000    0.916   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.700  +11.000    3.700   \n",
      "          [ 0.00000000e+00  0.00000 ->  +5.713  +4.000    1.713   \n",
      "          [ 0.00000000e+00  0.00000 ->  +14.664  +9.000    5.664   \n",
      "          [ 0.0000000e+00  0.000000 ->  +7.999  +7.000    0.999   \n",
      "          [ 0.00000000e+00  0.00000 ->  +6.167  +8.000    1.833   \n",
      "          [ 0.00000000e+00  0.00000 ->  +3.085  +4.000    0.915   \n",
      "          [ 0.00000000e+00  1.00000 ->  +9.147  +8.000    1.147   \n",
      "\n",
      "+++++   +++++      +++++   +++++   \n",
      "average abs error: 3.24479468599374\n",
      "+++++   +++++      +++++   +++++   \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# how did it do? now we're making progress (by regressing)\n",
    "#\n",
    "\n",
    "def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc) # all predictions\n",
    "    Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "    # measure error\n",
    "    error = 0.0\n",
    "    # printing\n",
    "    print(f\"{'input ':>35s} ->  {'pred':^6s}  {'des.':^6s}  {'absdiff':^10s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        desired = y[i]\n",
    "        result = abs(desired - pred)\n",
    "        error += result\n",
    "        # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "        s_to_show = str(Xpr[i,:])\n",
    "        s_to_show = s_to_show[0:25]  # we'll just take 25 of these\n",
    "        print(f\"{s_to_show!s:>35s} ->  {pred:<+6.3f}  {desired:<+6.3f}  {result:^10.3f}\") \n",
    "\n",
    "    print(\"\\n\" + \"+++++   +++++      +++++   +++++   \")\n",
    "    print(f\"average abs error: {error/len(y)}\")\n",
    "    print(\"+++++   +++++      +++++   +++++   \")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data \n",
    "# \n",
    "if True:\n",
    "    ascii_table_for_regressor(X_test_scaled,\n",
    "                            y_test_scaled,\n",
    "                            nn_regressor,\n",
    "                            scaler)   # this is our own f'n, above\n",
    "\n",
    "# and how it did on the training data!\n",
    "#\n",
    "if False:\n",
    "    ascii_table_for_regressor(X_train_scaled,\n",
    "                            y_train_scaled,\n",
    "                            nn_regressor,\n",
    "                            scaler)   # this is our own f'n, above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "\n",
      "\n",
      "The (sq) prediction error (the loss) is 3.2957483134964387\n",
      "So, the 'average' error per pixel is 1.8154195970894549\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# let's create a final nn_regressor for pix52\n",
    "#\n",
    "pix52_final_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                                    max_iter=400, \n",
    "                                    activation=\"tanh\",\n",
    "                                    solver='sgd', \n",
    "                                    verbose=False, \n",
    "                                    shuffle=True,\n",
    "                                    random_state=None, # reproduceability!\n",
    "                                    learning_rate_init=.1, \n",
    "                                    learning_rate = 'adaptive')\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "pix52_final_regressor.fit(X_all_scaled, y_all_scaled)\n",
    "print(\"\\n\\n++++++++++  TRAINING:   end  +++++++++++++++\\n\\n\")\n",
    "\n",
    "print(f\"The (sq) prediction error (the loss) is {pix52_final_regressor.loss_}\") \n",
    "print(f\"So, the 'average' error per pixel is {pix52_final_regressor.loss_**0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numeral is a 2\n",
      "\n",
      "pix52 [predicted] vs. actual:  [13.09002417] vs. 15.0\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# and, let's be sure we can use our \"finalized\" model:\n",
    "#\n",
    "\n",
    "def predict_from_model(pixels, model):\n",
    "    \"\"\" returns the prediction on the input pixels using the input model\n",
    "    \"\"\"\n",
    "    pixels_array = np.asarray([pixels])   # the extra sq. brackets are needed!\n",
    "    pixels_scaled = scaler.transform(pixels_array)  # need to use the scaler!\n",
    "    predicted = model.predict(pixels_scaled)\n",
    "    return predicted\n",
    "\n",
    "#\n",
    "# let's choose a digit to try...\n",
    "#\n",
    "row_to_show = 4                         # different indexing from X_all and y_all (they were reordered)\n",
    "numeral = A[row_to_show,64]\n",
    "print(f\"The numeral is a {int(numeral)}\\n\")\n",
    "\n",
    "all_pixels = A[row_to_show,0:64] \n",
    "first48pixels = A[row_to_show,0:48] \n",
    "\n",
    "pix52_predicted = predict_from_model(first48pixels,pix52_final_regressor)\n",
    "pix52_actual = A[row_to_show,52]\n",
    "\n",
    "print(f\"pix52 [predicted] vs. actual:  {pix52_predicted} vs. {pix52_actual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's visualize!   Here's the idea: \n",
    "# \n",
    "# Choose a row index (row_to_show)\n",
    "# Show the original digit\n",
    "# Show the original digit with pix52 replaced (may not be noticeable...)\n",
    "# show the original digit with the bottom-two rows zero'ed out _except_ pix 52 :-)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's create a function to show one digit\n",
    "#\n",
    "\n",
    "def show_digit( pixels ):\n",
    "    \"\"\" should create a heatmap (image) of the digit contained in row \n",
    "            input: pixels should be a 1d numpy array\n",
    "            if it's more then 64 values, it will be truncated\n",
    "            if it's fewer than 64 values, 0's will be appended\n",
    "            \n",
    "    \"\"\"\n",
    "    # make sure the sizes are ok!\n",
    "    num_pixels = len(pixels)\n",
    "    if num_pixels != 64:\n",
    "        print(f\"(in show_digit) num_pixels was {num_pixels}; now set to 64\")\n",
    "    if num_pixels > 64:   # an elif would be a poor choice here, as I found!\n",
    "        pixels = pixels[0:64]\n",
    "    if num_pixels < 64:   \n",
    "        num_zeros = 64-len(pixels)\n",
    "        pixels = np.concatenate( (pixels, np.zeros(num_zeros)), axis=0 )\n",
    "        \n",
    "    pixels = pixels.astype(int)         # convert to integers for plotting\n",
    "    pixels = np.reshape(pixels, (8,8))  # make 8x8\n",
    "    # print(f\"The pixels are\\n{pixels}\")  \n",
    "    f, ax = plt.subplots(figsize=(9, 6))  # Draw a heatmap w/option of numeric values in each cell\n",
    "    \n",
    "    #my_cmap = sns.dark_palette(\"Purple\", as_cmap=True)\n",
    "    my_cmap = sns.light_palette(\"Gray\", as_cmap=True)    # all seaborn palettes: medium.com/@morganjonesartist/color-guide-to-seaborn-palettes-da849406d44f\n",
    "    # plot! annot=True to see the values...   palettes listed at very bottom of this notebook\n",
    "    sns.heatmap(pixels, annot=False, fmt=\"d\", linewidths=.5, ax=ax, cmap=my_cmap) # 'seismic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numeral is a 6\n",
      "\n",
      "pix52 [predicted] vs. actual:  [5.17208096] 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/fcml8myd34d74tm8j9ww_l600000gn/T/ipykernel_50454/3729244964.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  all_pixels[52] = np.round(pix52_predicted)    # include this one\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Another example of predicting one pixel\n",
    "#\n",
    "row_to_show = 42                         \n",
    "numeral = A[row_to_show,64]\n",
    "print(f\"The numeral is a {int(numeral)}\\n\")\n",
    "# show all from the original data\n",
    "show_digit( A[row_to_show,0:64] )   # show full original\n",
    "\n",
    "all_pixels = A[row_to_show,0:64].copy()\n",
    "first48pixels = all_pixels[0:48] \n",
    "\n",
    "pix52_predicted = predict_from_model(first48pixels,pix52_final_regressor)\n",
    "pix52_actual = A[row_to_show,52]\n",
    "\n",
    "print(f\"pix52 [predicted] vs. actual:  {pix52_predicted} {pix52_actual}\")\n",
    "\n",
    "# erase last 16 pixels\n",
    "all_pixels[48:64] = np.zeros(16)\n",
    "\n",
    "# show without pix52\n",
    "all_pixels[52] = 0         # omit this one\n",
    "show_digit( all_pixels )   # show without pixel 52\n",
    "\n",
    "# show with pix52\n",
    "all_pixels[52] = np.round(pix52_predicted)    # include this one\n",
    "show_digit( all_pixels )   # show with pixel 52\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Your first task:\n",
    "+ create regressors for _all_ of the pixels on the bottom two rows\n",
    "+ Use a loop! 16 times is too many for copy-paste-edit!\n",
    "\n",
    "#### Second, create a slight variation on the visualization above, so that \n",
    "+ your system \"dreams\" all 16 of the bottom two rows of pixels\n",
    "+ You will be able to see the dreamt digit alongside the real digit\n",
    "+ <font color=\"Coral\"><b>Show three examples (real and hallucinated!)</b></font> &nbsp;&nbsp; Choose three digits from the dataset and show the original vs. the digit-dreamt version alongside each other / side-by-side in this way. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Adapt from the previous example:\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regressor for pixel 48\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 48.\n",
      "Training completed for pixel 48\n",
      "Train MSE: 0.0516, Test MSE: 0.0009\n",
      "\n",
      "Regressor for pixel 49\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 49.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (342) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for pixel 49\n",
      "Train MSE: 0.6774, Test MSE: 2.2618\n",
      "\n",
      "Regressor for pixel 50\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 50.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (342) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for pixel 50\n",
      "Train MSE: 9.1151, Test MSE: 13.5189\n",
      "\n",
      "Regressor for pixel 51\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 51.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (342) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for pixel 51\n",
      "Train MSE: 8.5012, Test MSE: 13.9096\n",
      "\n",
      "Regressor for pixel 52\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 52.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (342) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for pixel 52\n",
      "Train MSE: 7.6964, Test MSE: 13.6375\n",
      "\n",
      "Regressor for pixel 53\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 53.\n",
      "Training completed for pixel 53\n",
      "Train MSE: 10.9087, Test MSE: 14.5221\n",
      "\n",
      "Regressor for pixel 54\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 54.\n",
      "Training completed for pixel 54\n",
      "Train MSE: 3.9255, Test MSE: 8.8269\n",
      "\n",
      "Regressor for pixel 55\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 55.\n",
      "Training completed for pixel 55\n",
      "Train MSE: 0.3928, Test MSE: 0.6157\n",
      "\n",
      "Regressor for pixel 56\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 56.\n",
      "Training completed for pixel 56\n",
      "Train MSE: 0.0007, Test MSE: 0.0001\n",
      "\n",
      "Regressor for pixel 57\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 57.\n",
      "Training completed for pixel 57\n",
      "Train MSE: 0.0790, Test MSE: 0.2696\n",
      "\n",
      "Regressor for pixel 58\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 58.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (342) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for pixel 58\n",
      "Train MSE: 1.7802, Test MSE: 4.5305\n",
      "\n",
      "Regressor for pixel 59\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 59.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (342) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for pixel 59\n",
      "Train MSE: 3.6766, Test MSE: 6.1929\n",
      "\n",
      "Regressor for pixel 60\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 60.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (342) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for pixel 60\n",
      "Train MSE: 7.8583, Test MSE: 14.9591\n",
      "\n",
      "Regressor for pixel 61\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 61.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (342) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for pixel 61\n",
      "Train MSE: 14.1590, Test MSE: 18.5866\n",
      "\n",
      "Regressor for pixel 62\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 62.\n",
      "Training completed for pixel 62\n",
      "Train MSE: 6.9395, Test MSE: 12.7038\n",
      "\n",
      "Regressor for pixel 63\n",
      "Training with 1414 rows; testing with 354 rows\n",
      "Training regressor for pixel 63.\n",
      "Training completed for pixel 63\n",
      "Train MSE: 0.8445, Test MSE: 1.0128\n"
     ]
    }
   ],
   "source": [
    "pixel_models = {}\n",
    "for pixel_index in range(48, 64):\n",
    "    print(f\"\\nRegressor for pixel {pixel_index}\")\n",
    "    \n",
    "    X_all = A[:, 0:48] #first 48 pixels\n",
    "    y_all = A[:, pixel_index]\n",
    "    \n",
    "    indices = np.random.permutation(len(y_all))\n",
    "    X_all = X_all[indices]\n",
    "    y_all = y_all[indices]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "    print(f\"Training with {len(y_train)} rows; testing with {len(y_test)} rows\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    nn_regressor = MLPRegressor(\n",
    "        hidden_layer_sizes=(6, 7), \n",
    "        max_iter=342,          \n",
    "        activation=\"tanh\",     \n",
    "        solver='sgd',          \n",
    "        verbose=False,         # Set to True to see training progress\n",
    "        shuffle=True,          \n",
    "        random_state=None,     \n",
    "        learning_rate_init=.1, \n",
    "        learning_rate='adaptive'\n",
    "    )\n",
    "\n",
    "    print(f\"Training regressor for pixel {pixel_index}.\")\n",
    "    nn_regressor.fit(X_train_scaled, y_train)\n",
    "    print(f\"Training completed for pixel {pixel_index}\")\n",
    "    \n",
    "    pred_train = nn_regressor.predict(X_train_scaled)\n",
    "    train_error = np.mean((pred_train - y_train) ** 2)\n",
    "    \n",
    "    pred_test = nn_regressor.predict(X_test_scaled)\n",
    "    test_error = np.mean((pred_test - y_test) ** 2)\n",
    "    \n",
    "    print(f\"Train MSE: {train_error:.4f}, Test MSE: {test_error:.4f}\")\n",
    "    \n",
    "    pixel_models[pixel_index] = {\n",
    "        'regressor': nn_regressor,\n",
    "        'scaler': scaler\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second task:\n",
    "\n",
    "#### Then, predict __four__ rows, instead of two\n",
    "+ Then, repeat/expand this process so that your code predicts the bottom **four** rows, instead of two\n",
    "+ This time, you'll train and create 32 regressors (wow!)\n",
    "+ Again, choose three digits (perhaps the same ones) and show what the system dreams for their four bottom rows vs. the actual!\n",
    "+ For EC, try the bottom <b>six</b> rows -- or try the actually-unknown digits in <tt>partial_digits.csv</tt> -- and, perhaps, then try to classify the full digits the system generates from those partial ones... !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "More details:     Your task is to expand this process, so that...\n",
    "\n",
    "[1]\n",
    "  + Your system can predict the value of _any_ of the last 16 pixels from the first 48\n",
    "  +     Make sure the pixels-used and pixels-predicted are easily changed,\n",
    "  +     Because this problem also asks you to predict the last 32 pixels (from the first 32...)\n",
    "[2]\n",
    "  + Next, predict the value of _any_ of the last 32 pixels from the first 32\n",
    "  +     This will be smooth if the earlier step is modular + easily changeable\n",
    "[3]\n",
    "  + Create \"dreamed-digit\" images for four digits (your choice)\n",
    "  +     Use the visualization cells above and below as starting points\n",
    "  +     Make sure two out of the four use 48 pixels and predict 16\n",
    "  +     Make sure two out of the four use 32 pixels and predict 32\n",
    "[4a]\n",
    "  + Extra!  Read in the file 'partial_digits.csv'\n",
    "  +     there are 10 digits with _only_ the first six rows (48 pixels)   [the last 16 are artificially set to 0]\n",
    "  +     there are 10 digits with _only_ the first four rows (32 pixels)  [the last 32 are artificially set to 0]\n",
    "  +     And, hallucinate the missing data! (just as above)  Visualize!\n",
    "[4b]\n",
    "  + Extra!  Then, _Classify_ those newly-imputed digits, using the \"dreamt pixels\"\n",
    "  +     (Remember you created a classification network in pr2.)\n",
    "  +     How does it do?\n",
    "  +     Compare with how it does if you leave the zeros in the data...\n",
    "\n",
    "\n",
    "Steps [1]-[3] is an example of \"imputing\" missing data, and then the EC uses this hallucinated (imputed) data \n",
    "so that the original digit-classifier would work.\n",
    "\n",
    "_Any_ modeling technique can be used to impute missing data. It can be complex (NNet or RF)\n",
    "or very simple, e.g., replace missing data with the average value of that feature in the dataset. \n",
    "\n",
    "In this spirit, check out Google's \"Deep Dream\" and its \"Inceptionism\" gallery, e.g.,\n",
    "https://photos.google.com/share/AF1QipPX0SCl7OzWilt9LnuQliattX4OUCj_8EP65_cTVnBmS1jnYgsGQAieQUc1VQWdgQ?pli=1&key=aVBxWjhwSzg2RjJWLWRuVFBBZEN1d205bUdEMnhB\n",
    "\n",
    "in which the effects (the weights) learned by the network are allowed to \"spill out\" into other images.\n",
    "This is different than the _generated-image_ artifacts (now familiar) ...\n",
    "\n",
    "Here, it's the weights themselves that are _intentionally_ visualized!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pixel_models(A, first_n_pixels, predict_range):\n",
    "    \"\"\"\n",
    "    Train nn to predict pixels in predict_range using first_n_pixels.\n",
    "    \"\"\"\n",
    "    pixel_models = {}\n",
    "    \n",
    "    for pixel_index in predict_range:\n",
    "        print(f\"\\nRegressor for pixel {pixel_index}\")\n",
    "        \n",
    "        X_all = A[:, 0:first_n_pixels]  # Use first_n_pixels as features\n",
    "        y_all = A[:, pixel_index]       # Predict this pixel\n",
    "        \n",
    "        indices = np.random.permutation(len(y_all))\n",
    "        X_all = X_all[indices]\n",
    "        y_all = y_all[indices]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "        print(f\"Training with {len(y_train)} rows; testing with {len(y_test)} rows\")\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        nn_regressor = MLPRegressor(\n",
    "            hidden_layer_sizes=(6, 7), \n",
    "            max_iter=342,          \n",
    "            activation=\"tanh\",     \n",
    "            solver='sgd',          \n",
    "            verbose=False,         \n",
    "            shuffle=True,          \n",
    "            random_state=None,     \n",
    "            learning_rate_init=.1, \n",
    "            learning_rate='adaptive'\n",
    "        )\n",
    "\n",
    "        print(f\"Training regressor for pixel {pixel_index}.\")\n",
    "        nn_regressor.fit(X_train_scaled, y_train)\n",
    "        print(f\"Training completed for pixel {pixel_index}\")\n",
    "\n",
    "        pred_train = nn_regressor.predict(X_train_scaled)\n",
    "        train_error = np.mean((pred_train - y_train) ** 2)\n",
    "        \n",
    "        pred_test = nn_regressor.predict(X_test_scaled)\n",
    "        test_error = np.mean((pred_test - y_test) ** 2)\n",
    "        \n",
    "        print(f\"Train MSE: {train_error:.4f}, Test MSE: {test_error:.4f}\")\n",
    "        \n",
    "        pixel_models[pixel_index] = {\n",
    "            'regressor': nn_regressor,\n",
    "            'scaler': scaler\n",
    "        }\n",
    "    return pixel_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dream_digit(digit_data, pixel_models, first_n_pixels, predict_range):\n",
    "    \"\"\"\n",
    "    Predict missing pixels in a digit\n",
    "    \"\"\"\n",
    "    completed_digit = digit_data.copy()\n",
    "    \n",
    "    features = digit_data[0:first_n_pixels].reshape(1, -1)\n",
    "    \n",
    "    for pixel_index in predict_range:\n",
    "        model = pixel_models[pixel_index]['regressor']\n",
    "        scaler = pixel_models[pixel_index]['scaler']\n",
    "        \n",
    "        features_scaled = scaler.transform(features)\n",
    "        prediction = model.predict(features_scaled)[0]\n",
    "        \n",
    "        completed_digit[pixel_index] = prediction\n",
    "    \n",
    "    return completed_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_digit_completion(original_digit, completed_digit, num_rows=8, num_cols=8, title=\"Digit Completion\"):\n",
    "    \"\"\"\n",
    "    Visualize original and predicted digits side by side.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Original digit\n",
    "    axes[0].imshow(original_digit.reshape(num_rows, num_cols), cmap='gray')\n",
    "    axes[0].set_title('Original Digit')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Completed digit\n",
    "    axes[1].imshow(completed_digit.reshape(num_rows, num_cols), cmap='gray')\n",
    "    axes[1].set_title('Dreamed Digit')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_predicted_area(A, digit_idx, input_pixels, output_pixels, nrows=8, ncols=8):\n",
    "    \"\"\"\n",
    "    Visualize the input and output regions of a digit.\n",
    "    \"\"\"\n",
    "    digit = A[digit_idx].reshape(nrows, ncols)\n",
    "    \n",
    "    input_mask = np.zeros((nrows, ncols))\n",
    "    output_mask = np.zeros((nrows, ncols))\n",
    "    \n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            pixel_idx = i * ncols + j\n",
    "            if pixel_idx in input_pixels:\n",
    "                input_mask[i, j] = 1\n",
    "            if pixel_idx in output_pixels:\n",
    "                output_mask[i, j] = 1\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Original digit\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(digit, cmap='gray')\n",
    "    plt.title(f\"Original Digit (idx={digit_idx})\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Input \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(digit, cmap='gray')\n",
    "    plt.imshow(np.ma.masked_array(digit, mask=1-input_mask), cmap='Blues', alpha=0.5)\n",
    "    plt.title(\"Input Region\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Output \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(digit, cmap='gray')\n",
    "    plt.imshow(np.ma.masked_array(digit, mask=1-output_mask), cmap='Reds', alpha=0.5)\n",
    "    plt.title(\"Output Region (to be predicted)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m input_pixels_scenario1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m48\u001b[39m)\n\u001b[1;32m      3\u001b[0m output_pixels_scenario1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m pixel_models_scenario1 \u001b[38;5;241m=\u001b[39m train_pixel_models(\u001b[43mA\u001b[49m, input_pixels_scenario1, output_pixels_scenario1)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#predict bottom 4 rows (32 pixels) from top 4 rows (32 pixels)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m input_pixels_scenario2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "#predict bottom 2 rows (16 pixels) from top 6 rows (48 pixels)\n",
    "input_pixels_scenario1 = range(0, 48)\n",
    "output_pixels_scenario1 = range(48, 64)\n",
    "pixel_models_scenario1 = train_pixel_models(A, input_pixels_scenario1, output_pixels_scenario1)\n",
    "\n",
    "#predict bottom 4 rows (32 pixels) from top 4 rows (32 pixels)\n",
    "input_pixels_scenario2 = range(0, 32)\n",
    "output_pixels_scenario2 = range(32, 64)\n",
    "pixel_models_scenario2 = train_pixel_models(A, input_pixels_scenario2, output_pixels_scenario2)\n",
    "\n",
    "#digits to dream (e.g., digits 0, 1, 2, 3)\n",
    "digit_indices = [42, 123, 456, 789]  # Replace with actual indices of digits from your dataset\n",
    "\n",
    "#dream two digits using scenario 1 (48 -> 16)\n",
    "dreams1 = dream_digits(A, digit_indices[:2], pixel_models_scenario1, \n",
    "                      input_pixels_scenario1, output_pixels_scenario1)\n",
    "\n",
    "#dream two digits using scenario 2 (32 -> 32)\n",
    "dreams2 = dream_digits(A, digit_indices[2:], pixel_models_scenario2,\n",
    "                      input_pixels_scenario2, output_pixels_scenario2)\n",
    "\n",
    "#combine results and visualize\n",
    "all_dreams = {**dreams1, **dreams2}\n",
    "visualize_dreams(all_dreams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
