# Welcome to cs35, hw1pr0 ! #### Homework 1, problem 0 is a "reading-and-response problem"
+ There is usually one of these each week
+ It often reflects on the programming portion of that week...
+ **Response**    We ask you to compose a response to each article. It can be short (4-5 sentences is great), and the goal is to engage with the article, as well as to incorporate your own thoughts and experiences into your response. 
  + Include your response in the cell at the bottom of this notebook
  + Then, submit this notebook `hw1pr0.ipynb` to the submission site

<br>

+ To get familiar with Gradescope, feel free to submit this notebook, for the moment, _without_ the reading-response 
  + It's always possible to re-submit a file to Gradescope.
  + Gradescope keeps all submitted versions; the graders see the last version submitted

<br>

<hr>### The reading

Similar in spirit to cs5's reading/response assignments, each week there will be a short article overlapping that week's topics (sometimes broadly, other times narrowly). 

This week has "1+" readings. 

<br>

The "1" of "1+" comes from a short Nature article that describes a "credit-blame asymmetry" caused by AI, i.e., LLMs.  That is, LLMs make it more difficult to feel that creditable work is being accomplished -- but they do not change the standards for blame.  This is timely, seeing as this class -- and so much else -- is about using and exploring AI.  First, the article:

+ The article:     [Generative AI entails a creditâ€“blame asymmetry](https://www.nature.com/articles/s42256-023-00653-1)  &nbsp;&nbsp;   [pdf](https://drive.google.com/file/d/1rgIBOjld0N6QZpkI6G4Vm4YZAFVyYQe_/view?usp=drive_link)  &nbsp;&nbsp;     <font size="-2">(Oxford's  <a href="https://www.ox.ac.uk/news/2023-05-05-tackling-ethical-dilemma-responsibility-large-language-models">overview</a>  of the article)</font>

<br>

The prompt below will ask if you sense the "asymmetry" the article claims. But, first, the "+" of "1+":

The "+" of "1+" is the first two sections, 1.1 and 1.2, of the [ACM's Code of Ethics and Professional Conduct](https://ethics.acm.org/). The ACM is the Association for Computing Machinery and  is the world's largest professional-computing group. In 2018, the ACM released "the Code," its broad ethical guide for all computing work:  

+ Read sections 1.1 and 1.2  of "[the Code](https://ethics.acm.org/)."  More, of course, if you'd like...

<br><hr><br>
#### The prompt

As each week, this "problem 0" asks you to compose a one-paragraph reflection (~5 sentences, give or take), with the goal of bridging your personal experiences with the ideas in the articles. 

This week's prompts:

<br>

(a) The Nature article - and its overview - suggests that society is "justified in holding persons accountable for deliberate or careless errors in generated text if they put such text to use in ways that negatively impact others"  [<font color="DodgerBlue">the "blame" side</font>] and also that society "might not think people deserve credit for text generated without much skill and effort, such as ChatGPT-generated exam papers" [<font color="DodgerBlue">the "credit" side</font>].  <font color="Coral">Do you agree with neither, either, or both of these two "sides" of the article's "credit-blame asymmetry"? In a sentence or two, elaborate how/why</font>

(b) The ACM Code of Ethics, especially sections 1.1 (<i>Contribute...</i>) and 1.2 (<i>Avoid harm</i>), mirror this credit-blame dichotomy.  With these principles in mind, <font color="Coral">do you feel that, in particular, AI systems are ethics-promoting? detracting? neutral? amplifying what's already there?</font>  What changes to norms or expectations would you like to see around AI systems, credit for creative work, and blame for misuse? 

<br>

Here, and in general, your response should incorporate ideas from the article(s), connecting them with your own thoughts, takes, and experiences. 

And - you can always add detours, if you'd like ...

<br><hr><br>#### The Response
I agree with the two sides of the article's credit blame asymmetry as more responsibility is placed in our (human) hands. These LLM outputs are predictions based on patterns we feed them and thus require finer care in catching mistakes or misinformation if any. We should look to use LLMs as assistants so that we may reach higher insights, not taking what LLMs give us as is. 

I feel that AI systems are and can be all of theh above. I've heard stories in which AI systems are largely misused in personal life and even lethal. We've had professors that encourage and some that disallow the use of AI for academic work. I'd like to see a collective effort to understand and teach the proper use of AI systems to thus really honor the ACM Code of Ethics
<br><br>
<hr>
<br>

Additional thoughts on LLMs: not necessary for cs35.

In fact, we will explore more about the technical workings of LLMs around weeks 8-9.

+ [LLMs don't have a coherent model of the world](https://www.lesswrong.com/posts/wkws2WgraeN8AYJjv/llms-don-t-have-a-coherent-model-of-the-world-what-it-means) &nbsp; Interesting...
+ [Human Immortality using LLMs](https://danielmiessler.com/p/human-immortality-using-llms) &nbsp; Is that word warranted? ...
+ [LLM Programs](https://mpost.io/llm-programs-the-new-path-to-fine-tuning-neural-models-in-complex-situations/) &nbsp; An overview article
+ [Large language model programs](https://arxiv.org/abs/2305.05364) &nbsp; An Arxiv article
