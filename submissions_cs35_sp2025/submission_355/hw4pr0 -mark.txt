<br>

#### week4 ~ <i>... and into the data we dive ...</i>  &nbsp;&nbsp; (hw4pr0.ipynb)

<a href="https://docs.google.com/document/d/1OdQ-01Gp7XAk9vbZ61zM3CaTdGsZVWEvZxgu1Z0toSY/edit?tab=t.0">This week's hw page</a>

<hr><br>

#### Reading for hw4...     (hw4pr0.ipynb)

As we transition into AI/Machine Learning for the next few weeks, this week's reading takes a more <u>policy-based</u>, rather than socially-normed, view:
+ it's a NYTimes article from a few years ago -- nice, because it has a definite, wide-angle stance on regulating AI:
+ [How to Regulate Artificial Intelligence](https://www.nytimes.com/2017/09/01/opinion/artificial-intelligence-regulations-rules.html)
+ [locally-hosted pdf copy](https://drive.google.com/file/d/1EZcygQdk40J0dJTZ1vp0F20rIoQU27nH/view)

Consider the author's three proposed principles, which elaborate Isaac Asimov's "laws":
+ AIs must obey human society's laws
+ AIs must disclose themselves as such
+ AIs cannot retain confidential information (w/o permission)

Choose one (or more) of these principles with which to agree or disagree, bringing in your own experience and perspective. 

Alternative paths and balancing acts are, as always, welcome! For example -- this article was written before conversational AI was a reality. Do you feel there are differences in the appropriate principles or guidance for regulating today's AI agents and their authors?

<hr>#### Reading response

I think the first proposed law--that AIs must obey human society's laws--is a super interesting one. There are some laws that seem easier to program than others. For example, having an autonomous car stop at a red light seems straightforward enough. However, other laws are much harder, for example that it is against the law to threaten another person. I personally believe that AI cannot 'learn' was threatening is. It can be trained to identify words or phrases that are usually labeled as threatening, but because it does not have human vulnerabilities or fears, it will never fully understand what a threat is. So it could generate text that I may perceive as threatening, but have no awareness or intention of that. Even with additional training to make the output of a threatening (or otherwise illegal) statement less and less likely, it seems impossible to enforce certain laws, let alone hold the right entity accountable. Overall, I agree with the principle, but do not see it as implementable. 