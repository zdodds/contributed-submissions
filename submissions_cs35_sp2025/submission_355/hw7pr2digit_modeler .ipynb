{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "###   <font color=\"Coral\">hw7pr2digits_modeler</font>\n",
    "+ digits clasification -- and regression -- via NNets\n",
    "\n",
    "Feel free to re-use your previous digits_cleaned.csv file \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, use the iris example to create a digit-predicting NNet\n",
    "\n",
    "This is similar to the past two digits challenges, hw5 and hw6\n",
    "\n",
    "**However**, because we're using NNets, you'll need to\n",
    "+ create TEN categorical variables. You can use just one ``get_dummies`` pandas call\n",
    "+ use a SCALER to make sure the network can, in a fair way, \"hear\" all of the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# coding cells, for copy-paste-and-adapt...\n",
    "\n",
    "# libraries...\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd     # Pandas is Python's \"data\" library (\"dataframe\" == spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits_cleaned.csv : file read into a pandas dataframe.\n"
     ]
    }
   ],
   "source": [
    "filename = 'digits_cleaned.csv' # neighborhoods\n",
    "df_tidy = pd.read_csv(filename)      # encoding = \"utf-8\", \"latin1\"\n",
    "print(f\"{filename} : file read into a pandas dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_tidy.shape is (1768, 65)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1768 entries, 0 to 1767\n",
      "Data columns (total 65 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   pix0          1768 non-null   int64\n",
      " 1   pix1          1768 non-null   int64\n",
      " 2   pix2          1768 non-null   int64\n",
      " 3   pix3          1768 non-null   int64\n",
      " 4   pix4          1768 non-null   int64\n",
      " 5   pix5          1768 non-null   int64\n",
      " 6   pix6          1768 non-null   int64\n",
      " 7   pix7          1768 non-null   int64\n",
      " 8   pix8          1768 non-null   int64\n",
      " 9   pix9          1768 non-null   int64\n",
      " 10  pix10         1768 non-null   int64\n",
      " 11  pix11         1768 non-null   int64\n",
      " 12  pix12         1768 non-null   int64\n",
      " 13  pix13         1768 non-null   int64\n",
      " 14  pix14         1768 non-null   int64\n",
      " 15  pix15         1768 non-null   int64\n",
      " 16  pix16         1768 non-null   int64\n",
      " 17  pix17         1768 non-null   int64\n",
      " 18  pix18         1768 non-null   int64\n",
      " 19  pix19         1768 non-null   int64\n",
      " 20  pix20         1768 non-null   int64\n",
      " 21  pix21         1768 non-null   int64\n",
      " 22  pix22         1768 non-null   int64\n",
      " 23  pix23         1768 non-null   int64\n",
      " 24  pix24         1768 non-null   int64\n",
      " 25  pix25         1768 non-null   int64\n",
      " 26  pix26         1768 non-null   int64\n",
      " 27  pix27         1768 non-null   int64\n",
      " 28  pix28         1768 non-null   int64\n",
      " 29  pix29         1768 non-null   int64\n",
      " 30  pix30         1768 non-null   int64\n",
      " 31  pix31         1768 non-null   int64\n",
      " 32  pix32         1768 non-null   int64\n",
      " 33  pix33         1768 non-null   int64\n",
      " 34  pix34         1768 non-null   int64\n",
      " 35  pix35         1768 non-null   int64\n",
      " 36  pix36         1768 non-null   int64\n",
      " 37  pix37         1768 non-null   int64\n",
      " 38  pix38         1768 non-null   int64\n",
      " 39  pix39         1768 non-null   int64\n",
      " 40  pix40         1768 non-null   int64\n",
      " 41  pix41         1768 non-null   int64\n",
      " 42  pix42         1768 non-null   int64\n",
      " 43  pix43         1768 non-null   int64\n",
      " 44  pix44         1768 non-null   int64\n",
      " 45  pix45         1768 non-null   int64\n",
      " 46  pix46         1768 non-null   int64\n",
      " 47  pix47         1768 non-null   int64\n",
      " 48  pix48         1768 non-null   int64\n",
      " 49  pix49         1768 non-null   int64\n",
      " 50  pix50         1768 non-null   int64\n",
      " 51  pix51         1768 non-null   int64\n",
      " 52  pix52         1768 non-null   int64\n",
      " 53  pix53         1768 non-null   int64\n",
      " 54  pix54         1768 non-null   int64\n",
      " 55  pix55         1768 non-null   int64\n",
      " 56  pix56         1768 non-null   int64\n",
      " 57  pix57         1768 non-null   int64\n",
      " 58  pix58         1768 non-null   int64\n",
      " 59  pix59         1768 non-null   int64\n",
      " 60  pix60         1768 non-null   int64\n",
      " 61  pix61         1768 non-null   int64\n",
      " 62  pix62         1768 non-null   int64\n",
      " 63  pix63         1768 non-null   int64\n",
      " 64  actual_digit  1768 non-null   int64\n",
      "dtypes: int64(65)\n",
      "memory usage: 897.9 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix0</th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>...</th>\n",
       "      <th>pix55</th>\n",
       "      <th>pix56</th>\n",
       "      <th>pix57</th>\n",
       "      <th>pix58</th>\n",
       "      <th>pix59</th>\n",
       "      <th>pix60</th>\n",
       "      <th>pix61</th>\n",
       "      <th>pix62</th>\n",
       "      <th>pix63</th>\n",
       "      <th>actual_digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1768 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pix0  pix1  pix2  pix3  pix4  pix5  pix6  pix7  pix8  pix9  ...  pix55  \\\n",
       "0        0     0     9    14     8     1     0     0     0     0  ...      0   \n",
       "1        0     0    11    12     0     0     0     0     0     2  ...      0   \n",
       "2        0     0     1     9    15    11     0     0     0     0  ...      0   \n",
       "3        0     0     0     0    14    13     1     0     0     0  ...      0   \n",
       "4        0     0     5    12     1     0     0     0     0     0  ...      2   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "1763     0     0     4    10    13     6     0     0     0     1  ...      0   \n",
       "1764     0     0     6    16    13    11     1     0     0     0  ...      0   \n",
       "1765     0     0     1    11    15     1     0     0     0     0  ...      0   \n",
       "1766     0     0     2    10     7     0     0     0     0     0  ...      0   \n",
       "1767     0     0    10    14     8     1     0     0     0     2  ...      0   \n",
       "\n",
       "      pix56  pix57  pix58  pix59  pix60  pix61  pix62  pix63  actual_digit  \n",
       "0         0      0     11     16     15     11      1      0             8  \n",
       "1         0      0      9     12     13      3      0      0             9  \n",
       "2         0      0      1     10     13      3      0      0             0  \n",
       "3         0      0      0      1     13     16      1      0             1  \n",
       "4         0      0      3     11      8     13     12      4             2  \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...           ...  \n",
       "1763      0      0      2     14     15      9      0      0             9  \n",
       "1764      0      0      6     16     14      6      0      0             0  \n",
       "1765      0      0      2      9     13      6      0      0             8  \n",
       "1766      0      0      5     12     16     12      0      0             9  \n",
       "1767      0      1      8     12     14     12      1      0             8  \n",
       "\n",
       "[1768 rows x 65 columns]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"df_tidy.shape is {df_tidy.shape}\\n\")\n",
    "df_tidy.info()  # prints column information\n",
    "df_tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix0</th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>...</th>\n",
       "      <th>is_0</th>\n",
       "      <th>is_1</th>\n",
       "      <th>is_2</th>\n",
       "      <th>is_3</th>\n",
       "      <th>is_4</th>\n",
       "      <th>is_5</th>\n",
       "      <th>is_6</th>\n",
       "      <th>is_7</th>\n",
       "      <th>is_8</th>\n",
       "      <th>is_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1768 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pix0  pix1  pix2  pix3  pix4  pix5  pix6  pix7  pix8  pix9  ...   is_0  \\\n",
       "0        0     0     9    14     8     1     0     0     0     0  ...  False   \n",
       "1        0     0    11    12     0     0     0     0     0     2  ...  False   \n",
       "2        0     0     1     9    15    11     0     0     0     0  ...   True   \n",
       "3        0     0     0     0    14    13     1     0     0     0  ...  False   \n",
       "4        0     0     5    12     1     0     0     0     0     0  ...  False   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "1763     0     0     4    10    13     6     0     0     0     1  ...  False   \n",
       "1764     0     0     6    16    13    11     1     0     0     0  ...   True   \n",
       "1765     0     0     1    11    15     1     0     0     0     0  ...  False   \n",
       "1766     0     0     2    10     7     0     0     0     0     0  ...  False   \n",
       "1767     0     0    10    14     8     1     0     0     0     2  ...  False   \n",
       "\n",
       "       is_1   is_2   is_3   is_4   is_5   is_6   is_7   is_8   is_9  \n",
       "0     False  False  False  False  False  False  False   True  False  \n",
       "1     False  False  False  False  False  False  False  False   True  \n",
       "2     False  False  False  False  False  False  False  False  False  \n",
       "3      True  False  False  False  False  False  False  False  False  \n",
       "4     False   True  False  False  False  False  False  False  False  \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "1763  False  False  False  False  False  False  False  False   True  \n",
       "1764  False  False  False  False  False  False  False  False  False  \n",
       "1765  False  False  False  False  False  False  False   True  False  \n",
       "1766  False  False  False  False  False  False  False  False   True  \n",
       "1767  False  False  False  False  False  False  False   True  False  \n",
       "\n",
       "[1768 rows x 74 columns]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tidy_cat = pd.get_dummies(data=df_tidy,prefix=\"is\",columns=['actual_digit'])\n",
    "df_tidy_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix0</th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>...</th>\n",
       "      <th>is_0</th>\n",
       "      <th>is_1</th>\n",
       "      <th>is_2</th>\n",
       "      <th>is_3</th>\n",
       "      <th>is_4</th>\n",
       "      <th>is_5</th>\n",
       "      <th>is_6</th>\n",
       "      <th>is_7</th>\n",
       "      <th>is_8</th>\n",
       "      <th>is_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1768 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pix0  pix1  pix2  pix3  pix4  pix5  pix6  pix7  pix8  pix9  ...   is_0  \\\n",
       "0        0     0     9    14     8     1     0     0     0     0  ...  False   \n",
       "1        0     0    11    12     0     0     0     0     0     2  ...  False   \n",
       "2        0     0     1     9    15    11     0     0     0     0  ...   True   \n",
       "3        0     0     0     0    14    13     1     0     0     0  ...  False   \n",
       "4        0     0     5    12     1     0     0     0     0     0  ...  False   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "1763     0     0     4    10    13     6     0     0     0     1  ...  False   \n",
       "1764     0     0     6    16    13    11     1     0     0     0  ...   True   \n",
       "1765     0     0     1    11    15     1     0     0     0     0  ...  False   \n",
       "1766     0     0     2    10     7     0     0     0     0     0  ...  False   \n",
       "1767     0     0    10    14     8     1     0     0     0     2  ...  False   \n",
       "\n",
       "       is_1   is_2   is_3   is_4   is_5   is_6   is_7   is_8   is_9  \n",
       "0     False  False  False  False  False  False  False   True  False  \n",
       "1     False  False  False  False  False  False  False  False   True  \n",
       "2     False  False  False  False  False  False  False  False  False  \n",
       "3      True  False  False  False  False  False  False  False  False  \n",
       "4     False   True  False  False  False  False  False  False  False  \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "1763  False  False  False  False  False  False  False  False   True  \n",
       "1764  False  False  False  False  False  False  False  False  False  \n",
       "1765  False  False  False  False  False  False  False   True  False  \n",
       "1766  False  False  False  False  False  False  False  False   True  \n",
       "1767  False  False  False  False  False  False  False   True  False  \n",
       "\n",
       "[1768 rows x 74 columns]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROW = 0\n",
    "COLUMN = 1\n",
    "df_model1 = df_tidy_cat #.drop('actual_digit', axis=COLUMN )\n",
    "df_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS is Index(['pix0', 'pix1', 'pix2', 'pix3', 'pix4', 'pix5', 'pix6', 'pix7', 'pix8',\n",
      "       'pix9', 'pix10', 'pix11', 'pix12', 'pix13', 'pix14', 'pix15', 'pix16',\n",
      "       'pix17', 'pix18', 'pix19', 'pix20', 'pix21', 'pix22', 'pix23', 'pix24',\n",
      "       'pix25', 'pix26', 'pix27', 'pix28', 'pix29', 'pix30', 'pix31', 'pix32',\n",
      "       'pix33', 'pix34', 'pix35', 'pix36', 'pix37', 'pix38', 'pix39', 'pix40',\n",
      "       'pix41', 'pix42', 'pix43', 'pix44', 'pix45', 'pix46', 'pix47', 'pix48',\n",
      "       'pix49', 'pix50', 'pix51', 'pix52', 'pix53', 'pix54', 'pix55', 'pix56',\n",
      "       'pix57', 'pix58', 'pix59', 'pix60', 'pix61', 'pix62', 'pix63', 'is_0',\n",
      "       'is_1', 'is_2', 'is_3', 'is_4', 'is_5', 'is_6', 'is_7', 'is_8', 'is_9'],\n",
      "      dtype='object')\n",
      "\n",
      "COLUMNS[0] is pix0\n",
      "\n",
      "COL_INDEX is {'pix0': 0, 'pix1': 1, 'pix2': 2, 'pix3': 3, 'pix4': 4, 'pix5': 5, 'pix6': 6, 'pix7': 7, 'pix8': 8, 'pix9': 9, 'pix10': 10, 'pix11': 11, 'pix12': 12, 'pix13': 13, 'pix14': 14, 'pix15': 15, 'pix16': 16, 'pix17': 17, 'pix18': 18, 'pix19': 19, 'pix20': 20, 'pix21': 21, 'pix22': 22, 'pix23': 23, 'pix24': 24, 'pix25': 25, 'pix26': 26, 'pix27': 27, 'pix28': 28, 'pix29': 29, 'pix30': 30, 'pix31': 31, 'pix32': 32, 'pix33': 33, 'pix34': 34, 'pix35': 35, 'pix36': 36, 'pix37': 37, 'pix38': 38, 'pix39': 39, 'pix40': 40, 'pix41': 41, 'pix42': 42, 'pix43': 43, 'pix44': 44, 'pix45': 45, 'pix46': 46, 'pix47': 47, 'pix48': 48, 'pix49': 49, 'pix50': 50, 'pix51': 51, 'pix52': 52, 'pix53': 53, 'pix54': 54, 'pix55': 55, 'pix56': 56, 'pix57': 57, 'pix58': 58, 'pix59': 59, 'pix60': 60, 'pix61': 61, 'pix62': 62, 'pix63': 63, 'is_0': 64, 'is_1': 65, 'is_2': 66, 'is_3': 67, 'is_4': 68, 'is_5': 69, 'is_6': 70, 'is_7': 71, 'is_8': 72, 'is_9': 73}\n",
      "\n",
      "\n",
      "is_0 maps to 0\n",
      "is_1 maps to 1\n",
      "is_2 maps to 2\n",
      "is_3 maps to 3\n",
      "is_4 maps to 4\n",
      "is_5 maps to 5\n",
      "is_6 maps to 6\n",
      "is_7 maps to 7\n",
      "is_8 maps to 8\n",
      "is_9 maps to 9\n"
     ]
    }
   ],
   "source": [
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "COLUMNS = df_model1.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\\n\\n\")\n",
    "\n",
    "\n",
    "FEATURES = COLUMNS[0:64]\n",
    "\n",
    "#\n",
    "# and our \"species\" names\n",
    "#\n",
    "\n",
    "# all of scikit-learn's ML routines need numbers, not strings\n",
    "#   ... even for categories/classifications (like species!)\n",
    "#   so, we will convert the flower-species to numbers:\n",
    "\n",
    "SPECIES = ['is_0','is_1', 'is_2', 'is_3', 'is_4', 'is_5', 'is_6', 'is_7', 'is_8', 'is_9']   # int to str\n",
    "SPECIES_INDEX = {'is_0':0,'is_1':1, 'is_2':2, 'is_3':3, 'is_4':4, 'is_5':5, 'is_6':6, 'is_7':7, 'is_8':8, 'is_9':9}  # str to int\n",
    "\n",
    "#SPECIES = ['0','1','2','3','4','5','6','7','8','9']\n",
    "#SPECIES_INDEX = {'0':0,'1':1,'2':2,'3':3,'4':4,'5':5,'6':6,'7':7,'8':8,'9':9}\n",
    "\n",
    "# Let's try it out...\n",
    "for name in SPECIES:\n",
    "    print(f\"{name} maps to {SPECIES_INDEX[name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  9. 14.  8.  1.  0.  0.  0.  0. 12. 14. 14. 12.  0.  0.  0.  0.\n",
      "   9. 10.  0. 15.  4.  0.  0.  0.  3. 16. 12. 14.  2.  0.  0.  0.  4. 16.\n",
      "  16.  2.  0.  0.  0.  3. 16.  8. 10. 13.  2.  0.  0.  1. 15.  1.  3. 16.\n",
      "   8.  0.  0.  0. 11. 16. 15. 11.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]\n",
      " [ 0.  0. 11. 12.  0.  0.  0.  0.  0.  2. 16. 16. 16. 13.  0.  0.  0.  3.\n",
      "  16. 12. 10. 14.  0.  0.  0.  1. 16.  1. 12. 15.  0.  0.  0.  0. 13. 16.\n",
      "   9. 15.  2.  0.  0.  0.  0.  3.  0.  9. 11.  0.  0.  0.  0.  0.  9. 15.\n",
      "   4.  0.  0.  0.  9. 12. 13.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  1.  9. 15. 11.  0.  0.  0.  0. 11. 16.  8. 14.  6.  0.  0.  2.\n",
      "  16. 10.  0.  9.  9.  0.  0.  1. 16.  4.  0.  8.  8.  0.  0.  4. 16.  4.\n",
      "   0.  8.  8.  0.  0.  1. 16.  5.  1. 11.  3.  0.  0.  0. 12. 12. 10. 10.\n",
      "   0.  0.  0.  0.  1. 10. 13.  3.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0. 14. 13.  1.  0.  0.  0.  0.  5. 16. 16.  2.  0.  0.  0.\n",
      "   0. 14. 16. 12.  0.  0.  0.  1. 10. 16. 16. 12.  0.  0.  0.  3. 12. 14.\n",
      "  16.  9.  0.  0.  0.  0.  0.  5. 16. 15.  0.  0.  0.  0.  0.  4. 16. 14.\n",
      "   0.  0.  0.  0.  0.  1. 13. 16.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  5. 12.  1.  0.  0.  0.  0.  0. 15. 14.  7.  0.  0.  0.  0.  0.\n",
      "  13.  1. 12.  0.  0.  0.  0.  2. 10.  0. 14.  0.  0.  0.  0.  0.  2.  0.\n",
      "  16.  1.  0.  0.  0.  0.  0.  6. 15.  0.  0.  0.  0.  0.  9. 16. 15.  9.\n",
      "   8.  2.  0.  0.  3. 11.  8. 13. 12.  4.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "A = df_model1.to_numpy()   \n",
    "A = A.astype('float64')    # many types:  www.tutorialspoint.com/numpy/numpy_data_types.htm\n",
    "print(A[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset has 1768 rows and 74 cols\n"
     ]
    }
   ],
   "source": [
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data definitions +++\n",
      "\n",
      "y_all (just the labels/species, first few rows) are \n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "X_all (just the features, first few rows) are \n",
      " [[ 0.  0.  9. 14.  8.  1.  0.  0.  0.  0. 12. 14. 14. 12.  0.  0.  0.  0.\n",
      "   9. 10.  0. 15.  4.  0.  0.  0.  3. 16. 12. 14.  2.  0.  0.  0.  4. 16.\n",
      "  16.  2.  0.  0.  0.  3. 16.  8. 10. 13.  2.  0.  0.  1. 15.  1.  3. 16.\n",
      "   8.  0.  0.  0. 11. 16. 15. 11.  1.  0.]\n",
      " [ 0.  0. 11. 12.  0.  0.  0.  0.  0.  2. 16. 16. 16. 13.  0.  0.  0.  3.\n",
      "  16. 12. 10. 14.  0.  0.  0.  1. 16.  1. 12. 15.  0.  0.  0.  0. 13. 16.\n",
      "   9. 15.  2.  0.  0.  0.  0.  3.  0.  9. 11.  0.  0.  0.  0.  0.  9. 15.\n",
      "   4.  0.  0.  0.  9. 12. 13.  3.  0.  0.]\n",
      " [ 0.  0.  1.  9. 15. 11.  0.  0.  0.  0. 11. 16.  8. 14.  6.  0.  0.  2.\n",
      "  16. 10.  0.  9.  9.  0.  0.  1. 16.  4.  0.  8.  8.  0.  0.  4. 16.  4.\n",
      "   0.  8.  8.  0.  0.  1. 16.  5.  1. 11.  3.  0.  0.  0. 12. 12. 10. 10.\n",
      "   0.  0.  0.  0.  1. 10. 13.  3.  0.  0.]\n",
      " [ 0.  0.  0.  0. 14. 13.  1.  0.  0.  0.  0.  5. 16. 16.  2.  0.  0.  0.\n",
      "   0. 14. 16. 12.  0.  0.  0.  1. 10. 16. 16. 12.  0.  0.  0.  3. 12. 14.\n",
      "  16.  9.  0.  0.  0.  0.  0.  5. 16. 15.  0.  0.  0.  0.  0.  4. 16. 14.\n",
      "   0.  0.  0.  0.  0.  1. 13. 16.  1.  0.]\n",
      " [ 0.  0.  5. 12.  1.  0.  0.  0.  0.  0. 15. 14.  7.  0.  0.  0.  0.  0.\n",
      "  13.  1. 12.  0.  0.  0.  0.  2. 10.  0. 14.  0.  0.  0.  0.  0.  2.  0.\n",
      "  16.  1.  0.  0.  0.  0.  0.  6. 15.  0.  0.  0.  0.  0.  9. 16. 15.  9.\n",
      "   8.  2.  0.  0.  3. 11.  8. 13. 12.  4.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of data definitions +++\\n\")\n",
    "\n",
    "#\n",
    "# we could do this at the data-frame level, too!\n",
    "#\n",
    "\n",
    "X_all = A[:,0:64]   # X (features) ... is all rows, columns 0, 1, 2, 3 thru 63\n",
    "y_all = A[:,64:]    # y (labels) ... is all rows, columns 64\n",
    "\n",
    "print(f\"y_all (just the labels/species, first few rows) are \\n {y_all[0:5]}\")\n",
    "print()\n",
    "print(f\"X_all (just the features, first few rows) are \\n {X_all[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scrambled labels/species are \n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "The corresponding data rows are \n",
      " [[ 0.  0.  6. 16. 15.  5.  0.  0.  0.  1. 16. 14.  8. 15.  1.  0.  0.  9.\n",
      "  13.  1.  0. 12.  6.  0.  0.  5.  9.  0.  0.  9. 10.  0.  0.  6.  9.  0.\n",
      "   0.  9. 11.  0.  0.  7. 16.  1.  0. 11. 11.  0.  0.  3. 16. 11. 13. 16.\n",
      "   8.  0.  0.  0.  8. 16. 16. 12.  1.  0.]\n",
      " [ 0.  0.  0.  1. 13.  2.  0.  0.  0.  0.  0. 12. 14.  0.  0.  0.  0.  0.\n",
      "   6. 14.  0.  0.  0.  0.  0.  1. 14.  5.  0.  0.  0.  0.  0.  9. 12.  0.\n",
      "  12.  7.  0.  0.  0. 12. 14.  6. 16. 14.  1.  0.  0.  6. 16. 16. 16.  5.\n",
      "   0.  0.  0.  0.  0.  3. 14.  0.  0.  0.]\n",
      " [ 0.  0.  6. 12. 16. 10.  0.  0.  0.  4. 15.  8. 12. 14.  0.  0.  0.  0.\n",
      "   0.  0. 13.  8.  0.  0.  0.  0.  0.  6. 14.  1.  0.  0.  0.  0.  0.  5.\n",
      "  15.  8.  0.  0.  0.  0.  0.  0.  2. 15.  5.  0.  0.  0.  1.  4.  5. 15.\n",
      "   8.  0.  0.  0.  5. 16. 14.  9.  1.  0.]\n",
      " [ 0.  0.  0.  9. 14.  4.  0.  0.  0.  0.  6. 16. 12.  4.  0.  0.  0.  1.\n",
      "  16. 11.  0.  0.  0.  0.  0.  2. 16.  7.  3.  0.  0.  0.  0.  4. 16. 14.\n",
      "  16. 10.  0.  0.  0.  3. 16. 15. 10. 16.  6.  0.  0.  0. 12. 16.  7. 13.\n",
      "   9.  0.  0.  0.  1. 11. 16. 16.  9.  0.]\n",
      " [ 0.  0. 10. 16. 16.  8.  0.  0.  0.  0.  5.  8. 13. 13.  0.  0.  0.  0.\n",
      "   0.  0.  9. 13.  0.  0.  0.  0.  0.  2. 13. 12.  0.  0.  0.  0.  2. 15.\n",
      "  16. 16.  7.  0.  0.  0.  0. 13. 13.  5.  1.  0.  0.  0.  1. 14.  5.  0.\n",
      "   0.  0.  0.  0.  9. 13.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_all = y_all[indices]              # again...\n",
    "print(f\"The scrambled labels/species are \\n {y_all[0:5]}\")\n",
    "print()\n",
    "print(f\"The corresponding data rows are \\n {X_all[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 1414 rows;  testing with 354 rows\n",
      "\n",
      "+++ Testing +++   Held-out data... (testing data: 354)\n",
      "\n",
      "y_test: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "X_test (few rows): [[ 0.  0.  7. 16. 14.  3.  0.  0.  0.  0.  9. 14. 11. 15.  0.  0.  0.  0.\n",
      "   1.  5.  0. 15.  5.  0.  0.  0.  0.  0.  0. 16.  5.  0.  0.  0.  0.  0.\n",
      "   3. 16.  4.  0.  0.  0.  0.  1. 12. 14.  1.  0.  0.  0.  5. 12. 16. 16.\n",
      "  14.  1.  0.  0.  8. 16. 14. 10. 13.  3.]\n",
      " [ 0.  0.  0. 16. 11.  0.  0.  0.  0.  0.  6. 16. 10.  0.  0.  0.  0.  0.\n",
      "  11. 11.  0.  0.  0.  0.  0.  0. 12. 15. 11.  5.  0.  0.  0.  0. 14. 15.\n",
      "  12. 15. 11.  0.  0.  0. 12. 13.  0.  0. 16.  5.  0.  0.  6. 15.  4. 11.\n",
      "  16.  4.  0.  0.  0. 13. 16. 14.  9.  0.]\n",
      " [ 0.  0.  0. 13.  1.  0.  0.  0.  0.  0. 10. 12.  1.  0.  0.  0.  0.  0.\n",
      "  14.  3.  0.  0.  0.  0.  0.  4. 14.  0.  4.  5.  0.  0.  0.  7. 16.  4.\n",
      "   7. 14.  7.  0.  0.  3. 14.  0.  0.  4. 12.  0.  0.  0. 10. 10.  4. 10.\n",
      "  12.  0.  0.  0.  1.  9. 16. 14.  2.  0.]\n",
      " [ 0.  0.  7. 13. 10.  1.  0.  0.  0.  1. 15.  3.  9. 10.  0.  0.  0.  3.\n",
      "  16.  4. 13. 11.  0.  0.  0.  0.  6. 12. 12. 16.  0.  0.  0.  0.  0.  0.\n",
      "   0. 12.  5.  0.  0.  0.  0.  0.  0.  5. 11.  0.  0.  1. 11.  2.  0.  7.\n",
      "  11.  0.  0.  0.  7. 13. 16. 15.  4.  0.]\n",
      " [ 0.  0.  6. 16. 15.  2.  0.  0.  0.  0.  7. 13. 16.  4.  0.  0.  0.  0.\n",
      "   0.  1. 16.  3.  0.  0.  0.  0.  1. 10. 16.  6.  1.  0.  0.  0.  9. 16.\n",
      "  16. 16.  8.  0.  0.  0.  1. 16.  8.  4.  0.  0.  0.  0.  5. 13.  0.  0.\n",
      "   0.  0.  0.  0. 11.  7.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "+++ Training +++   Data used for modeling... (training data: 1414)\n",
      "\n",
      "y_train: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "X_train (few rows): [[ 0.  0.  0. 10. 12.  8.  1.  0.  0.  0.  5. 16. 16. 16.  0.  0.  0.  0.\n",
      "  10. 16. 16.  9.  0.  0.  0.  2. 15. 16. 13.  2.  0.  0.  0.  4. 16. 16.\n",
      "   8.  0.  0.  0.  0.  1. 15. 16.  7.  0.  0.  0.  0.  0.  9. 16. 11.  1.\n",
      "   0.  0.  0.  0.  0.  6. 12.  6.  0.  0.]\n",
      " [ 0.  2. 12. 16. 12.  0.  0.  0.  0.  7. 16. 13. 16.  3.  0.  0.  0.  0.\n",
      "   3.  5. 16.  0.  0.  0.  0.  0.  3. 15.  7.  0.  0.  0.  0.  0. 11. 13.\n",
      "   0.  0.  0.  0.  0.  6. 13.  1.  0.  0.  0.  0.  0.  6. 16. 11.  8. 11.\n",
      "   5.  0.  0.  0. 15. 16. 16. 15.  3.  0.]\n",
      " [ 0.  0.  2. 10. 16. 12.  0.  0.  0.  2. 15. 14.  8.  1.  0.  0.  0.  2.\n",
      "  16.  4.  0.  0.  0.  0.  0.  6. 16. 14. 13.  3.  0.  0.  0. 10. 16.  7.\n",
      "  11. 12.  0.  0.  0.  0.  1.  0.  4. 16.  0.  0.  0.  0.  0.  0.  7. 16.\n",
      "   2.  0.  0.  0.  2. 14. 16.  8.  0.  0.]\n",
      " [ 0.  0.  0.  8. 14.  0.  0.  0.  0.  0.  5. 16. 11.  0.  0.  0.  0.  1.\n",
      "  15. 14.  1.  6.  0.  0.  0.  7. 16.  5.  3. 16.  8.  0.  0.  8. 16.  8.\n",
      "  14. 16.  2.  0.  0.  0.  6. 14. 16. 11.  0.  0.  0.  0.  0.  6. 16.  4.\n",
      "   0.  0.  0.  0.  0. 10. 15.  0.  0.  0.]\n",
      " [ 0.  0.  4. 16. 16.  8.  0.  0.  0.  0.  6. 16. 16. 15.  1.  0.  0.  0.\n",
      "   4. 16. 16. 12.  0.  0.  0.  0.  3. 16. 16. 15.  0.  0.  0.  0.  8. 16.\n",
      "  16.  6.  0.  0.  0.  1. 13. 16. 16.  4.  0.  0.  0.  3. 16. 16. 15.  2.\n",
      "   0.  0.  0.  0.  6. 12. 12.  2.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"+++ Testing +++   Held-out data... (testing data: {len(y_test)})\\n\")\n",
    "print(f\"y_test: {y_test[0:5,:]}\\n\")\n",
    "print(f\"X_test (few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print(\"\\n\")\n",
    "print(f\"+++ Training +++   Data used for modeling... (training data: {len(y_train)})\\n\")\n",
    "print(f\"y_train: {y_train[0:5,:]}\\n\")\n",
    "print(f\"X_train (few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#\n",
    "# do we want to use a Scaler?\n",
    "#\n",
    "USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "# we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "if USE_SCALER == True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "else:\n",
    "    # this one does no scaling!  We still create it to be consistent:\n",
    "    scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "    scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "scaler   # is now defined and ready to use...\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# Here are our scaled training and testing sets:\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train) # scale!\n",
    "X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "y_train_scaled = y_train.copy()  # the predicted/desired labels are not scaled\n",
    "y_test_scaled = y_test.copy()  # not using the scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input  ->  pred   des  \n",
      "                                 [ 0.   -0.34 -1.09 -0.44] ->    ?    [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "                                     [0.   2.   1.46 0.98] ->    ?    [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "                                 [ 0.   -0.34 -0.66 -0.44] ->    ?    [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "                                 [ 0.   -0.34 -1.09 -0.91] ->    ?    [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "                                 [ 0.   -0.34 -0.24  0.98] ->    ?    [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "                                                    input  ->  pred   des  \n",
      "                 [ 0.00e+00  0.00e+00 -8.88e-16  1.00e+01] ->    ?    [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "                                         [ 0.  2. 12. 16.] ->    ?    [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "                                         [ 0.  0.  2. 10.] ->    ?    [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "                 [ 0.00e+00  0.00e+00 -8.88e-16  8.00e+00] ->    ?    [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "                                         [ 0.  0.  4. 16.] ->    ?    [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ascii_table(X,y,scaler_to_invert=None):\n",
    "    \"\"\" print a table of inputs and outputs \"\"\"\n",
    "    np.set_printoptions(precision=2)  # Let's use less precision\n",
    "    if scaler_to_invert == None:  # don't use the scaler\n",
    "        X = X\n",
    "    else:\n",
    "        X = scaler_to_invert.inverse_transform(X)\n",
    "    print(f\"{'input ':>58s} -> {'pred':^7s} {'des':<5s}\") \n",
    "    for i in range(len(y)):\n",
    "        # whoa! serious f-string formatting:\n",
    "        print(f\"{str(X[i,0:4]):>58s} -> {'?':^7s} {str(y[i]):<21s}\")   # !s is str ...\n",
    "    print()\n",
    "    \n",
    "# to show the table with the scaled data:\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],None)\n",
    "\n",
    "# to show the table with the original data:\n",
    "ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],scaler_to_invert=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++++++++++  TRAINING:  begin  +++++++++++++++\n",
      "\n",
      "\n",
      "Iteration 1, loss = 4.74989495\n",
      "Iteration 2, loss = 2.83975467\n",
      "Iteration 3, loss = 2.44253925\n",
      "Iteration 4, loss = 2.16232522\n",
      "Iteration 5, loss = 2.00659586\n",
      "Iteration 6, loss = 1.92330265\n",
      "Iteration 7, loss = 1.84284383\n",
      "Iteration 8, loss = 1.77604590\n",
      "Iteration 9, loss = 1.70465529\n",
      "Iteration 10, loss = 1.62037925\n",
      "Iteration 11, loss = 1.57983051\n",
      "Iteration 12, loss = 1.57966168\n",
      "Iteration 13, loss = 1.50814681\n",
      "Iteration 14, loss = 1.42192273\n",
      "Iteration 15, loss = 1.32696727\n",
      "Iteration 16, loss = 1.29248427\n",
      "Iteration 17, loss = 1.37588699\n",
      "Iteration 18, loss = 1.33101732\n",
      "Iteration 19, loss = 1.23723720\n",
      "Iteration 20, loss = 1.19147665\n",
      "Iteration 21, loss = 1.08186634\n",
      "Iteration 22, loss = 0.98725826\n",
      "Iteration 23, loss = 0.91972117\n",
      "Iteration 24, loss = 0.91499733\n",
      "Iteration 25, loss = 0.91616442\n",
      "Iteration 26, loss = 0.89496509\n",
      "Iteration 27, loss = 0.84592956\n",
      "Iteration 28, loss = 0.90593338\n",
      "Iteration 29, loss = 0.91598955\n",
      "Iteration 30, loss = 0.95544146\n",
      "Iteration 31, loss = 0.98530373\n",
      "Iteration 32, loss = 0.90616932\n",
      "Iteration 33, loss = 0.89549798\n",
      "Iteration 34, loss = 0.86535309\n",
      "Iteration 35, loss = 0.79800636\n",
      "Iteration 36, loss = 0.78274889\n",
      "Iteration 37, loss = 0.76543385\n",
      "Iteration 38, loss = 0.74091330\n",
      "Iteration 39, loss = 0.70482635\n",
      "Iteration 40, loss = 0.70269782\n",
      "Iteration 41, loss = 0.67273264\n",
      "Iteration 42, loss = 0.68805238\n",
      "Iteration 43, loss = 0.71253393\n",
      "Iteration 44, loss = 0.78863015\n",
      "Iteration 45, loss = 0.75611586\n",
      "Iteration 46, loss = 0.75061911\n",
      "Iteration 47, loss = 0.70947988\n",
      "Iteration 48, loss = 0.67236699\n",
      "Iteration 49, loss = 0.66203671\n",
      "Iteration 50, loss = 0.64137914\n",
      "Iteration 51, loss = 0.61283629\n",
      "Iteration 52, loss = 0.58980958\n",
      "Iteration 53, loss = 0.65380268\n",
      "Iteration 54, loss = 0.81260958\n",
      "Iteration 55, loss = 0.93308681\n",
      "Iteration 56, loss = 0.77304847\n",
      "Iteration 57, loss = 0.77109705\n",
      "Iteration 58, loss = 0.82532096\n",
      "Iteration 59, loss = 0.88853421\n",
      "Iteration 60, loss = 0.71680491\n",
      "Iteration 61, loss = 0.71465575\n",
      "Iteration 62, loss = 0.63204663\n",
      "Iteration 63, loss = 0.63989549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "++++++++++  TRAINING:   end  +++++++++++++++\n",
      "The analog prediction error (the loss) is 0.6398954872094275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#\n",
    "# Here's where you can change the number of hidden layers\n",
    "# and number of neurons!  It's in the tuple  hidden_layer_sizes:\n",
    "#\n",
    "nn_classifier = MLPClassifier(hidden_layer_sizes=(6,7),  \n",
    "                    # hidden_layer_sizes=(6,7)   means   4 inputs -> 6 hidden -> 7 hidden -> 3 outputs\n",
    "                    max_iter=500,      # how many times to train\n",
    "                    # activation=\"tanh\", # the \"activation function\" input -> output\n",
    "                    # solver='sgd',      # the algorithm for optimizing weights\n",
    "                    verbose=True,      # False to \"mute\" the training\n",
    "                    shuffle=True,      # reshuffle the training epochs?\n",
    "                    random_state=None, # set for reproduceability\n",
    "                    learning_rate_init=.1,       # learning rate: the amt of error to backpropagate!\n",
    "                    learning_rate = 'adaptive')  # soften feedback as it converges\n",
    "\n",
    "# documentation:\n",
    "# scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html \n",
    "#     Try other network sizes / other parameters ...\n",
    "\n",
    "print(\"\\n\\n++++++++++  TRAINING:  begin  +++++++++++++++\\n\\n\")\n",
    "nn_classifier.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"\\n++++++++++  TRAINING:   end  +++++++++++++++\")\n",
    "print(f\"The analog prediction error (the loss) is {nn_classifier.loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0. 11. 12.  0.  0.  0.  0.  0.  2. 16. 16. 16. 13.  0.  0.  0.  3.\n",
      " 16. 12. 10. 14.  0.  0.  0.  1. 16.  1. 12. 15.  0.  0.  0.  0. 13. 16.\n",
      "  9. 15.  2.  0.  0.  0.  0.  3.  0.  9. 11.  0.  0.  0.  0.  0.  9. 15.\n",
      "  4.  0.  0.  0.  9. 12. 13.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.]\n",
      "['is_0', 'is_1', 'is_2', 'is_3', 'is_4', 'is_5', 'is_6', 'is_7', 'is_8', 'is_9']\n"
     ]
    }
   ],
   "source": [
    "print(A[1])\n",
    "\n",
    "print(SPECIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      input  ->     pred         des.    \n",
      "           [ 0.  0.  7. 16.] ->  no species  is_2           incorrect: [3.14e-05 3.53e-04 4.40e-04 8.13e-03 1.49e-03 8.32e-05 4.94e-05 7.00e-09\n",
      " 3.00e-02 1.89e-04]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.60e+01] ->     is_6     is_6           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.30e+01] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  7. 13.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  6. 16.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  3. 16.] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  6. 16.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  7. 15.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  3. 15.] ->     is_4     is_4           correct \n",
      "           [ 0.  0. 15. 14.] ->     is_5     is_5           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  8.00e+00] ->     is_6     is_6           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  6.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  8. 15.] ->     is_3     is_3           correct \n",
      "           [ 0.  0. 11. 13.] ->     is_5     is_5           correct \n",
      "           [ 0.  0. 13. 15.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  6. 15.] ->     is_7     is_7           correct \n",
      "           [ 0.  2. 13. 16.] ->     is_3     is_3           correct \n",
      "           [ 0.  0. 11. 10.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  8. 16.] ->     is_8     is_8           correct \n",
      "           [ 0.  2.  9. 15.] ->     is_3     is_3           correct \n",
      "               [0. 0. 8. 9.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  4. 14.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  7. 10.] ->     is_2     is_8           incorrect: [1.75e-04 2.27e-02 5.79e-01 3.64e-02 4.14e-03 6.29e-05 6.78e-04 5.05e-08\n",
      " 2.38e-01 2.50e-04]\n",
      "           [ 0.  0.  4. 14.] ->     is_0     is_0           correct \n",
      "               [0. 0. 1. 8.] ->     is_3     is_8           incorrect: [1.13e-04 3.72e-04 3.14e-05 5.51e-01 3.53e-04 7.94e-04 5.19e-05 1.22e-07\n",
      " 2.38e-01 1.95e-02]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.60e+01] ->     is_6     is_6           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  0.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  4. 15.] ->     is_7     is_7           correct \n",
      "               [0. 0. 3. 6.] ->     is_2     is_8           incorrect: [1.85e-04 2.69e-02 5.65e-01 3.00e-02 4.25e-03 5.57e-05 6.29e-04 6.01e-08\n",
      " 2.10e-01 2.23e-04]\n",
      "           [ 0.  0.  5. 13.] ->     is_3     is_4           incorrect: [1.04e-04 1.62e-04 3.35e-06 7.10e-01 2.12e-04 1.32e-03 2.99e-05 1.52e-07\n",
      " 2.33e-01 4.61e-02]\n",
      "           [ 0.  0.  3. 12.] ->     is_3     is_3           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.20e+01] ->     is_1     is_1           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  0.00e+00] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  2. 15.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  9. 16.] ->     is_9     is_9           correct \n",
      "           [ 0.  2.  6. 10.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  2. 14.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  4. 16.] ->     is_0     is_6           incorrect: [8.75e-001 8.04e-085 2.49e-066 4.65e-048 2.42e-018 1.45e-044 9.54e-001\n",
      " 6.75e-112 4.61e-006 3.64e-049]\n",
      "           [ 0.  0.  8. 12.] ->     is_2     is_3           incorrect: [1.85e-04 2.69e-02 5.65e-01 3.00e-02 4.25e-03 5.57e-05 6.29e-04 6.01e-08\n",
      " 2.10e-01 2.23e-04]\n",
      "           [ 0.  0.  7. 16.] ->     is_2     is_2           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.50e+01] ->     is_4     is_4           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  2.00e+00] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  7. 15.] ->  no species  is_3           incorrect: [8.33e-04 1.62e-01 4.31e-10 8.66e-05 2.83e-04 4.13e-06 9.98e-08 1.50e-03\n",
      " 2.68e-05 3.29e-04]\n",
      "           [ 0.  0.  6. 13.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  1. 13.] ->     is_2     is_2           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  5.00e+00] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  3. 11.] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  7. 15.] ->  no species  is_9           incorrect: [9.34e-06 1.54e-07 4.18e-17 1.72e-02 7.87e-06 8.90e-04 4.47e-09 4.71e-07\n",
      " 8.48e-05 5.34e-02]\n",
      "           [ 0.  1. 12. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  3. 15.] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  5. 11.] ->     is_1     is_1           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  8.00e+00] ->     is_7     is_7           correct \n",
      "               [0. 0. 4. 6.] ->     is_9     is_7           incorrect: [3.76e-04 6.35e-05 1.51e-19 6.29e-02 2.01e-06 5.52e-04 4.69e-10 1.26e-02\n",
      " 1.96e-05 6.47e-01]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  4.00e+00] ->     is_4     is_4           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.10e+01] ->     is_6     is_6           correct \n",
      "           [ 0.  0. 11. 16.] ->     is_2     is_8           incorrect: [1.85e-04 2.69e-02 5.65e-01 3.00e-02 4.25e-03 5.57e-05 6.29e-04 6.01e-08\n",
      " 2.10e-01 2.23e-04]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.00e+00] ->     is_1     is_1           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  9.00e+00] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  5. 16.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  7. 16.] ->     is_1     is_1           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  3.00e+00] ->     is_4     is_4           correct \n",
      "               [0. 0. 5. 8.] ->     is_5     is_5           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.10e+01] ->     is_4     is_4           correct \n",
      "           [ 0.  2. 10. 14.] ->  no species  is_3           incorrect: [1.26e-04 1.13e-03 6.42e-04 3.27e-01 7.00e-04 4.02e-04 1.09e-04 9.10e-08\n",
      " 2.46e-01 5.95e-03]\n",
      "           [ 0.  0.  3. 16.] ->     is_7     is_7           correct \n",
      "           [ 0.  5. 16. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0. 10.  9.] ->  no species  is_8           incorrect: [1.26e-04 1.18e-03 7.21e-04 3.19e-01 7.19e-04 3.92e-04 1.12e-04 8.99e-08\n",
      " 2.46e-01 5.68e-03]\n",
      "           [ 0.  0. 13. 16.] ->     is_2     is_8           incorrect: [1.85e-04 2.69e-02 5.65e-01 3.00e-02 4.25e-03 5.57e-05 6.29e-04 6.01e-08\n",
      " 2.10e-01 2.23e-04]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.10e+01] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  3. 15.] ->     is_3     is_4           incorrect: [2.78e-04 1.91e-06 4.16e-24 5.14e-01 2.00e-07 4.44e-03 3.13e-11 4.54e-02\n",
      " 1.23e-05 9.89e-01]\n",
      "           [ 0.  0.  5. 14.] ->     is_8     is_8           correct \n",
      "           [ 0.  1. 15. 16.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  9. 16.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  3. 13.] ->     is_2     is_2           correct \n",
      "           [ 0.  0. 13.  9.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  6. 14.] ->     is_3     is_3           correct \n",
      "           [ 0.  0.  3. 16.] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  2. 15.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  2. 11.] ->     is_8     is_8           correct \n",
      "           [ 0.  0.  2. 15.] ->     is_7     is_5           incorrect: [2.86e-07 1.65e-05 8.04e-67 4.25e-34 2.07e-08 2.59e-17 3.10e-32 1.00e+00\n",
      " 3.04e-36 2.41e-16]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  9.00e+00] ->     is_4     is_4           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  9.00e+00] ->     is_6     is_6           correct \n",
      "           [ 0.  0. 10. 15.] ->     is_2     is_8           incorrect: [1.85e-04 2.69e-02 5.65e-01 3.00e-02 4.25e-03 5.57e-05 6.29e-04 6.01e-08\n",
      " 2.10e-01 2.23e-04]\n",
      "           [ 0.  2. 10. 15.] ->     is_3     is_3           correct \n",
      "           [ 0.  0. 15. 16.] ->     is_3     is_3           correct \n",
      "           [ 0.  0.  6. 14.] ->     is_3     is_3           correct \n",
      "           [ 0.  0.  2. 15.] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  4. 12.] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  8. 13.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  3. 14.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  4. 12.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  3. 11.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  7. 13.] ->     is_9     is_9           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  2. 16.] ->     is_0     is_0           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  2.00e+00] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  1. 11.] ->     is_7     is_7           correct \n",
      "           [ 0.  0. 10. 16.] ->     is_7     is_7           correct \n",
      "           [ 0.  1. 13. 16.] ->  no species  is_3           incorrect: [9.19e-13 1.04e-22 4.64e-24 4.69e-03 1.23e-06 3.74e-01 4.69e-10 1.84e-22\n",
      " 1.08e-04 7.86e-04]\n",
      "           [ 0.  0.  9. 15.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  3. 11.] ->  no species  is_3           incorrect: [1.31e-04 1.67e-03 1.85e-03 2.60e-01 8.91e-04 3.17e-04 1.41e-04 8.21e-08\n",
      " 2.48e-01 3.92e-03]\n",
      "           [ 0.  1. 11. 16.] ->     is_7     is_1           incorrect: [5.00e-04 4.95e-01 7.26e-20 2.30e-11 1.02e-04 5.44e-09 2.22e-12 6.32e-01\n",
      " 9.75e-12 2.87e-07]\n",
      "               [0. 0. 1. 6.] ->     is_8     is_8           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  2.00e+00] ->     is_4     is_4           correct \n",
      "               [0. 0. 7. 8.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  4. 14.] ->     is_1     is_1           correct \n",
      "           [ 0.  2. 15. 15.] ->     is_2     is_2           correct \n",
      "           [ 0.  0. 11. 15.] ->  no species  is_3           incorrect: [1.22e-04 8.57e-04 3.01e-04 3.80e-01 5.90e-04 4.77e-04 9.04e-05 9.79e-08\n",
      " 2.44e-01 8.02e-03]\n",
      "           [ 0.  2. 13. 16.] ->     is_3     is_3           correct \n",
      "           [ 0.  0.  2. 11.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  3. 11.] ->     is_7     is_7           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  5.00e+00] ->     is_2     is_8           incorrect: [1.85e-04 2.69e-02 5.65e-01 3.00e-02 4.25e-03 5.57e-05 6.29e-04 6.01e-08\n",
      " 2.10e-01 2.23e-04]\n",
      "           [ 0.  0.  2. 12.] ->     is_0     is_0           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  7. 14.] ->     is_3     is_3           correct \n",
      "               [0. 0. 2. 9.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  2. 14.] ->     is_4     is_4           correct \n",
      "               [0. 0. 3. 8.] ->     is_5     is_5           correct \n",
      "           [ 0.  1. 13. 16.] ->  no species  is_3           incorrect: [1.20e-04 7.17e-04 1.86e-04 4.16e-01 5.29e-04 5.32e-04 8.03e-05 1.03e-07\n",
      " 2.43e-01 9.69e-03]\n",
      "           [ 0.  0. 11. 12.] ->     is_3     is_9           incorrect: [1.37e-05 1.32e-07 6.94e-15 5.43e-01 8.49e-06 5.41e-03 5.78e-08 1.60e-07\n",
      " 3.77e-03 3.54e-01]\n",
      "           [ 0.  0.  5. 13.] ->     is_7     is_7           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  2.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0. 14. 16.] ->     is_2     is_3           incorrect: [1.66e-04 1.76e-02 5.28e-01 4.69e-02 3.81e-03 7.46e-05 6.81e-04 4.41e-08\n",
      " 2.64e-01 3.09e-04]\n",
      "           [ 0.  0.  3. 16.] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  4. 15.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  2. 12.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  2. 13.] ->     is_6     is_6           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  3. 14.] ->     is_7     is_7           correct \n",
      "               [0. 0. 1. 8.] ->     is_7     is_7           correct \n",
      "           [ 0.  0. 14. 12.] ->     is_5     is_5           correct \n",
      "           [ 0.  0. 11. 12.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  3. 11.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  3. 12.] ->     is_8     is_8           correct \n",
      "           [ 0.  0. 10. 13.] ->     is_2     is_8           incorrect: [1.85e-04 2.69e-02 5.65e-01 3.00e-02 4.25e-03 5.57e-05 6.29e-04 6.01e-08\n",
      " 2.10e-01 2.23e-04]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  7.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  3. 14.] ->     is_0     is_0           correct \n",
      "           [ 0.  1. 11. 12.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  7. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  2. 13.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  3. 13.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  7. 16.] ->     is_2     is_2           correct \n",
      "           [ 0.  1. 12. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  2. 12.] ->     is_0     is_0           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.00e+01] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  3. 13.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  7. 15.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  3. 16.] ->     is_6     is_6           correct \n",
      "           [ 0.  0. 11. 11.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  5. 16.] ->     is_0     is_0           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  2.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0. 10.  9.] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  7. 14.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  3. 12.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  3. 15.] ->     is_7     is_7           correct \n",
      "           [ 0.  0. 13. 16.] ->     is_3     is_3           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  2.00e+00] ->     is_4     is_4           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.20e+01] ->     is_1     is_1           correct \n",
      "           [ 0.  7. 16. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  2. 14. 16.] ->     is_3     is_3           correct \n",
      "           [ 0.  2. 13. 16.] ->     is_3     is_3           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.00e+00] ->     is_1     is_1           correct \n",
      "           [ 0.  0. 10. 16.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  7. 16.] ->     is_9     is_9           correct \n",
      "           [ 0.  4. 16. 15.] ->     is_2     is_2           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.00e+00] ->     is_1     is_1           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  5.00e+00] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  4. 15.] ->     is_0     is_0           correct \n",
      "           [ 0.  2. 13. 16.] ->     is_2     is_2           correct \n",
      "           [ 0.  3. 13. 16.] ->  no species  is_7           incorrect: [4.66e-04 6.25e-02 4.00e-07 9.45e-04 5.99e-04 1.30e-05 1.79e-06 4.35e-05\n",
      " 7.08e-04 4.17e-04]\n",
      "           [ 0.  4. 15. 14.] ->  no species  is_9           incorrect: [8.59e-06 2.76e-08 8.36e-17 4.73e-01 4.26e-06 7.00e-03 1.42e-08 1.56e-07\n",
      " 1.34e-03 4.60e-01]\n",
      "           [ 0.  0.  1. 13.] ->     is_0     is_6           incorrect: [8.27e-001 1.47e-096 6.09e-074 1.98e-052 2.67e-020 1.92e-048 9.95e-001\n",
      " 2.50e-126 1.20e-005 4.94e-054]\n",
      "           [ 0.  0.  4. 15.] ->     is_8     is_8           correct \n",
      "               [0. 0. 2. 9.] ->  no species  is_3           incorrect: [1.28e-04 1.30e-03 9.40e-04 3.02e-01 7.64e-04 3.69e-04 1.20e-04 8.76e-08\n",
      " 2.47e-01 5.12e-03]\n",
      "           [ 0.  0.  9. 14.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  3. 12.] ->     is_9     is_9           correct \n",
      "           [ 0.  0. 15. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0. 10. 13.] ->     is_1     is_1           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  7.00e+00] ->     is_7     is_7           correct \n",
      "           [ 0.  3. 16.  9.] ->     is_2     is_8           incorrect: [1.85e-04 2.69e-02 5.65e-01 3.00e-02 4.25e-03 5.57e-05 6.29e-04 6.01e-08\n",
      " 2.10e-01 2.23e-04]\n",
      "           [ 0.  0.  2. 15.] ->     is_4     is_1           incorrect: [2.28e-08 2.91e-01 1.03e-09 1.84e-27 8.95e-01 3.13e-16 6.02e-12 1.14e-10\n",
      " 1.01e-18 1.25e-22]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  7.00e+00] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  2. 13.] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  5. 15.] ->     is_0     is_0           correct \n",
      "           [ 0.  1. 12. 16.] ->     is_2     is_2           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  6.00e+00] ->     is_3     is_3           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  9.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  1. 12. 14.] ->     is_2     is_2           correct \n",
      "               [0. 0. 1. 8.] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  2. 12.] ->     is_0     is_0           correct \n",
      "           [ 0.  1.  8. 16.] ->     is_2     is_2           correct \n",
      "           [ 0.  0. 15. 13.] ->     is_2     is_8           incorrect: [1.85e-04 2.69e-02 5.65e-01 3.00e-02 4.25e-03 5.57e-05 6.29e-04 6.01e-08\n",
      " 2.10e-01 2.23e-04]\n",
      "           [ 0.  0.  6. 15.] ->     is_3     is_3           correct \n",
      "           [ 0.  0.  4. 12.] ->     is_0     is_0           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  0.00e+00] ->     is_1     is_6           incorrect: [1.44e-03 9.77e-01 1.04e-01 7.11e-06 1.22e-02 3.40e-07 2.72e-05 7.96e-05\n",
      " 3.52e-04 1.72e-06]\n",
      "           [ 0.  0.  6. 15.] ->  no species  is_3           incorrect: [1.23e-04 8.79e-04 3.23e-04 3.75e-01 5.99e-04 4.70e-04 9.19e-05 9.72e-08\n",
      " 2.44e-01 7.81e-03]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.00e+00] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  4. 12.] ->     is_8     is_8           correct \n",
      "           [ 0.  0.  2. 10.] ->     is_3     is_9           incorrect: [1.60e-05 2.20e-07 2.92e-14 5.65e-01 1.06e-05 4.97e-03 9.13e-08 1.61e-07\n",
      " 5.28e-03 3.22e-01]\n",
      "           [ 0.  0.  5. 15.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  6. 16.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  3. 11.] ->     is_7     is_7           correct \n",
      "           [ 0.  2. 11. 13.] ->  no species  is_9           incorrect: [8.58e-06 2.75e-08 8.24e-17 4.73e-01 4.26e-06 7.01e-03 1.41e-08 1.56e-07\n",
      " 1.33e-03 4.60e-01]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  3.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0. 10. 15.] ->     is_7     is_7           correct \n",
      "           [ 0.  0. 12. 16.] ->     is_7     is_7           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.10e+01] ->     is_4     is_1           incorrect: [1.76e-07 4.91e-01 3.19e-08 1.82e-23 7.17e-01 1.47e-14 1.03e-10 1.38e-09\n",
      " 4.95e-16 1.21e-19]\n",
      "           [ 0.  0.  4. 10.] ->     is_7     is_7           correct \n",
      "           [ 0.  0. 12. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0. 10. 12.] ->  no species  is_0           incorrect: [1.31e-01 4.37e-53 3.08e-78 9.09e-40 7.00e-17 4.31e-32 3.82e-16 4.44e-58\n",
      " 1.45e-19 3.04e-31]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.50e+01] ->     is_7     is_7           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  0.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  1. 13. 16.] ->     is_2     is_2           correct \n",
      "           [ 0.  1. 11. 16.] ->     is_7     is_7           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  0.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  2. 10.] ->     is_9     is_9           correct \n",
      "           [ 0.  0. 15. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  3. 10.] ->     is_3     is_9           incorrect: [1.12e-05 6.74e-08 1.03e-15 5.13e-01 6.31e-06 6.04e-03 3.16e-08 1.58e-07\n",
      " 2.42e-03 3.98e-01]\n",
      "           [ 0.  0.  1. 13.] ->  no species  is_6           incorrect: [4.53e-03 2.04e-02 2.03e-11 2.61e-21 1.37e-02 1.42e-16 1.05e-08 3.41e-11\n",
      " 7.90e-13 4.48e-18]\n",
      "           [ 0.  0.  6. 14.] ->  no species  is_3           incorrect: [8.88e-06 3.09e-08 1.15e-16 4.78e-01 4.48e-06 6.87e-03 1.57e-08 1.57e-07\n",
      " 1.44e-03 4.52e-01]\n",
      "               [0. 0. 1. 9.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  6. 14.] ->     is_9     is_9           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.00e+01] ->     is_6     is_6           correct \n",
      "           [ 0.  2. 12. 14.] ->     is_2     is_8           incorrect: [1.85e-04 2.69e-02 5.65e-01 3.00e-02 4.25e-03 5.57e-05 6.29e-04 6.01e-08\n",
      " 2.10e-01 2.23e-04]\n",
      "           [ 0.  0.  4. 13.] ->     is_7     is_7           correct \n",
      "           [ 0.  3. 15. 14.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  5. 16.] ->     is_1     is_1           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  6.00e+00] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  8. 16.] ->     is_9     is_9           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  4.00e+00] ->     is_3     is_9           incorrect: [1.08e-05 5.86e-08 6.99e-16 5.07e-01 5.94e-06 6.18e-03 2.79e-08 1.58e-07\n",
      " 2.20e-03 4.08e-01]\n",
      "           [ 0.  1. 10. 12.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  9. 10.] ->  no species  is_3           incorrect: [1.36e-04 2.48e-03 5.36e-03 2.02e-01 1.14e-03 2.49e-04 1.84e-04 7.40e-08\n",
      " 2.51e-01 2.56e-03]\n",
      "           [ 0.  0.  5. 13.] ->     is_9     is_9           correct \n",
      "           [ 0.  0. 11. 16.] ->     is_5     is_5           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.00e+00] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  4. 10.] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  3. 10.] ->     is_8     is_8           correct \n",
      "           [ 0.  0.  2. 14.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  1. 14.] ->     is_4     is_4           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  5.00e+00] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  4. 12.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  9. 16.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  8. 14.] ->     is_7     is_7           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  8.00e+00] ->     is_8     is_6           incorrect: [2.03e-17 7.67e-50 4.53e-19 1.02e-14 2.74e-06 1.94e-09 1.48e-01 1.49e-65\n",
      " 7.71e-01 4.85e-21]\n",
      "           [ 0.  0.  3. 12.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  7. 14.] ->     is_8     is_8           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.50e+01] ->  no species  is_6           incorrect: [8.46e-12 1.08e-55 8.69e-30 1.21e-21 2.49e-09 6.88e-18 4.42e-01 1.57e-73\n",
      " 2.60e-01 1.46e-26]\n",
      "           [ 0.  0.  6. 12.] ->     is_2     is_1           incorrect: [2.78e-04 6.43e-02 5.69e-01 2.53e-02 4.33e-03 4.31e-05 5.59e-04 1.69e-07\n",
      " 1.74e-01 2.30e-04]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  2.00e+00] ->     is_4     is_4           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.00e+00] ->     is_4     is_4           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  6.00e+00] ->  no species  is_3           incorrect: [1.43e-04 4.13e-03 2.10e-02 1.42e-01 1.55e-03 1.82e-04 2.58e-04 6.47e-08\n",
      " 2.54e-01 1.48e-03]\n",
      "               [0. 0. 1. 6.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  2. 15.] ->     is_4     is_4           correct \n",
      "           [ 0.  4. 10. 15.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  5. 14.] ->     is_1     is_1           correct \n",
      "           [ 0.  0. 15. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  2. 12.] ->     is_2     is_2           correct \n",
      "           [ 0.  2. 10. 16.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  8. 14.] ->     is_5     is_5           correct \n",
      "           [ 0.  2. 13. 16.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  1. 12.] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  5. 11.] ->     is_5     is_5           correct \n",
      "           [ 0.  0. 10. 13.] ->     is_9     is_9           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  1. 10. 16.] ->  no species  is_3           incorrect: [1.18e-04 5.71e-04 1.01e-04 4.62e-01 4.59e-04 6.11e-04 6.90e-05 1.09e-07\n",
      " 2.41e-01 1.24e-02]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  3.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0. 13. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  5. 15.] ->     is_0     is_0           correct \n",
      "           [ 0.  0. 11. 12.] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  1. 14.] ->     is_6     is_6           correct \n",
      "               [0. 0. 1. 8.] ->     is_7     is_7           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  7.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  2. 13.] ->     is_0     is_0           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  6.00e+00] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  8. 12.] ->  no species  is_9           incorrect: [1.01e-05 4.74e-08 3.84e-16 4.97e-01 5.41e-06 6.41e-03 2.31e-08 1.58e-07\n",
      " 1.91e-03 4.22e-01]\n",
      "           [ 0.  0.  6. 12.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  3. 12.] ->     is_7     is_7           correct \n",
      "           [ 0.  2. 15. 13.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  3. 10.] ->     is_8     is_8           correct \n",
      "           [ 0.  0.  1. 13.] ->     is_6     is_6           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  8.00e+00] ->     is_7     is_9           incorrect: [7.96e-04 2.73e-05 6.10e-28 9.51e-03 1.01e-07 3.86e-04 4.86e-13 9.28e-01\n",
      " 5.69e-08 9.57e-01]\n",
      "           [ 0.  4. 16. 15.] ->     is_2     is_2           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.10e+01] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  7. 15.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  4. 13.] ->     is_3     is_3           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.20e+01] ->     is_2     is_8           incorrect: [1.85e-04 2.69e-02 5.65e-01 3.00e-02 4.25e-03 5.57e-05 6.29e-04 6.01e-08\n",
      " 2.10e-01 2.23e-04]\n",
      "           [ 0.  0.  7. 15.] ->  no species  is_3           incorrect: [1.60e-04 1.20e-02 2.84e-01 6.33e-02 3.01e-03 9.43e-05 5.28e-04 4.87e-08\n",
      " 2.62e-01 4.66e-04]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  8.00e+00] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  1. 14.] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  3. 10.] ->     is_9     is_9           correct \n",
      "           [ 0.  1. 10. 13.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  2. 10.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  7. 13.] ->     is_8     is_8           correct \n",
      "           [ 0.  0.  2. 13.] ->     is_6     is_6           correct \n",
      "           [ 0.  4. 15. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  3. 12.] ->     is_8     is_8           correct \n",
      "           [ 0.  0.  2. 13.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  5. 15.] ->     is_1     is_1           correct \n",
      "               [0. 0. 7. 5.] ->  no species  is_0           incorrect: [1.17e-03 3.27e-41 6.26e-48 8.45e-27 5.74e-12 3.87e-23 4.47e-09 6.63e-50\n",
      " 1.70e-10 4.62e-24]\n",
      "           [ 0.  0.  1. 10.] ->  no species  is_3           incorrect: [1.20e-04 6.96e-04 1.72e-04 4.22e-01 5.19e-04 5.42e-04 7.87e-05 1.03e-07\n",
      " 2.42e-01 1.00e-02]\n",
      "           [ 0.  0.  4. 15.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  2. 12.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  4. 14.] ->     is_0     is_0           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  6.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  6. 16.] ->     is_2     is_2           correct \n",
      "           [ 0.  0.  9. 12.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  8. 14.] ->     is_1     is_1           correct \n",
      "           [ 0.  5. 16. 12.] ->     is_2     is_2           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  9.00e+00] ->     is_4     is_4           correct \n",
      "           [ 0.  0.  3. 12.] ->     is_0     is_0           correct \n",
      "           [ 0.  0.  4. 15.] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  1. 15.] ->     is_1     is_1           correct \n",
      "           [ 0.  0. 14. 12.] ->     is_5     is_5           correct \n",
      "           [ 0.  4. 13.  9.] ->     is_5     is_5           correct \n",
      "           [ 0.  4. 16. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0. 12. 15.] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  8. 12.] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  9. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0. 13. 16.] ->     is_2     is_2           correct \n",
      "               [0. 0. 3. 8.] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  9. 16.] ->     is_3     is_3           correct \n",
      "           [ 0.  0.  2. 14.] ->     is_6     is_6           correct \n",
      "               [0. 0. 2. 9.] ->     is_7     is_7           correct \n",
      "           [ 0.  1.  3. 15.] ->     is_2     is_8           incorrect: [1.85e-04 2.69e-02 5.65e-01 3.00e-02 4.25e-03 5.57e-05 6.29e-04 6.01e-08\n",
      " 2.10e-01 2.23e-04]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  9.00e+00] ->     is_6     is_6           correct \n",
      "           [ 0.  0.  6. 14.] ->     is_7     is_7           correct \n",
      "           [ 0.  0. 11. 16.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  3. 12.] ->     is_2     is_2           correct \n",
      "               [0. 0. 1. 9.] ->     is_0     is_0           correct \n",
      "           [ 0.  0. 12. 16.] ->     is_7     is_7           correct \n",
      "           [ 0.  1.  5. 11.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  3. 13.] ->     is_1     is_1           correct \n",
      "               [0. 0. 6. 8.] ->  no species  is_8           incorrect: [1.57e-04 1.01e-02 1.99e-01 7.24e-02 2.71e-03 1.05e-04 4.70e-04 5.10e-08\n",
      " 2.61e-01 5.62e-04]\n",
      "           [ 0.  0.  6. 12.] ->     is_7     is_7           correct \n",
      "           [ 0.  0. 13. 16.] ->     is_7     is_7           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.60e+01] ->     is_1     is_1           correct \n",
      "           [ 0.  0.  5. 16.] ->     is_0     is_0           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  3.00e+00] ->     is_9     is_9           correct \n",
      "           [ 0.  0.  9. 16.] ->     is_9     is_3           incorrect: [7.12e-06 1.48e-08 1.43e-17 4.46e-01 3.24e-06 7.76e-03 8.12e-09 1.55e-07\n",
      " 8.84e-04 5.04e-01]\n",
      "           [ 0.  8. 16. 12.] ->     is_5     is_5           correct \n",
      "           [ 0.  3. 11. 15.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  7. 11.] ->     is_5     is_5           correct \n",
      "               [0. 0. 1. 9.] ->     is_3     is_9           incorrect: [1.33e-05 1.18e-07 5.08e-15 5.38e-01 8.09e-06 5.51e-03 5.24e-08 1.59e-07\n",
      " 3.51e-03 3.61e-01]\n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  1.30e+01] ->     is_6     is_6           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  9.00e+00] ->     is_9     is_9           correct \n",
      "           [ 0.  0. 14. 16.] ->     is_5     is_5           correct \n",
      "           [ 0.  0.  1. 14.] ->     is_7     is_7           correct \n",
      "           [ 0.  0.  8. 16.] ->     is_3     is_3           correct \n",
      "           [ 0.  1. 11. 14.] ->     is_3     is_3           correct \n",
      "           [ 0.  2. 11. 16.] ->     is_2     is_2           correct \n",
      "           [ 0.  3. 16. 13.] ->     is_5     is_5           correct \n",
      "           [ 0.  1. 12. 15.] ->     is_9     is_9           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  3.00e+00] ->     is_4     is_4           correct \n",
      "[ 0.00e+00  0.00e+00 -8.88e-16  4.00e+00] ->     is_4     is_4           correct \n",
      "\n",
      "correct predictions: 294 out of 354\n"
     ]
    }
   ],
   "source": [
    "def get_species(A):\n",
    "    \"\"\" returns the species for A ~ [1 0 0] or [0 1 0] or ... \"\"\"\n",
    "    for i in range(len(SPECIES)):\n",
    "        if A[i] == 1: \n",
    "            return SPECIES[i]  # note that this \"takes the first one\"\n",
    "    return \"no species\" \n",
    "\n",
    "SEE_PROBS = False\n",
    "\n",
    "def ascii_table_for_classifier(Xsc,y,nn,scaler):\n",
    "    \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "    predictions = nn.predict(Xsc)            # all predictions\n",
    "    prediction_probs = nn.predict_proba(Xsc) # all prediction probabilities\n",
    "    Xpr = scaler.inverse_transform(Xsc)      # Xpr is the \"X to print\": the unscaled data\n",
    "    # count correct\n",
    "    num_correct = 0\n",
    "    # printing\n",
    "    print(f\"{'input ':>28s} -> {'pred':^12s} {'des.':^12s}\") \n",
    "    for i in range(len(y)):\n",
    "        pred = predictions[i]\n",
    "        pred_probs = str(prediction_probs[i,:])\n",
    "        desired = y[i].astype(int)\n",
    "        # print(pred, desired, pred_probs)\n",
    "        pred_species = get_species(pred)\n",
    "        des_species  = get_species(desired)\n",
    "        if pred_species != des_species: result = \"  incorrect: \" + pred_probs\n",
    "        else: result = \"  correct\" + (\": \"+pred_probs if SEE_PROBS else \"\") ; num_correct += 1\n",
    "        # Xpr = Xsc  # if you want to see the scaled versions\n",
    "        print(f\"{Xpr[i,0:4]!s:>28s} -> {pred_species:^12s} {des_species:12s} {result:^10s}\") \n",
    "    print(f\"\\ncorrect predictions: {num_correct} out of {len(y)}\")\n",
    "    \n",
    "#\n",
    "# let's see how it did on the test data (also the training data!)\n",
    "#\n",
    "ascii_table_for_classifier(X_test_scaled,\n",
    "                           y_test_scaled,\n",
    "                           nn_classifier,\n",
    "                           scaler)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "+++++ parameters, weights, etc. +++++\n",
      "\n",
      "\n",
      "weights/coefficients:\n",
      "\n",
      "[[ Layer 0 ]]\n",
      "   has shape = (64, 6) and weights =\n",
      "[[ 3.46e-03 -5.65e-03 -3.08e-04  1.53e-04 -1.53e-04  1.58e-02]\n",
      " [-5.12e-01  2.68e-01  1.55e+00 -1.05e+00 -3.90e-01 -3.97e-01]\n",
      " [ 1.64e+00 -1.75e+00 -1.83e-01 -1.91e+00 -4.30e-01  5.61e-01]\n",
      " [-1.44e+00 -8.80e-01  7.60e-01 -1.55e+00  8.42e-01  1.74e+00]\n",
      " [ 7.97e-01 -1.90e+00  8.82e-01  7.22e-01  3.10e-01  8.66e-01]\n",
      " [ 1.31e+00  3.34e-02 -3.43e-01 -1.77e+00 -2.14e+00 -4.39e-01]\n",
      " [ 2.17e+00 -4.58e-01  9.84e-01 -1.74e+00 -1.35e+00  7.26e-01]\n",
      " [-1.86e+00 -1.05e+00  2.09e-01  2.04e-01 -4.94e-01 -8.01e-01]\n",
      " [-6.78e-01 -1.26e+00 -4.22e-01  3.44e-01  2.43e-01  5.10e-02]\n",
      " [-2.24e-01 -1.99e+00  5.21e-01 -1.07e+00 -3.44e-01 -7.86e-01]\n",
      " [ 3.49e-01 -2.68e+00  1.95e+00 -1.47e+00 -5.52e-01  9.37e-02]\n",
      " [-1.08e+00 -1.07e+00  1.37e+00 -4.54e-01  1.17e+00  6.86e-01]\n",
      " [ 2.35e-01 -3.25e+00  1.34e+00 -1.33e+00  8.13e-01  1.08e-01]\n",
      " [-3.56e-02 -2.96e-01 -1.01e+00  5.23e-01 -4.31e-01 -6.06e-01]\n",
      " [-5.37e-01 -3.10e+00 -6.79e-02  2.89e+00  2.11e-01  3.93e-01]\n",
      " [-7.34e-01  1.00e+00  4.26e-01 -9.80e-01 -1.45e+00 -7.38e-01]\n",
      " [-1.40e+00  7.12e-01 -5.46e-01 -1.59e+00 -3.40e-01 -1.29e+00]\n",
      " [-1.04e-01  1.65e+00 -1.48e+00 -1.51e-01 -4.26e-01 -3.34e-01]\n",
      " [ 1.70e+00  1.05e+00 -2.91e+00  7.45e-01  9.69e-01  3.73e-01]\n",
      " [ 4.75e-01  4.57e+00 -8.09e-01 -1.04e+00  8.12e-01 -2.49e-01]\n",
      " [-4.93e+00  2.32e+00  5.64e-01 -2.98e+00 -3.32e+00 -2.70e-01]\n",
      " [-4.07e+00 -1.26e+00 -1.81e+00 -2.33e+00 -2.42e-01  1.72e+00]\n",
      " [-3.93e+00 -1.16e+00 -6.28e-01 -1.13e+00 -4.92e-02  1.40e-01]\n",
      " [-6.32e-02 -5.85e-03  8.11e-01 -4.42e-01  1.28e+00 -5.51e-01]\n",
      " [ 6.55e-01  2.66e+00  4.84e-01 -1.45e+00 -3.53e-02 -9.92e-01]\n",
      " [ 8.61e-01  2.03e+00 -1.40e-01 -5.70e-01  5.86e-01  7.60e-01]\n",
      " [ 1.11e+00 -5.62e-01  2.68e+00  3.18e-01  1.31e+00  1.93e+00]\n",
      " [-9.09e-01  1.01e+00 -2.67e+00  7.09e-01 -8.76e-01  7.85e-01]\n",
      " [ 1.26e+00 -3.47e-01 -8.38e-01 -1.55e+00 -3.35e+00  1.45e+00]\n",
      " [-4.12e-01 -5.58e-02 -1.02e+00 -2.17e+00 -1.02e+00  8.45e-01]\n",
      " [-1.24e+00  3.52e+00  2.60e+00 -9.82e-01  1.77e+00  6.25e-01]\n",
      " [ 1.58e-01  2.34e+00  1.68e+00 -4.02e-01  2.07e-01 -3.82e-01]\n",
      " [-6.48e-06  6.39e-03 -1.03e-02 -7.06e-04  7.53e-03 -1.89e-04]\n",
      " [ 3.93e-01  3.28e+00  5.05e-01  4.37e-01  2.42e+00 -1.25e+00]\n",
      " [ 2.31e+00  7.61e-01  1.85e+00  6.85e-01  7.18e-01 -5.65e-01]\n",
      " [-5.68e-01  1.22e-01 -1.77e-01  1.29e+00 -1.27e+00  8.07e-01]\n",
      " [-8.55e-01  9.71e-01  3.46e+00  1.65e+00 -1.47e+00 -2.17e+00]\n",
      " [-1.26e+00  2.13e+00  3.74e+00  1.89e+00  7.96e-01 -9.12e-01]\n",
      " [ 1.39e+00  1.41e+00  2.74e+00 -1.25e-01  5.09e-01  5.63e-01]\n",
      " [ 1.77e-04  8.48e-03  1.82e-03  2.43e-05 -3.54e-05  1.81e-03]\n",
      " [ 2.65e+00  3.94e+00  2.73e+00 -1.07e-01  6.74e-01 -1.50e-01]\n",
      " [-1.75e+00  3.65e-01  1.29e+00 -1.19e+00 -1.34e+00 -4.96e-01]\n",
      " [-2.75e+00 -1.41e+00  1.21e+00  3.11e+00  1.94e+00 -4.00e+00]\n",
      " [-1.93e+00  3.07e+00  1.44e+00  1.68e+00 -7.35e-01 -4.95e+00]\n",
      " [ 2.25e-01  2.27e+00  3.79e+00  2.65e+00  3.20e-01 -5.43e-01]\n",
      " [ 1.41e+00 -5.88e-02  5.57e-03  9.73e-01  1.45e+00  1.21e+00]\n",
      " [ 8.06e-01 -1.15e+00  1.37e+00  1.19e+00  8.05e-01  2.78e-01]\n",
      " [-3.94e-01 -4.13e-01 -1.16e+00  1.16e+00  8.96e-01  3.43e-01]\n",
      " [ 5.91e-01  9.74e-01 -2.34e-01 -2.81e-01 -1.06e+00 -3.56e-01]\n",
      " [ 8.53e-01  1.27e+00  3.70e-01 -5.04e-01 -3.76e-01 -9.04e-01]\n",
      " [ 1.51e-02 -9.24e-01 -1.48e+00  1.88e+00  1.36e+00  4.06e-01]\n",
      " [ 9.88e-01 -1.47e+00  2.57e+00  1.31e+00  7.03e-02 -2.15e-01]\n",
      " [ 3.70e-02  2.25e+00 -4.05e+00 -2.62e+00  1.13e+00 -4.96e-01]\n",
      " [ 3.53e-01 -9.83e-01 -3.67e+00  1.92e+00 -3.85e-01 -1.71e+00]\n",
      " [-1.55e+00 -1.16e+00 -3.15e+00  1.19e+00 -4.16e-02  2.05e-01]\n",
      " [ 3.44e-01  5.41e-01 -8.77e-01 -1.22e+00 -3.96e+00 -1.50e-01]\n",
      " [-1.70e+00 -3.76e-01 -3.76e-01 -1.46e+00 -1.04e+00 -3.63e-01]\n",
      " [ 1.29e-02 -2.50e+00 -2.51e+00 -4.43e-01 -5.43e-01  1.36e-01]\n",
      " [ 1.99e+00 -1.72e+00  2.77e-02 -4.92e-01 -7.45e-01 -5.69e-02]\n",
      " [-3.03e-01  1.15e+00 -2.39e+00  1.51e+00  4.52e-02  9.51e-01]\n",
      " [-1.40e+00  4.96e-01 -1.01e+00  8.12e-01 -1.01e+00  1.89e-01]\n",
      " [ 5.89e-01  1.09e+00 -1.16e+00  9.82e-01 -3.71e-01 -1.13e+00]\n",
      " [-1.19e+00  1.13e-01 -5.50e+00 -1.62e+00  2.96e-01 -8.18e-01]\n",
      " [-1.61e+00  8.46e-02  1.31e+00  4.04e-01 -2.36e+00 -2.86e+00]]\n",
      "   with intercepts:\n",
      " [-7.67 -7.65 -5.61 -7.52 -3.85 -0.48]\n",
      "\n",
      "[[ Layer 1 ]]\n",
      "   has shape = (6, 7) and weights =\n",
      "[[-1.06 -1.05  0.96 -0.64 -1.2  -1.05  1.68]\n",
      " [ 0.45 -1.06 -1.66 -0.48 -0.54 -0.75 -1.2 ]\n",
      " [ 0.87 -1.28  1.24 -0.4  -0.76 -1.12 -0.56]\n",
      " [-0.69  1.3  -1.18 -0.8  -0.56 -0.6   2.06]\n",
      " [ 0.53  1.79  0.12 -0.31 -0.58  0.01  0.23]\n",
      " [-0.28 -0.8   0.65 -0.69 -0.18 -0.18  0.22]]\n",
      "   with intercepts:\n",
      " [ 1.69 -1.66 -0.29 -0.3  -0.78 -0.38  2.16]\n",
      "\n",
      "[[ Layer 2 ]]\n",
      "   has shape = (7, 10) and weights =\n",
      "[[-0.77 -0.32 -1.3  -3.48  0.46 -1.46 -1.07 -0.94 -2.35 -2.6 ]\n",
      " [ 1.56 -1.06 -2.56 -1.63 -0.75 -2.11  0.15 -1.51 -0.25 -1.24]\n",
      " [ 0.19 -0.31 -4.39 -0.08 -0.75  0.1  -1.48  1.55 -1.1   0.83]\n",
      " [-0.59 -0.57 -0.39 -0.83 -0.48 -0.37 -0.98 -0.82 -0.99 -0.23]\n",
      " [-0.42 -0.59 -0.15 -0.28 -0.09 -1.18 -1.15 -0.11 -0.59 -0.3 ]\n",
      " [ 0.01 -0.94 -0.71 -0.74 -0.99 -0.91 -0.47 -1.48 -0.4  -0.3 ]\n",
      " [-1.94 -3.8  -0.55 -0.59  0.09  0.49  0.08 -4.53  0.06 -1.09]]\n",
      "   with intercepts:\n",
      " [-3.08  5.17  3.65  3.7  -6.44 -8.38 -5.72 -5.23  2.53 -1.64]\n",
      "\n",
      "\n",
      "\n",
      "all parameters: {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (6, 7), 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_fun': 15000, 'max_iter': 500, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': True, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# We don't usually look inside the NNet, but we can: it's open-box modeling...\n",
    "#\n",
    "if True:  # do we want to see all of the parameters?\n",
    "    np.set_printoptions(precision=2)  # Let's use less precision\n",
    "    nn = nn_classifier  # less to type?\n",
    "    print(\"\\n\\n+++++ parameters, weights, etc. +++++\\n\")\n",
    "    print(f\"\\nweights/coefficients:\\n\")\n",
    "    for i, wts in enumerate(nn.coefs_):\n",
    "        print(f\"[[ Layer {i} ]]\\n   has shape = {wts.shape} and weights =\\n{wts}\")\n",
    "        print(f\"   with intercepts:\\n {nn.intercepts_[i]}\\n\")\n",
    "    print()\n",
    "    print(f\"\\nall parameters: {nn.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict         is_4 from the features [0, 0, 0, 8, 14, 0, 0, 0, 0, 0, 5, 16, 11, 0, 0, 0, 0, 1, 15, 14, 1, 6, 0, 0, 0, 7, 16, 5, 3, 16, 8, 0, 0, 8, 16, 8, 14, 16, 2, 0, 0, 0, 6, 14, 16, 11, 0, 0, 0, 0, 0, 6, 16, 4, 0, 0, 0, 0, 0, 10, 15, 0, 0, 0]  \n",
      "I predict         is_2 from the features [0, 0, 0, 5, 14, 12, 2, 0, 0, 0, 7, 15, 8, 14, 4, 0, 0, 0, 6, 2, 3, 13, 1, 0, 0, 0, 0, 1, 13, 4, 0, 0, 0, 0, 1, 11, 9, 0, 0, 0, 0, 8, 16, 13, 0, 0, 0, 0, 0, 5, 14, 16, 11, 2, 0, 0, 0, 0, 0, 6, 12, 13, 3, 0]  \n",
      "I predict         is_4 from the features [0, 0, 0, 3, 16, 3, 0, 0, 0, 0, 0, 12, 16, 2, 0, 0, 0, 0, 8, 16, 16, 4, 0, 0, 0, 7, 16, 15, 16, 12, 11, 0, 0, 8, 16, 16, 16, 13, 3, 0, 0, 0, 0, 7, 14, 1, 0, 0, 0, 0, 0, 6, 16, 0, 0, 0, 0, 0, 0, 4, 14, 0, 0, 0]  \n",
      "I predict         is_2 from the features [0, 0, 0, 3, 15, 10, 1, 0, 0, 0, 0, 11, 10, 16, 4, 0, 0, 0, 0, 12, 1, 15, 6, 0, 0, 0, 0, 3, 4, 15, 4, 0, 0, 0, 0, 6, 15, 6, 0, 0, 0, 4, 15, 16, 9, 0, 0, 0, 0, 0, 13, 16, 15, 9, 3, 0, 0, 0, 0, 4, 9, 14, 7, 0]  \n",
      "I predict         is_4 from the features [0, 0, 0, 3, 16, 3, 0, 0, 0, 0, 0, 10, 16, 11, 0, 0, 0, 0, 4, 16, 16, 8, 0, 0, 0, 2, 14, 12, 16, 5, 0, 0, 0, 10, 16, 14, 16, 16, 11, 0, 0, 5, 12, 13, 16, 8, 3, 0, 0, 0, 0, 2, 15, 3, 0, 0, 0, 0, 0, 4, 12, 0, 0, 0]  \n",
      "I predict         is_2 from the features [0, 0, 7, 15, 15, 4, 0, 0, 0, 8, 16, 16, 16, 4, 0, 0, 0, 8, 15, 8, 16, 4, 0, 0, 0, 0, 0, 10, 15, 0, 0, 0, 0, 0, 1, 15, 9, 0, 0, 0, 0, 0, 6, 16, 2, 0, 0, 0, 0, 0, 8, 16, 8, 11, 9, 0, 0, 0, 9, 16, 16, 12, 3, 0]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_species(A):\n",
    "    \"\"\" returns the species for A ~ [1 0 0] or [0 1 0] or ... \"\"\"\n",
    "    for i in range(len(SPECIES)):\n",
    "        if A[i] == 1: \n",
    "            return SPECIES[i]  # note that this \"takes the first one\"\n",
    "    return \"no species\" \n",
    "\n",
    "\n",
    "def predictive_model( Features, MODEL, SCALER ):\n",
    "    \"\"\" input: a list of four features \n",
    "                [ sepallen, sepalwid, petallen, petalwid ]\n",
    "        output: the predicted species of iris, from\n",
    "                  setosa (0), versicolor (1), virginica (2)\n",
    "    \"\"\"\n",
    "    our_features = np.asarray([Features])                 # extra brackets needed\n",
    "    scaled_features = SCALER.transform(our_features)      # we have to scale the features into \"scaled space\"\n",
    "    predicted_cat = MODEL.predict(scaled_features)        # then, the nnet can predict our \"cat\" variables\n",
    "    prediction_probs = nn.predict_proba(scaled_features) # all prediction probabilities\n",
    "    # our_features = SCALER.inverse_transform(scaled_features)  # we can convert back (optional!)\n",
    "    predicted_species = get_species(predicted_cat[0])     # (it's extra-nested) get the species name\n",
    "    return predicted_species, prediction_probs\n",
    "   \n",
    "#\n",
    "# Try it!\n",
    "# \n",
    "# Features = eval(input(\"Enter new Features: \"))\n",
    "#\n",
    "Features = [0,0,0,8,14,0,0,0,0,0,5,16,11,0,0,0,0,1,15,14,1,6,0,0,0,7,16,5,3,16,8,0,0,8,16,8,14,16,2,0,0,0,6,14,16,11,0,0,0,0,0,6,16,4,0,0,0,0,0,10,15,0,0,0] # [5.8,2.7,4.1,1.0] # [4.6,3.6,3.0,2.2] [6.7,3.3,5.7,2.1]\n",
    "\n",
    "LoD = [[0,0,0,8,14,0,0,0,0,0,5,16,11,0,0,0,0,1,15,14,1,6,0,0,0,7,16,5,3,16,8,0,0,8,16,8,14,16,2,0,0,0,6,14,16,11,0,0,0,0,0,6,16,4,0,0,0,0,0,10,15,0,0,0],\n",
    "[0,0,0,5,14,12,2,0,0,0,7,15,8,14,4,0,0,0,6,2,3,13,1,0,0,0,0,1,13,4,0,0,0,0,1,11,9,0,0,0,0,8,16,13,0,0,0,0,0,5,14,16,11,2,0,0,0,0,0,6,12,13,3,0],\n",
    "[0,0,0,3,16,3,0,0,0,0,0,12,16,2,0,0,0,0,8,16,16,4,0,0,0,7,16,15,16,12,11,0,0,8,16,16,16,13,3,0,0,0,0,7,14,1,0,0,0,0,0,6,16,0,0,0,0,0,0,4,14,0,0,0],\n",
    "[0,0,0,3,15,10,1,0,0,0,0,11,10,16,4,0,0,0,0,12,1,15,6,0,0,0,0,3,4,15,4,0,0,0,0,6,15,6,0,0,0,4,15,16,9,0,0,0,0,0,13,16,15,9,3,0,0,0,0,4,9,14,7,0],\n",
    "[0,0,0,3,16,3,0,0,0,0,0,10,16,11,0,0,0,0,4,16,16,8,0,0,0,2,14,12,16,5,0,0,0,10,16,14,16,16,11,0,0,5,12,13,16,8,3,0,0,0,0,2,15,3,0,0,0,0,0,4,12,0,0,0],\n",
    "[0,0,7,15,15,4,0,0,0,8,16,16,16,4,0,0,0,8,15,8,16,4,0,0,0,0,0,10,15,0,0,0,0,0,1,15,9,0,0,0,0,0,6,16,2,0,0,0,0,0,8,16,8,11,9,0,0,0,9,16,16,12,3,0]]\n",
    "\n",
    "\n",
    "SEE_PROBS = False\n",
    "\n",
    "# run on each one:\n",
    "for Features in LoD:\n",
    "    MODEL = nn_classifier\n",
    "    SCALER = scaler\n",
    "    name, probs = predictive_model( Features, MODEL, SCALER )  # pass in the model, too!\n",
    "    prob_str = \"   with probs: \" + str(probs) if SEE_PROBS == True else \"\"\n",
    "    print(f\"I predict {name:>12s} from the features {Features}  {prob_str}\")    # Answers in the assignment..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, from the iris example create a _pixel-predicting_ &nbsp; NNet\n",
    "\n",
    "Choose a pixel to predict!\n",
    "+ It _can_ be #42 -- or choose another one?!\n",
    "+ This will be _regression_, not classification\n",
    "+ It will show off NNets' ability to generate or \"hallucinate\" pixels/digits/images/etc.!\n",
    "+ The _digit dreaming_ problem will extend this further..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Predict-a-pixel</b> (regression)...\n",
    "+ As the penultimate part of this digits-analysis with NNets, \n",
    "+ create a regressor that predicts pixel 42 from the other 63 pixels!\n",
    "+ Remember that pixel 42 will be `A[:,42]`\n",
    "+ and, the other 63, plus the digit-species, will be `np.concatenate((A[:,0:42], A[:,43:]),axis=1)`\n",
    "+ see the iris_modeler for an example for the irises' botanical features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pix0': 0, 'pix1': 1, 'pix2': 2, 'pix3': 3, 'pix4': 4, 'pix5': 5, 'pix6': 6, 'pix7': 7, 'pix8': 8, 'pix9': 9, 'pix10': 10, 'pix11': 11, 'pix12': 12, 'pix13': 13, 'pix14': 14, 'pix15': 15, 'pix16': 16, 'pix17': 17, 'pix18': 18, 'pix19': 19, 'pix20': 20, 'pix21': 21, 'pix22': 22, 'pix23': 23, 'pix24': 24, 'pix25': 25, 'pix26': 26, 'pix27': 27, 'pix28': 28, 'pix29': 29, 'pix30': 30, 'pix31': 31, 'pix32': 32, 'pix33': 33, 'pix34': 34, 'pix35': 35, 'pix36': 36, 'pix37': 37, 'pix38': 38, 'pix39': 39, 'pix40': 40, 'pix41': 41, 'pix42': 42, 'pix43': 43, 'pix44': 44, 'pix45': 45, 'pix46': 46, 'pix47': 47, 'pix48': 48, 'pix49': 49, 'pix50': 50, 'pix51': 51, 'pix52': 52, 'pix53': 53, 'pix54': 54, 'pix55': 55, 'pix56': 56, 'pix57': 57, 'pix58': 58, 'pix59': 59, 'pix60': 60, 'pix61': 61, 'pix62': 62, 'pix63': 63, 'is_0': 64, 'is_1': 65, 'is_2': 66, 'is_3': 67, 'is_4': 68, 'is_5': 69, 'is_6': 70, 'is_7': 71, 'is_8': 72, 'is_9': 73}\n",
      "\n",
      "Index(['pix0', 'pix1', 'pix2', 'pix3', 'pix4', 'pix5', 'pix6', 'pix7', 'pix8',\n",
      "       'pix9', 'pix10', 'pix11', 'pix12', 'pix13', 'pix14', 'pix15', 'pix16',\n",
      "       'pix17', 'pix18', 'pix19', 'pix20', 'pix21', 'pix22', 'pix23', 'pix24',\n",
      "       'pix25', 'pix26', 'pix27', 'pix28', 'pix29', 'pix30', 'pix31', 'pix32',\n",
      "       'pix33', 'pix34', 'pix35', 'pix36', 'pix37', 'pix38', 'pix39', 'pix40',\n",
      "       'pix41', 'pix42', 'pix43', 'pix44', 'pix45', 'pix46', 'pix47', 'pix48',\n",
      "       'pix49', 'pix50', 'pix51', 'pix52', 'pix53', 'pix54', 'pix55', 'pix56',\n",
      "       'pix57', 'pix58', 'pix59', 'pix60', 'pix61', 'pix62', 'pix63', 'is_0',\n",
      "       'is_1', 'is_2', 'is_3', 'is_4', 'is_5', 'is_6', 'is_7', 'is_8', 'is_9'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# coding cells, for copy-paste-and-adapt...\n",
    "\n",
    "print(COL_INDEX)\n",
    "print()\n",
    "print(COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 65)\n"
     ]
    }
   ],
   "source": [
    "print(range(0,65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data-assembly for feature-regression! +++\n",
      "\n",
      "Iteration 1, loss = 0.13363240\n",
      "Iteration 2, loss = 0.01598494\n",
      "Iteration 3, loss = 0.00433768\n",
      "Iteration 4, loss = 0.00136186\n",
      "Iteration 5, loss = 0.00065644\n",
      "Iteration 6, loss = 0.00040185\n",
      "Iteration 7, loss = 0.00014248\n",
      "Iteration 8, loss = 0.00011090\n",
      "Iteration 9, loss = 0.00009293\n",
      "Iteration 10, loss = 0.00007831\n",
      "Iteration 11, loss = 0.00006846\n",
      "Iteration 12, loss = 0.00006346\n",
      "Iteration 13, loss = 0.00006037\n",
      "Iteration 14, loss = 0.00005750\n",
      "Iteration 15, loss = 0.00005499\n",
      "Iteration 16, loss = 0.00005354\n",
      "Iteration 17, loss = 0.00005164\n",
      "Iteration 18, loss = 0.00005076\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 4.5280590565304206e-05\n",
      "Iteration 1, loss = 0.68293641\n",
      "Iteration 2, loss = 0.32301796\n",
      "Iteration 3, loss = 0.18886315\n",
      "Iteration 4, loss = 0.13369206\n",
      "Iteration 5, loss = 0.11115648\n",
      "Iteration 6, loss = 0.19775552\n",
      "Iteration 7, loss = 0.14343119\n",
      "Iteration 8, loss = 0.08907435\n",
      "Iteration 9, loss = 0.08332816\n",
      "Iteration 10, loss = 0.08258853\n",
      "Iteration 11, loss = 0.08784673\n",
      "Iteration 12, loss = 0.07143242\n",
      "Iteration 13, loss = 0.06859317\n",
      "Iteration 14, loss = 0.07277389\n",
      "Iteration 15, loss = 0.07073159\n",
      "Iteration 16, loss = 0.05931903\n",
      "Iteration 17, loss = 0.06117143\n",
      "Iteration 18, loss = 0.05689840\n",
      "Iteration 19, loss = 0.05555780\n",
      "Iteration 20, loss = 0.05339022\n",
      "Iteration 21, loss = 0.05525609\n",
      "Iteration 22, loss = 0.05268958\n",
      "Iteration 23, loss = 0.05715572\n",
      "Iteration 24, loss = 0.05518934\n",
      "Iteration 25, loss = 0.06590801\n",
      "Iteration 26, loss = 0.06122318\n",
      "Iteration 27, loss = 0.05491184\n",
      "Iteration 28, loss = 0.08397504\n",
      "Iteration 29, loss = 0.08291047\n",
      "Iteration 30, loss = 0.06637922\n",
      "Iteration 31, loss = 0.05619738\n",
      "Iteration 32, loss = 0.05217706\n",
      "Iteration 33, loss = 0.04897676\n",
      "Iteration 34, loss = 0.04691213\n",
      "Iteration 35, loss = 0.04672403\n",
      "Iteration 36, loss = 0.04810918\n",
      "Iteration 37, loss = 0.04542195\n",
      "Iteration 38, loss = 0.05164830\n",
      "Iteration 39, loss = 0.04959705\n",
      "Iteration 40, loss = 0.05555745\n",
      "Iteration 41, loss = 0.04750546\n",
      "Iteration 42, loss = 0.05476919\n",
      "Iteration 43, loss = 0.04850871\n",
      "Iteration 44, loss = 0.11504611\n",
      "Iteration 45, loss = 0.12205617\n",
      "Iteration 46, loss = 0.06390656\n",
      "Iteration 47, loss = 0.06403801\n",
      "Iteration 48, loss = 0.05763926\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 1.3994700720130266\n",
      "Iteration 1, loss = 15.26398945\n",
      "Iteration 2, loss = 4.39971956\n",
      "Iteration 3, loss = 2.81197455\n",
      "Iteration 4, loss = 2.25295604\n",
      "Iteration 5, loss = 1.89334978\n",
      "Iteration 6, loss = 1.66512582\n",
      "Iteration 7, loss = 1.54553682\n",
      "Iteration 8, loss = 1.34829516\n",
      "Iteration 9, loss = 1.32275456\n",
      "Iteration 10, loss = 1.31208338\n",
      "Iteration 11, loss = 1.22541405\n",
      "Iteration 12, loss = 1.16934367\n",
      "Iteration 13, loss = 1.16177901\n",
      "Iteration 14, loss = 1.12858747\n",
      "Iteration 15, loss = 1.10190776\n",
      "Iteration 16, loss = 1.03755491\n",
      "Iteration 17, loss = 1.19533168\n",
      "Iteration 18, loss = 1.20337851\n",
      "Iteration 19, loss = 1.13881512\n",
      "Iteration 20, loss = 1.32677459\n",
      "Iteration 21, loss = 1.35377254\n",
      "Iteration 22, loss = 1.17037552\n",
      "Iteration 23, loss = 1.14785085\n",
      "Iteration 24, loss = 1.01639043\n",
      "Iteration 25, loss = 0.99185826\n",
      "Iteration 26, loss = 0.99891413\n",
      "Iteration 27, loss = 1.04756601\n",
      "Iteration 28, loss = 0.99676699\n",
      "Iteration 29, loss = 0.98376460\n",
      "Iteration 30, loss = 0.92326045\n",
      "Iteration 31, loss = 0.89881304\n",
      "Iteration 32, loss = 0.89133230\n",
      "Iteration 33, loss = 0.88240531\n",
      "Iteration 34, loss = 0.84817750\n",
      "Iteration 35, loss = 0.81100471\n",
      "Iteration 36, loss = 0.85826037\n",
      "Iteration 37, loss = 0.85401790\n",
      "Iteration 38, loss = 0.87016247\n",
      "Iteration 39, loss = 0.82622542\n",
      "Iteration 40, loss = 0.81493014\n",
      "Iteration 41, loss = 0.83786843\n",
      "Iteration 42, loss = 0.79214010\n",
      "Iteration 43, loss = 0.88816818\n",
      "Iteration 44, loss = 0.95301802\n",
      "Iteration 45, loss = 1.00251342\n",
      "Iteration 46, loss = 0.90991432\n",
      "Iteration 47, loss = 0.78343512\n",
      "Iteration 48, loss = 0.82366148\n",
      "Iteration 49, loss = 0.84644329\n",
      "Iteration 50, loss = 0.87028858\n",
      "Iteration 51, loss = 0.85927630\n",
      "Iteration 52, loss = 0.82759139\n",
      "Iteration 53, loss = 0.96848373\n",
      "Iteration 54, loss = 0.91888917\n",
      "Iteration 55, loss = 1.04237689\n",
      "Iteration 56, loss = 1.00259243\n",
      "Iteration 57, loss = 0.88572784\n",
      "Iteration 58, loss = 0.93966388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 7.9674612348365255\n",
      "Iteration 1, loss = 34.33013748\n",
      "Iteration 2, loss = 9.71204773\n",
      "Iteration 3, loss = 5.24658842\n",
      "Iteration 4, loss = 4.04712600\n",
      "Iteration 5, loss = 3.03972213\n",
      "Iteration 6, loss = 2.66188108\n",
      "Iteration 7, loss = 2.30011730\n",
      "Iteration 8, loss = 2.13644716\n",
      "Iteration 9, loss = 2.10452093\n",
      "Iteration 10, loss = 2.07531208\n",
      "Iteration 11, loss = 1.89138247\n",
      "Iteration 12, loss = 1.93210675\n",
      "Iteration 13, loss = 1.84936060\n",
      "Iteration 14, loss = 1.81216758\n",
      "Iteration 15, loss = 1.76337568\n",
      "Iteration 16, loss = 1.70684171\n",
      "Iteration 17, loss = 1.71416372\n",
      "Iteration 18, loss = 1.74915101\n",
      "Iteration 19, loss = 1.68701283\n",
      "Iteration 20, loss = 1.78386566\n",
      "Iteration 21, loss = 1.75736550\n",
      "Iteration 22, loss = 1.88142093\n",
      "Iteration 23, loss = 1.77728263\n",
      "Iteration 24, loss = 1.60283522\n",
      "Iteration 25, loss = 1.63019133\n",
      "Iteration 26, loss = 1.52899829\n",
      "Iteration 27, loss = 1.54252433\n",
      "Iteration 28, loss = 1.48774515\n",
      "Iteration 29, loss = 1.56909523\n",
      "Iteration 30, loss = 1.46179707\n",
      "Iteration 31, loss = 1.52880231\n",
      "Iteration 32, loss = 1.51961070\n",
      "Iteration 33, loss = 1.55101958\n",
      "Iteration 34, loss = 1.58996566\n",
      "Iteration 35, loss = 1.46267526\n",
      "Iteration 36, loss = 1.41709333\n",
      "Iteration 37, loss = 1.49919293\n",
      "Iteration 38, loss = 1.45457513\n",
      "Iteration 39, loss = 1.52590489\n",
      "Iteration 40, loss = 1.41266677\n",
      "Iteration 41, loss = 1.39674264\n",
      "Iteration 42, loss = 1.52032426\n",
      "Iteration 43, loss = 1.53059508\n",
      "Iteration 44, loss = 1.42582480\n",
      "Iteration 45, loss = 1.52303322\n",
      "Iteration 46, loss = 1.65139070\n",
      "Iteration 47, loss = 1.60470286\n",
      "Iteration 48, loss = 1.44034304\n",
      "Iteration 49, loss = 1.43490991\n",
      "Iteration 50, loss = 1.38790843\n",
      "Iteration 51, loss = 1.29620063\n",
      "Iteration 52, loss = 1.39343215\n",
      "Iteration 53, loss = 1.39004083\n",
      "Iteration 54, loss = 1.39256486\n",
      "Iteration 55, loss = 1.50125811\n",
      "Iteration 56, loss = 1.30775549\n",
      "Iteration 57, loss = 1.31788206\n",
      "Iteration 58, loss = 1.36196034\n",
      "Iteration 59, loss = 1.27789300\n",
      "Iteration 60, loss = 1.27596249\n",
      "Iteration 61, loss = 1.36531860\n",
      "Iteration 62, loss = 1.46928974\n",
      "Iteration 63, loss = 1.33247129\n",
      "Iteration 64, loss = 1.26444902\n",
      "Iteration 65, loss = 1.38525274\n",
      "Iteration 66, loss = 1.49948674\n",
      "Iteration 67, loss = 1.58026494\n",
      "Iteration 68, loss = 1.48235908\n",
      "Iteration 69, loss = 1.41333631\n",
      "Iteration 70, loss = 1.41686424\n",
      "Iteration 71, loss = 1.34233002\n",
      "Iteration 72, loss = 1.46276188\n",
      "Iteration 73, loss = 1.55194442\n",
      "Iteration 74, loss = 1.25471662\n",
      "Iteration 75, loss = 1.29452404\n",
      "Iteration 76, loss = 1.49154512\n",
      "Iteration 77, loss = 1.30322858\n",
      "Iteration 78, loss = 1.25559188\n",
      "Iteration 79, loss = 1.29013814\n",
      "Iteration 80, loss = 1.29980754\n",
      "Iteration 81, loss = 1.35978823\n",
      "Iteration 82, loss = 1.32675885\n",
      "Iteration 83, loss = 1.85394770\n",
      "Iteration 84, loss = 1.88273677\n",
      "Iteration 85, loss = 1.59006101\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 14.063594807849839\n",
      "Iteration 1, loss = 39.99105430\n",
      "Iteration 2, loss = 10.36370141\n",
      "Iteration 3, loss = 6.50756809\n",
      "Iteration 4, loss = 4.93865307\n",
      "Iteration 5, loss = 4.55246520\n",
      "Iteration 6, loss = 4.12371824\n",
      "Iteration 7, loss = 3.85032746\n",
      "Iteration 8, loss = 3.64496501\n",
      "Iteration 9, loss = 3.52790733\n",
      "Iteration 10, loss = 3.46436428\n",
      "Iteration 11, loss = 3.49452572\n",
      "Iteration 12, loss = 3.44080436\n",
      "Iteration 13, loss = 3.35865028\n",
      "Iteration 14, loss = 3.23044367\n",
      "Iteration 15, loss = 2.97708662\n",
      "Iteration 16, loss = 3.18023439\n",
      "Iteration 17, loss = 3.01028919\n",
      "Iteration 18, loss = 2.81675626\n",
      "Iteration 19, loss = 2.97555787\n",
      "Iteration 20, loss = 2.87971033\n",
      "Iteration 21, loss = 2.88188178\n",
      "Iteration 22, loss = 2.88411560\n",
      "Iteration 23, loss = 2.73211849\n",
      "Iteration 24, loss = 3.21119799\n",
      "Iteration 25, loss = 3.23214472\n",
      "Iteration 26, loss = 2.80679468\n",
      "Iteration 27, loss = 2.65758009\n",
      "Iteration 28, loss = 2.42393785\n",
      "Iteration 29, loss = 2.91711362\n",
      "Iteration 30, loss = 2.59434478\n",
      "Iteration 31, loss = 2.42924761\n",
      "Iteration 32, loss = 2.43829013\n",
      "Iteration 33, loss = 2.31155185\n",
      "Iteration 34, loss = 2.40715411\n",
      "Iteration 35, loss = 2.40575837\n",
      "Iteration 36, loss = 2.21252804\n",
      "Iteration 37, loss = 2.50066236\n",
      "Iteration 38, loss = 2.30446027\n",
      "Iteration 39, loss = 2.45956983\n",
      "Iteration 40, loss = 2.57417583\n",
      "Iteration 41, loss = 2.32222032\n",
      "Iteration 42, loss = 2.21319528\n",
      "Iteration 43, loss = 2.26369697\n",
      "Iteration 44, loss = 2.26070236\n",
      "Iteration 45, loss = 2.63106803\n",
      "Iteration 46, loss = 2.55911683\n",
      "Iteration 47, loss = 2.36049277\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 8.463224433129787\n",
      "Iteration 1, loss = 16.77381662\n",
      "Iteration 2, loss = 5.80125381\n",
      "Iteration 3, loss = 4.22813316\n",
      "Iteration 4, loss = 3.76580168\n",
      "Iteration 5, loss = 3.37598324\n",
      "Iteration 6, loss = 3.14255157\n",
      "Iteration 7, loss = 3.05187888\n",
      "Iteration 8, loss = 2.83971606\n",
      "Iteration 9, loss = 2.50419126\n",
      "Iteration 10, loss = 2.39095271\n",
      "Iteration 11, loss = 2.22769670\n",
      "Iteration 12, loss = 2.21365339\n",
      "Iteration 13, loss = 2.18163524\n",
      "Iteration 14, loss = 2.09926692\n",
      "Iteration 15, loss = 2.16955582\n",
      "Iteration 16, loss = 2.29850565\n",
      "Iteration 17, loss = 2.42774501\n",
      "Iteration 18, loss = 2.13060870\n",
      "Iteration 19, loss = 2.02078826\n",
      "Iteration 20, loss = 2.06393478\n",
      "Iteration 21, loss = 2.09655676\n",
      "Iteration 22, loss = 1.97877510\n",
      "Iteration 23, loss = 2.08270051\n",
      "Iteration 24, loss = 1.88637408\n",
      "Iteration 25, loss = 1.91048880\n",
      "Iteration 26, loss = 2.01231420\n",
      "Iteration 27, loss = 1.77698425\n",
      "Iteration 28, loss = 1.86310364\n",
      "Iteration 29, loss = 1.85297558\n",
      "Iteration 30, loss = 1.67437017\n",
      "Iteration 31, loss = 1.63542081\n",
      "Iteration 32, loss = 1.71978526\n",
      "Iteration 33, loss = 1.68900400\n",
      "Iteration 34, loss = 1.78600361\n",
      "Iteration 35, loss = 1.86125800\n",
      "Iteration 36, loss = 1.76792333\n",
      "Iteration 37, loss = 1.69394960\n",
      "Iteration 38, loss = 1.61580600\n",
      "Iteration 39, loss = 1.61297039\n",
      "Iteration 40, loss = 1.64479680\n",
      "Iteration 41, loss = 1.98581993\n",
      "Iteration 42, loss = 2.37358726\n",
      "Iteration 43, loss = 2.26140136\n",
      "Iteration 44, loss = 2.41083074\n",
      "Iteration 45, loss = 1.85488049\n",
      "Iteration 46, loss = 1.82698622\n",
      "Iteration 47, loss = 2.07782351\n",
      "Iteration 48, loss = 1.86795668\n",
      "Iteration 49, loss = 1.79618941\n",
      "Iteration 50, loss = 1.69563058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.016066967928427545\n",
      "Iteration 1, loss = 4.08025768\n",
      "Iteration 2, loss = 2.19134635\n",
      "Iteration 3, loss = 1.84075434\n",
      "Iteration 4, loss = 1.60937798\n",
      "Iteration 5, loss = 1.26132069\n",
      "Iteration 6, loss = 1.06245005\n",
      "Iteration 7, loss = 0.89680581\n",
      "Iteration 8, loss = 0.82655470\n",
      "Iteration 9, loss = 0.67738309\n",
      "Iteration 10, loss = 0.70209525\n",
      "Iteration 11, loss = 0.65370105\n",
      "Iteration 12, loss = 0.66704135\n",
      "Iteration 13, loss = 0.60116178\n",
      "Iteration 14, loss = 0.56718989\n",
      "Iteration 15, loss = 0.52265348\n",
      "Iteration 16, loss = 0.54905182\n",
      "Iteration 17, loss = 0.83864695\n",
      "Iteration 18, loss = 0.93211924\n",
      "Iteration 19, loss = 1.03927711\n",
      "Iteration 20, loss = 0.68439845\n",
      "Iteration 21, loss = 0.67774250\n",
      "Iteration 22, loss = 0.54939351\n",
      "Iteration 23, loss = 0.52339833\n",
      "Iteration 24, loss = 0.48894333\n",
      "Iteration 25, loss = 0.54699688\n",
      "Iteration 26, loss = 0.57137159\n",
      "Iteration 27, loss = 0.79726118\n",
      "Iteration 28, loss = 0.77357211\n",
      "Iteration 29, loss = 0.60879552\n",
      "Iteration 30, loss = 0.50657437\n",
      "Iteration 31, loss = 0.54546505\n",
      "Iteration 32, loss = 0.58159801\n",
      "Iteration 33, loss = 0.61765702\n",
      "Iteration 34, loss = 0.71390893\n",
      "Iteration 35, loss = 0.66482088\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.36517484288954527\n",
      "Iteration 1, loss = 1.09283799\n",
      "Iteration 2, loss = 0.57567983\n",
      "Iteration 3, loss = 0.37744961\n",
      "Iteration 4, loss = 0.30468108\n",
      "Iteration 5, loss = 0.21385443\n",
      "Iteration 6, loss = 0.37125320\n",
      "Iteration 7, loss = 0.23895776\n",
      "Iteration 8, loss = 0.19135415\n",
      "Iteration 9, loss = 0.14371576\n",
      "Iteration 10, loss = 0.25758926\n",
      "Iteration 11, loss = 0.17132926\n",
      "Iteration 12, loss = 0.23521498\n",
      "Iteration 13, loss = 0.17868492\n",
      "Iteration 14, loss = 0.11050404\n",
      "Iteration 15, loss = 0.08793591\n",
      "Iteration 16, loss = 0.07907866\n",
      "Iteration 17, loss = 0.06049248\n",
      "Iteration 18, loss = 0.04219923\n",
      "Iteration 19, loss = 0.04118489\n",
      "Iteration 20, loss = 0.06607807\n",
      "Iteration 21, loss = 0.05852585\n",
      "Iteration 22, loss = 0.04152281\n",
      "Iteration 23, loss = 0.03548308\n",
      "Iteration 24, loss = 0.01918899\n",
      "Iteration 25, loss = 0.01624632\n",
      "Iteration 26, loss = 0.01616330\n",
      "Iteration 27, loss = 0.02105425\n",
      "Iteration 28, loss = 0.02362670\n",
      "Iteration 29, loss = 0.03772820\n",
      "Iteration 30, loss = 0.03592389\n",
      "Iteration 31, loss = 0.25980958\n",
      "Iteration 32, loss = 0.44531545\n",
      "Iteration 33, loss = 0.19617175\n",
      "Iteration 34, loss = 0.16956920\n",
      "Iteration 35, loss = 0.13049226\n",
      "Iteration 36, loss = 0.13472624\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: -0.003972147908339526\n",
      "Iteration 1, loss = 0.09752148\n",
      "Iteration 2, loss = 0.01134548\n",
      "Iteration 3, loss = 0.00691525\n",
      "Iteration 4, loss = 0.11571346\n",
      "Iteration 5, loss = 0.01137242\n",
      "Iteration 6, loss = 0.00739664\n",
      "Iteration 7, loss = 0.00569616\n",
      "Iteration 8, loss = 0.00550001\n",
      "Iteration 9, loss = 0.00535644\n",
      "Iteration 10, loss = 0.00539473\n",
      "Iteration 11, loss = 0.00539342\n",
      "Iteration 12, loss = 0.00535039\n",
      "Iteration 13, loss = 0.00534001\n",
      "Iteration 14, loss = 0.00535361\n",
      "Iteration 15, loss = 0.00533285\n",
      "Iteration 16, loss = 0.00536006\n",
      "Iteration 17, loss = 0.00535660\n",
      "Iteration 18, loss = 0.00535417\n",
      "Iteration 19, loss = 0.00535192\n",
      "Iteration 20, loss = 0.00536366\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.012399517981183784\n",
      "Iteration 1, loss = 3.04713129\n",
      "Iteration 2, loss = 1.79411155\n",
      "Iteration 3, loss = 1.39389558\n",
      "Iteration 4, loss = 1.26445536\n",
      "Iteration 5, loss = 1.37887934\n",
      "Iteration 6, loss = 1.22295047\n",
      "Iteration 7, loss = 1.28181354\n",
      "Iteration 8, loss = 1.22459678\n",
      "Iteration 9, loss = 1.27024308\n",
      "Iteration 10, loss = 1.20174511\n",
      "Iteration 11, loss = 1.21736554\n",
      "Iteration 12, loss = 1.14724751\n",
      "Iteration 13, loss = 1.40447768\n",
      "Iteration 14, loss = 1.13665299\n",
      "Iteration 15, loss = 1.12695540\n",
      "Iteration 16, loss = 1.20562572\n",
      "Iteration 17, loss = 1.07261023\n",
      "Iteration 18, loss = 1.01073926\n",
      "Iteration 19, loss = 0.99150439\n",
      "Iteration 20, loss = 0.98475667\n",
      "Iteration 21, loss = 0.92344047\n",
      "Iteration 22, loss = 1.01887082\n",
      "Iteration 23, loss = 0.97587873\n",
      "Iteration 24, loss = 0.88084213\n",
      "Iteration 25, loss = 1.00450954\n",
      "Iteration 26, loss = 0.90884067\n",
      "Iteration 27, loss = 0.89085052\n",
      "Iteration 28, loss = 0.88070181\n",
      "Iteration 29, loss = 0.82260628\n",
      "Iteration 30, loss = 0.84306909\n",
      "Iteration 31, loss = 0.92590329\n",
      "Iteration 32, loss = 1.28283715\n",
      "Iteration 33, loss = 1.30021320\n",
      "Iteration 34, loss = 1.30348304\n",
      "Iteration 35, loss = 1.12779202\n",
      "Iteration 36, loss = 1.17755564\n",
      "Iteration 37, loss = 1.02164401\n",
      "Iteration 38, loss = 0.96372471\n",
      "Iteration 39, loss = 0.86770540\n",
      "Iteration 40, loss = 0.86761659\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 4.159369400031997\n",
      "Iteration 1, loss = 29.65134438\n",
      "Iteration 2, loss = 10.62737887\n",
      "Iteration 3, loss = 6.04396217\n",
      "Iteration 4, loss = 5.16922505\n",
      "Iteration 5, loss = 4.34733708\n",
      "Iteration 6, loss = 3.98458725\n",
      "Iteration 7, loss = 3.79655903\n",
      "Iteration 8, loss = 3.88264620\n",
      "Iteration 9, loss = 3.15550346\n",
      "Iteration 10, loss = 3.25707162\n",
      "Iteration 11, loss = 3.21055535\n",
      "Iteration 12, loss = 3.06442217\n",
      "Iteration 13, loss = 2.70812532\n",
      "Iteration 14, loss = 2.70645879\n",
      "Iteration 15, loss = 2.65809218\n",
      "Iteration 16, loss = 2.78316413\n",
      "Iteration 17, loss = 2.32314567\n",
      "Iteration 18, loss = 2.29644298\n",
      "Iteration 19, loss = 2.27921029\n",
      "Iteration 20, loss = 2.13383126\n",
      "Iteration 21, loss = 2.26584518\n",
      "Iteration 22, loss = 2.34173328\n",
      "Iteration 23, loss = 2.25757591\n",
      "Iteration 24, loss = 2.29424625\n",
      "Iteration 25, loss = 2.22527317\n",
      "Iteration 26, loss = 2.25309090\n",
      "Iteration 27, loss = 2.32373079\n",
      "Iteration 28, loss = 2.39028805\n",
      "Iteration 29, loss = 2.33459672\n",
      "Iteration 30, loss = 2.14072309\n",
      "Iteration 31, loss = 2.05598250\n",
      "Iteration 32, loss = 2.19201324\n",
      "Iteration 33, loss = 2.42034843\n",
      "Iteration 34, loss = 2.16542260\n",
      "Iteration 35, loss = 2.49015733\n",
      "Iteration 36, loss = 2.21336381\n",
      "Iteration 37, loss = 2.43921117\n",
      "Iteration 38, loss = 2.15968212\n",
      "Iteration 39, loss = 2.01200858\n",
      "Iteration 40, loss = 1.95908183\n",
      "Iteration 41, loss = 2.10597424\n",
      "Iteration 42, loss = 1.93880794\n",
      "Iteration 43, loss = 2.39832837\n",
      "Iteration 44, loss = 2.21332134\n",
      "Iteration 45, loss = 2.16567720\n",
      "Iteration 46, loss = 2.07730943\n",
      "Iteration 47, loss = 2.02551208\n",
      "Iteration 48, loss = 1.87103717\n",
      "Iteration 49, loss = 1.81291502\n",
      "Iteration 50, loss = 2.00766688\n",
      "Iteration 51, loss = 2.10409706\n",
      "Iteration 52, loss = 2.00127838\n",
      "Iteration 53, loss = 1.94997563\n",
      "Iteration 54, loss = 1.83368085\n",
      "Iteration 55, loss = 2.14370235\n",
      "Iteration 56, loss = 2.08439567\n",
      "Iteration 57, loss = 2.02855452\n",
      "Iteration 58, loss = 2.00189222\n",
      "Iteration 59, loss = 2.11441469\n",
      "Iteration 60, loss = 1.96912620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 13.971744369932786\n",
      "Iteration 1, loss = 37.49694685\n",
      "Iteration 2, loss = 8.84278426\n",
      "Iteration 3, loss = 4.63265211\n",
      "Iteration 4, loss = 4.07268957\n",
      "Iteration 5, loss = 3.70292005\n",
      "Iteration 6, loss = 3.10707200\n",
      "Iteration 7, loss = 2.83668666\n",
      "Iteration 8, loss = 2.75017208\n",
      "Iteration 9, loss = 2.65520440\n",
      "Iteration 10, loss = 2.67245307\n",
      "Iteration 11, loss = 2.69504981\n",
      "Iteration 12, loss = 2.73138239\n",
      "Iteration 13, loss = 2.36416937\n",
      "Iteration 14, loss = 2.35131169\n",
      "Iteration 15, loss = 2.35174808\n",
      "Iteration 16, loss = 2.19626103\n",
      "Iteration 17, loss = 2.13544845\n",
      "Iteration 18, loss = 2.07314777\n",
      "Iteration 19, loss = 2.20878294\n",
      "Iteration 20, loss = 2.06505281\n",
      "Iteration 21, loss = 2.12947887\n",
      "Iteration 22, loss = 2.00245252\n",
      "Iteration 23, loss = 1.94345308\n",
      "Iteration 24, loss = 1.90071122\n",
      "Iteration 25, loss = 1.80276022\n",
      "Iteration 26, loss = 1.85461365\n",
      "Iteration 27, loss = 1.81169812\n",
      "Iteration 28, loss = 1.85294740\n",
      "Iteration 29, loss = 1.99158469\n",
      "Iteration 30, loss = 1.92640114\n",
      "Iteration 31, loss = 1.88976641\n",
      "Iteration 32, loss = 2.15238176\n",
      "Iteration 33, loss = 1.87130409\n",
      "Iteration 34, loss = 1.85469296\n",
      "Iteration 35, loss = 1.85396591\n",
      "Iteration 36, loss = 1.83150260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 13.735995024806096\n",
      "Iteration 1, loss = 39.02423238\n",
      "Iteration 2, loss = 11.70533164\n",
      "Iteration 3, loss = 8.52833373\n",
      "Iteration 4, loss = 6.38874180\n",
      "Iteration 5, loss = 5.52528960\n",
      "Iteration 6, loss = 4.54782826\n",
      "Iteration 7, loss = 4.12837035\n",
      "Iteration 8, loss = 3.95475864\n",
      "Iteration 9, loss = 3.65889178\n",
      "Iteration 10, loss = 3.44412286\n",
      "Iteration 11, loss = 3.27394601\n",
      "Iteration 12, loss = 3.16253183\n",
      "Iteration 13, loss = 3.31565830\n",
      "Iteration 14, loss = 3.19564574\n",
      "Iteration 15, loss = 3.10407900\n",
      "Iteration 16, loss = 3.15760129\n",
      "Iteration 17, loss = 2.97529802\n",
      "Iteration 18, loss = 2.95867216\n",
      "Iteration 19, loss = 3.00217604\n",
      "Iteration 20, loss = 2.87728140\n",
      "Iteration 21, loss = 3.00234997\n",
      "Iteration 22, loss = 2.88610070\n",
      "Iteration 23, loss = 2.98602575\n",
      "Iteration 24, loss = 2.97247822\n",
      "Iteration 25, loss = 2.73997096\n",
      "Iteration 26, loss = 2.87382681\n",
      "Iteration 27, loss = 2.76533648\n",
      "Iteration 28, loss = 2.92257531\n",
      "Iteration 29, loss = 2.63692643\n",
      "Iteration 30, loss = 2.51761391\n",
      "Iteration 31, loss = 2.50834598\n",
      "Iteration 32, loss = 2.41148613\n",
      "Iteration 33, loss = 2.36554242\n",
      "Iteration 34, loss = 2.44574533\n",
      "Iteration 35, loss = 2.67982239\n",
      "Iteration 36, loss = 3.12537795\n",
      "Iteration 37, loss = 2.95749721\n",
      "Iteration 38, loss = 2.74834160\n",
      "Iteration 39, loss = 2.60698400\n",
      "Iteration 40, loss = 2.37617574\n",
      "Iteration 41, loss = 2.50067787\n",
      "Iteration 42, loss = 2.54625456\n",
      "Iteration 43, loss = 2.65784624\n",
      "Iteration 44, loss = 2.53367119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 6.7015825603686245\n",
      "Iteration 1, loss = 35.28556127\n",
      "Iteration 2, loss = 9.84713712\n",
      "Iteration 3, loss = 5.76515472\n",
      "Iteration 4, loss = 4.81260393\n",
      "Iteration 5, loss = 4.30105119\n",
      "Iteration 6, loss = 3.72838791\n",
      "Iteration 7, loss = 3.32987120\n",
      "Iteration 8, loss = 3.41292265\n",
      "Iteration 9, loss = 3.41133800\n",
      "Iteration 10, loss = 3.00952307\n",
      "Iteration 11, loss = 2.99287099\n",
      "Iteration 12, loss = 2.85846586\n",
      "Iteration 13, loss = 2.67317929\n",
      "Iteration 14, loss = 2.62108472\n",
      "Iteration 15, loss = 2.62548022\n",
      "Iteration 16, loss = 2.68527896\n",
      "Iteration 17, loss = 2.66231664\n",
      "Iteration 18, loss = 2.62023668\n",
      "Iteration 19, loss = 2.45452164\n",
      "Iteration 20, loss = 2.69106677\n",
      "Iteration 21, loss = 2.34206249\n",
      "Iteration 22, loss = 2.49994505\n",
      "Iteration 23, loss = 2.34374624\n",
      "Iteration 24, loss = 2.28104928\n",
      "Iteration 25, loss = 2.04424309\n",
      "Iteration 26, loss = 2.16906227\n",
      "Iteration 27, loss = 2.03722547\n",
      "Iteration 28, loss = 1.98169687\n",
      "Iteration 29, loss = 2.02057045\n",
      "Iteration 30, loss = 2.49778726\n",
      "Iteration 31, loss = 2.20093119\n",
      "Iteration 32, loss = 2.25109068\n",
      "Iteration 33, loss = 2.05477119\n",
      "Iteration 34, loss = 2.06839359\n",
      "Iteration 35, loss = 2.30536982\n",
      "Iteration 36, loss = 2.08347294\n",
      "Iteration 37, loss = 2.04604720\n",
      "Iteration 38, loss = 1.87139131\n",
      "Iteration 39, loss = 1.95142114\n",
      "Iteration 40, loss = 1.92944536\n",
      "Iteration 41, loss = 1.85453179\n",
      "Iteration 42, loss = 1.98349277\n",
      "Iteration 43, loss = 2.26365585\n",
      "Iteration 44, loss = 2.33005455\n",
      "Iteration 45, loss = 2.14941596\n",
      "Iteration 46, loss = 1.94595273\n",
      "Iteration 47, loss = 1.79635262\n",
      "Iteration 48, loss = 1.89850173\n",
      "Iteration 49, loss = 1.79884628\n",
      "Iteration 50, loss = 1.90726739\n",
      "Iteration 51, loss = 2.05835604\n",
      "Iteration 52, loss = 1.94973091\n",
      "Iteration 53, loss = 2.26221080\n",
      "Iteration 54, loss = 2.00665011\n",
      "Iteration 55, loss = 1.79306960\n",
      "Iteration 56, loss = 1.95380135\n",
      "Iteration 57, loss = 2.18898685\n",
      "Iteration 58, loss = 2.26793944\n",
      "Iteration 59, loss = 2.24820737\n",
      "Iteration 60, loss = 1.88965037\n",
      "Iteration 61, loss = 1.75100448\n",
      "Iteration 62, loss = 1.70072777\n",
      "Iteration 63, loss = 1.67500758\n",
      "Iteration 64, loss = 1.76140798\n",
      "Iteration 65, loss = 1.68048587\n",
      "Iteration 66, loss = 1.69962801\n",
      "Iteration 67, loss = 1.90410245\n",
      "Iteration 68, loss = 1.78173675\n",
      "Iteration 69, loss = 1.77582038\n",
      "Iteration 70, loss = 1.69754726\n",
      "Iteration 71, loss = 1.67681028\n",
      "Iteration 72, loss = 1.82981023\n",
      "Iteration 73, loss = 1.87924473\n",
      "Iteration 74, loss = 1.76879640\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 17.97390682209298\n",
      "Iteration 1, loss = 5.62855249\n",
      "Iteration 2, loss = 2.56765752\n",
      "Iteration 3, loss = 1.84871476\n",
      "Iteration 4, loss = 1.48748248\n",
      "Iteration 5, loss = 1.42958596\n",
      "Iteration 6, loss = 1.18221970\n",
      "Iteration 7, loss = 1.09785254\n",
      "Iteration 8, loss = 1.09766953\n",
      "Iteration 9, loss = 1.13194748\n",
      "Iteration 10, loss = 1.08508278\n",
      "Iteration 11, loss = 1.08418782\n",
      "Iteration 12, loss = 0.98559916\n",
      "Iteration 13, loss = 1.04876013\n",
      "Iteration 14, loss = 0.94679188\n",
      "Iteration 15, loss = 1.07147145\n",
      "Iteration 16, loss = 1.11707127\n",
      "Iteration 17, loss = 1.06211114\n",
      "Iteration 18, loss = 0.96382858\n",
      "Iteration 19, loss = 1.00340508\n",
      "Iteration 20, loss = 0.89643428\n",
      "Iteration 21, loss = 0.88385677\n",
      "Iteration 22, loss = 1.08218164\n",
      "Iteration 23, loss = 1.18828200\n",
      "Iteration 24, loss = 0.92709024\n",
      "Iteration 25, loss = 0.82853662\n",
      "Iteration 26, loss = 0.83929446\n",
      "Iteration 27, loss = 0.79870239\n",
      "Iteration 28, loss = 0.81273299\n",
      "Iteration 29, loss = 0.80245246\n",
      "Iteration 30, loss = 0.71425772\n",
      "Iteration 31, loss = 0.71987292\n",
      "Iteration 32, loss = 0.92631106\n",
      "Iteration 33, loss = 0.81406518\n",
      "Iteration 34, loss = 1.04960760\n",
      "Iteration 35, loss = 1.29899988\n",
      "Iteration 36, loss = 1.46383146\n",
      "Iteration 37, loss = 1.58497112\n",
      "Iteration 38, loss = 1.19953281\n",
      "Iteration 39, loss = 0.94857632\n",
      "Iteration 40, loss = 0.79752685\n",
      "Iteration 41, loss = 0.74911298\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.4272072085254046\n",
      "Iteration 1, loss = 0.80164984\n",
      "Iteration 2, loss = 0.21825727\n",
      "Iteration 3, loss = 0.15982758\n",
      "Iteration 4, loss = 0.16617760\n",
      "Iteration 5, loss = 0.20607635\n",
      "Iteration 6, loss = 0.07695774\n",
      "Iteration 7, loss = 0.07495065\n",
      "Iteration 8, loss = 0.11180460\n",
      "Iteration 9, loss = 0.18630161\n",
      "Iteration 10, loss = 0.07032449\n",
      "Iteration 11, loss = 0.04758674\n",
      "Iteration 12, loss = 0.04222461\n",
      "Iteration 13, loss = 0.04937712\n",
      "Iteration 14, loss = 0.04383747\n",
      "Iteration 15, loss = 0.04797426\n",
      "Iteration 16, loss = 0.32910123\n",
      "Iteration 17, loss = 0.39613593\n",
      "Iteration 18, loss = 0.39721028\n",
      "Iteration 19, loss = 0.41076472\n",
      "Iteration 20, loss = 0.40656150\n",
      "Iteration 21, loss = 0.40611570\n",
      "Iteration 22, loss = 0.40755963\n",
      "Iteration 23, loss = 0.40475074\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.058631761028423185\n",
      "Iteration 1, loss = 0.53925601\n",
      "Iteration 2, loss = 0.00922431\n",
      "Iteration 3, loss = 0.02558161\n",
      "Iteration 4, loss = 0.01153129\n",
      "Iteration 5, loss = 0.00321292\n",
      "Iteration 6, loss = 0.00488499\n",
      "Iteration 7, loss = 0.00281597\n",
      "Iteration 8, loss = 0.00286817\n",
      "Iteration 9, loss = 0.00262408\n",
      "Iteration 10, loss = 0.00254789\n",
      "Iteration 11, loss = 0.00254593\n",
      "Iteration 12, loss = 0.00251399\n",
      "Iteration 13, loss = 0.00251064\n",
      "Iteration 14, loss = 0.00250761\n",
      "Iteration 15, loss = 0.00254310\n",
      "Iteration 16, loss = 0.00252252\n",
      "Iteration 17, loss = 0.00251578\n",
      "Iteration 18, loss = 0.00250681\n",
      "Iteration 19, loss = 0.00250767\n",
      "Iteration 20, loss = 0.00250530\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.002231595429799046\n",
      "Iteration 1, loss = 6.40562512\n",
      "Iteration 2, loss = 2.92487910\n",
      "Iteration 3, loss = 2.42408980\n",
      "Iteration 4, loss = 2.18575360\n",
      "Iteration 5, loss = 1.94676244\n",
      "Iteration 6, loss = 1.69065060\n",
      "Iteration 7, loss = 1.72215863\n",
      "Iteration 8, loss = 1.60260182\n",
      "Iteration 9, loss = 1.62327034\n",
      "Iteration 10, loss = 1.50802625\n",
      "Iteration 11, loss = 1.59763081\n",
      "Iteration 12, loss = 1.59418679\n",
      "Iteration 13, loss = 1.52466414\n",
      "Iteration 14, loss = 1.51698268\n",
      "Iteration 15, loss = 1.54090470\n",
      "Iteration 16, loss = 1.47698808\n",
      "Iteration 17, loss = 1.55865407\n",
      "Iteration 18, loss = 1.61493627\n",
      "Iteration 19, loss = 1.61755504\n",
      "Iteration 20, loss = 1.47033981\n",
      "Iteration 21, loss = 1.39573306\n",
      "Iteration 22, loss = 1.70453538\n",
      "Iteration 23, loss = 1.58400364\n",
      "Iteration 24, loss = 1.59278302\n",
      "Iteration 25, loss = 1.35499951\n",
      "Iteration 26, loss = 1.33357797\n",
      "Iteration 27, loss = 1.29064369\n",
      "Iteration 28, loss = 1.21910696\n",
      "Iteration 29, loss = 1.29900225\n",
      "Iteration 30, loss = 1.32507914\n",
      "Iteration 31, loss = 1.29831280\n",
      "Iteration 32, loss = 1.29042140\n",
      "Iteration 33, loss = 1.24487905\n",
      "Iteration 34, loss = 1.15685651\n",
      "Iteration 35, loss = 1.14231489\n",
      "Iteration 36, loss = 1.19124337\n",
      "Iteration 37, loss = 1.14619716\n",
      "Iteration 38, loss = 1.03701458\n",
      "Iteration 39, loss = 0.99951564\n",
      "Iteration 40, loss = 1.01467879\n",
      "Iteration 41, loss = 1.05539685\n",
      "Iteration 42, loss = 1.05019536\n",
      "Iteration 43, loss = 1.05578963\n",
      "Iteration 44, loss = 1.07705414\n",
      "Iteration 45, loss = 0.99655277\n",
      "Iteration 46, loss = 1.10224269\n",
      "Iteration 47, loss = 1.15200107\n",
      "Iteration 48, loss = 1.01952571\n",
      "Iteration 49, loss = 1.04554262\n",
      "Iteration 50, loss = 1.02058768\n",
      "Iteration 51, loss = 1.09969018\n",
      "Iteration 52, loss = 1.23999522\n",
      "Iteration 53, loss = 1.03643659\n",
      "Iteration 54, loss = 1.03617552\n",
      "Iteration 55, loss = 0.98481033\n",
      "Iteration 56, loss = 1.12121665\n",
      "Iteration 57, loss = 1.07749338\n",
      "Iteration 58, loss = 0.99872423\n",
      "Iteration 59, loss = 1.87792855\n",
      "Iteration 60, loss = 1.55591178\n",
      "Iteration 61, loss = 1.72992421\n",
      "Iteration 62, loss = 1.16291450\n",
      "Iteration 63, loss = 1.17186392\n",
      "Iteration 64, loss = 0.99114351\n",
      "Iteration 65, loss = 0.94933611\n",
      "Iteration 66, loss = 0.91811885\n",
      "Iteration 67, loss = 1.02226677\n",
      "Iteration 68, loss = 0.98141150\n",
      "Iteration 69, loss = 0.93316925\n",
      "Iteration 70, loss = 1.02587355\n",
      "Iteration 71, loss = 1.10440443\n",
      "Iteration 72, loss = 1.06724973\n",
      "Iteration 73, loss = 1.08381383\n",
      "Iteration 74, loss = 1.08858538\n",
      "Iteration 75, loss = 1.04893035\n",
      "Iteration 76, loss = 1.06674038\n",
      "Iteration 77, loss = 1.10020727\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 12.452054977784991\n",
      "Iteration 1, loss = 31.29699803\n",
      "Iteration 2, loss = 9.59845785\n",
      "Iteration 3, loss = 6.56652565\n",
      "Iteration 4, loss = 5.18342860\n",
      "Iteration 5, loss = 4.33636930\n",
      "Iteration 6, loss = 3.90998076\n",
      "Iteration 7, loss = 3.89785794\n",
      "Iteration 8, loss = 3.96568791\n",
      "Iteration 9, loss = 3.89603388\n",
      "Iteration 10, loss = 3.75712263\n",
      "Iteration 11, loss = 3.56381005\n",
      "Iteration 12, loss = 3.46268348\n",
      "Iteration 13, loss = 3.46870545\n",
      "Iteration 14, loss = 3.16231085\n",
      "Iteration 15, loss = 2.91660542\n",
      "Iteration 16, loss = 2.94476285\n",
      "Iteration 17, loss = 2.77577489\n",
      "Iteration 18, loss = 2.72837167\n",
      "Iteration 19, loss = 3.61084931\n",
      "Iteration 20, loss = 3.20184389\n",
      "Iteration 21, loss = 2.91140079\n",
      "Iteration 22, loss = 3.12318539\n",
      "Iteration 23, loss = 2.82551449\n",
      "Iteration 24, loss = 2.86229490\n",
      "Iteration 25, loss = 2.89760984\n",
      "Iteration 26, loss = 2.70822095\n",
      "Iteration 27, loss = 2.60438115\n",
      "Iteration 28, loss = 2.53205844\n",
      "Iteration 29, loss = 2.49961170\n",
      "Iteration 30, loss = 2.38028560\n",
      "Iteration 31, loss = 2.29461913\n",
      "Iteration 32, loss = 2.22342015\n",
      "Iteration 33, loss = 2.34533583\n",
      "Iteration 34, loss = 2.61596291\n",
      "Iteration 35, loss = 2.63553529\n",
      "Iteration 36, loss = 2.45715337\n",
      "Iteration 37, loss = 2.29916655\n",
      "Iteration 38, loss = 2.52364392\n",
      "Iteration 39, loss = 2.40626176\n",
      "Iteration 40, loss = 2.39210322\n",
      "Iteration 41, loss = 2.50380501\n",
      "Iteration 42, loss = 2.55921313\n",
      "Iteration 43, loss = 2.56630103\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 15.221100034505245\n",
      "Iteration 1, loss = 22.78764809\n",
      "Iteration 2, loss = 9.29325104\n",
      "Iteration 3, loss = 6.70578882\n",
      "Iteration 4, loss = 5.50554438\n",
      "Iteration 5, loss = 4.88640576\n",
      "Iteration 6, loss = 4.47561359\n",
      "Iteration 7, loss = 4.46877151\n",
      "Iteration 8, loss = 4.21880525\n",
      "Iteration 9, loss = 4.10396016\n",
      "Iteration 10, loss = 3.63241966\n",
      "Iteration 11, loss = 3.84033871\n",
      "Iteration 12, loss = 3.49781715\n",
      "Iteration 13, loss = 3.87815470\n",
      "Iteration 14, loss = 3.73348346\n",
      "Iteration 15, loss = 3.64092744\n",
      "Iteration 16, loss = 3.25656811\n",
      "Iteration 17, loss = 3.10204507\n",
      "Iteration 18, loss = 2.91127188\n",
      "Iteration 19, loss = 3.15236140\n",
      "Iteration 20, loss = 3.09024921\n",
      "Iteration 21, loss = 2.98068937\n",
      "Iteration 22, loss = 3.18661683\n",
      "Iteration 23, loss = 3.27432047\n",
      "Iteration 24, loss = 2.96217686\n",
      "Iteration 25, loss = 2.90990851\n",
      "Iteration 26, loss = 3.01257774\n",
      "Iteration 27, loss = 2.80378875\n",
      "Iteration 28, loss = 2.93580376\n",
      "Iteration 29, loss = 2.89655570\n",
      "Iteration 30, loss = 2.72146920\n",
      "Iteration 31, loss = 2.57093646\n",
      "Iteration 32, loss = 2.83685336\n",
      "Iteration 33, loss = 2.77807218\n",
      "Iteration 34, loss = 2.74246741\n",
      "Iteration 35, loss = 2.81194186\n",
      "Iteration 36, loss = 3.12244320\n",
      "Iteration 37, loss = 3.07818358\n",
      "Iteration 38, loss = 3.17850466\n",
      "Iteration 39, loss = 3.07495738\n",
      "Iteration 40, loss = 3.31027728\n",
      "Iteration 41, loss = 3.98829223\n",
      "Iteration 42, loss = 3.16442005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.9492633223819199\n",
      "Iteration 1, loss = 17.60442288\n",
      "Iteration 2, loss = 7.67519839\n",
      "Iteration 3, loss = 5.26192293\n",
      "Iteration 4, loss = 4.73371134\n",
      "Iteration 5, loss = 3.74479813\n",
      "Iteration 6, loss = 3.42744426\n",
      "Iteration 7, loss = 3.11314487\n",
      "Iteration 8, loss = 3.01404918\n",
      "Iteration 9, loss = 3.04130402\n",
      "Iteration 10, loss = 2.89961227\n",
      "Iteration 11, loss = 2.58664025\n",
      "Iteration 12, loss = 2.59332389\n",
      "Iteration 13, loss = 2.50902261\n",
      "Iteration 14, loss = 2.67591213\n",
      "Iteration 15, loss = 2.80863717\n",
      "Iteration 16, loss = 2.63659357\n",
      "Iteration 17, loss = 2.49745416\n",
      "Iteration 18, loss = 2.46889582\n",
      "Iteration 19, loss = 2.55906510\n",
      "Iteration 20, loss = 2.40275952\n",
      "Iteration 21, loss = 2.66721130\n",
      "Iteration 22, loss = 3.73522294\n",
      "Iteration 23, loss = 3.41707850\n",
      "Iteration 24, loss = 2.65769182\n",
      "Iteration 25, loss = 2.48034831\n",
      "Iteration 26, loss = 2.22000772\n",
      "Iteration 27, loss = 2.50961096\n",
      "Iteration 28, loss = 2.34424133\n",
      "Iteration 29, loss = 2.18902261\n",
      "Iteration 30, loss = 2.47830173\n",
      "Iteration 31, loss = 2.65229300\n",
      "Iteration 32, loss = 3.20604660\n",
      "Iteration 33, loss = 2.34677543\n",
      "Iteration 34, loss = 2.19262591\n",
      "Iteration 35, loss = 2.08198565\n",
      "Iteration 36, loss = 1.95868194\n",
      "Iteration 37, loss = 2.12138352\n",
      "Iteration 38, loss = 2.03270285\n",
      "Iteration 39, loss = 2.09914103\n",
      "Iteration 40, loss = 1.94150659\n",
      "Iteration 41, loss = 2.02137105\n",
      "Iteration 42, loss = 2.23021481\n",
      "Iteration 43, loss = 2.16403750\n",
      "Iteration 44, loss = 2.20526637\n",
      "Iteration 45, loss = 2.21125483\n",
      "Iteration 46, loss = 2.22915767\n",
      "Iteration 47, loss = 2.20603106\n",
      "Iteration 48, loss = 2.14406736\n",
      "Iteration 49, loss = 2.37148930\n",
      "Iteration 50, loss = 2.27696398\n",
      "Iteration 51, loss = 2.11284283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 7.1994889250878416\n",
      "Iteration 1, loss = 28.51637697\n",
      "Iteration 2, loss = 9.51072382\n",
      "Iteration 3, loss = 6.32216466\n",
      "Iteration 4, loss = 4.80179202\n",
      "Iteration 5, loss = 3.65479725\n",
      "Iteration 6, loss = 3.34054654\n",
      "Iteration 7, loss = 3.25837536\n",
      "Iteration 8, loss = 2.98117926\n",
      "Iteration 9, loss = 2.87798181\n",
      "Iteration 10, loss = 2.51835792\n",
      "Iteration 11, loss = 2.47548909\n",
      "Iteration 12, loss = 2.25399381\n",
      "Iteration 13, loss = 2.20851227\n",
      "Iteration 14, loss = 2.18300925\n",
      "Iteration 15, loss = 2.19017407\n",
      "Iteration 16, loss = 1.99368980\n",
      "Iteration 17, loss = 2.09899052\n",
      "Iteration 18, loss = 2.25641519\n",
      "Iteration 19, loss = 2.32539218\n",
      "Iteration 20, loss = 2.18334803\n",
      "Iteration 21, loss = 2.07642997\n",
      "Iteration 22, loss = 1.95231007\n",
      "Iteration 23, loss = 2.05587504\n",
      "Iteration 24, loss = 1.98682290\n",
      "Iteration 25, loss = 1.88243807\n",
      "Iteration 26, loss = 1.78833012\n",
      "Iteration 27, loss = 1.80222369\n",
      "Iteration 28, loss = 1.76478188\n",
      "Iteration 29, loss = 1.93676647\n",
      "Iteration 30, loss = 1.95454826\n",
      "Iteration 31, loss = 2.07831081\n",
      "Iteration 32, loss = 2.10385464\n",
      "Iteration 33, loss = 1.96146655\n",
      "Iteration 34, loss = 1.93034643\n",
      "Iteration 35, loss = 1.99612428\n",
      "Iteration 36, loss = 1.82453834\n",
      "Iteration 37, loss = 1.70100469\n",
      "Iteration 38, loss = 1.61737992\n",
      "Iteration 39, loss = 1.58966146\n",
      "Iteration 40, loss = 1.67339893\n",
      "Iteration 41, loss = 1.67015461\n",
      "Iteration 42, loss = 1.60147242\n",
      "Iteration 43, loss = 1.66528617\n",
      "Iteration 44, loss = 1.84954757\n",
      "Iteration 45, loss = 1.71536016\n",
      "Iteration 46, loss = 1.75350086\n",
      "Iteration 47, loss = 1.61098555\n",
      "Iteration 48, loss = 1.67667761\n",
      "Iteration 49, loss = 1.67064731\n",
      "Iteration 50, loss = 1.97034741\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 2.259703111216444\n",
      "Iteration 1, loss = 4.34312011\n",
      "Iteration 2, loss = 2.07006947\n",
      "Iteration 3, loss = 1.94867139\n",
      "Iteration 4, loss = 1.53861226\n",
      "Iteration 5, loss = 1.41507254\n",
      "Iteration 6, loss = 1.27567895\n",
      "Iteration 7, loss = 1.17777743\n",
      "Iteration 8, loss = 1.14885021\n",
      "Iteration 9, loss = 1.09631027\n",
      "Iteration 10, loss = 1.32312508\n",
      "Iteration 11, loss = 1.10771957\n",
      "Iteration 12, loss = 1.07877245\n",
      "Iteration 13, loss = 1.14829993\n",
      "Iteration 14, loss = 1.04372067\n",
      "Iteration 15, loss = 0.98989156\n",
      "Iteration 16, loss = 0.98387480\n",
      "Iteration 17, loss = 0.94469005\n",
      "Iteration 18, loss = 0.91955920\n",
      "Iteration 19, loss = 0.82736044\n",
      "Iteration 20, loss = 0.96804978\n",
      "Iteration 21, loss = 0.93881328\n",
      "Iteration 22, loss = 0.78729003\n",
      "Iteration 23, loss = 1.42190012\n",
      "Iteration 24, loss = 1.65891850\n",
      "Iteration 25, loss = 1.18329489\n",
      "Iteration 26, loss = 1.16375956\n",
      "Iteration 27, loss = 0.83468668\n",
      "Iteration 28, loss = 1.41518799\n",
      "Iteration 29, loss = 1.02090735\n",
      "Iteration 30, loss = 0.87655105\n",
      "Iteration 31, loss = 0.87091755\n",
      "Iteration 32, loss = 0.86967829\n",
      "Iteration 33, loss = 0.80580723\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.15231386074912054\n",
      "Iteration 1, loss = 0.48460017\n",
      "Iteration 2, loss = 0.10704713\n",
      "Iteration 3, loss = 0.10065442\n",
      "Iteration 4, loss = 0.09591643\n",
      "Iteration 5, loss = 0.09245197\n",
      "Iteration 6, loss = 0.08354254\n",
      "Iteration 7, loss = 0.06278124\n",
      "Iteration 8, loss = 0.04357719\n",
      "Iteration 9, loss = 0.02971861\n",
      "Iteration 10, loss = 0.02028561\n",
      "Iteration 11, loss = 0.02036043\n",
      "Iteration 12, loss = 0.03390078\n",
      "Iteration 13, loss = 0.04263698\n",
      "Iteration 14, loss = 0.02903006\n",
      "Iteration 15, loss = 0.04594616\n",
      "Iteration 16, loss = 0.01988486\n",
      "Iteration 17, loss = 0.01562085\n",
      "Iteration 18, loss = 0.01383393\n",
      "Iteration 19, loss = 0.01430839\n",
      "Iteration 20, loss = 0.01731464\n",
      "Iteration 21, loss = 0.02169418\n",
      "Iteration 22, loss = 0.03010336\n",
      "Iteration 23, loss = 0.01545443\n",
      "Iteration 24, loss = 0.01160449\n",
      "Iteration 25, loss = 0.01218303\n",
      "Iteration 26, loss = 0.01126207\n",
      "Iteration 27, loss = 0.01069622\n",
      "Iteration 28, loss = 0.01157451\n",
      "Iteration 29, loss = 0.03141713\n",
      "Iteration 30, loss = 0.04568170\n",
      "Iteration 31, loss = 0.01775140\n",
      "Iteration 32, loss = 0.03717638\n",
      "Iteration 33, loss = 0.03165521\n",
      "Iteration 34, loss = 0.09808292\n",
      "Iteration 35, loss = 0.09871006\n",
      "Iteration 36, loss = 0.09848746\n",
      "Iteration 37, loss = 0.09848652\n",
      "Iteration 38, loss = 0.09797713\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.0476883830313312\n",
      "Iteration 1, loss = 0.18835841\n",
      "Iteration 2, loss = 0.01643615\n",
      "Iteration 3, loss = 0.00513573\n",
      "Iteration 4, loss = 0.00267935\n",
      "Iteration 5, loss = 0.00120659\n",
      "Iteration 6, loss = 0.00110918\n",
      "Iteration 7, loss = 0.00086999\n",
      "Iteration 8, loss = 0.00079560\n",
      "Iteration 9, loss = 0.00076246\n",
      "Iteration 10, loss = 0.00075664\n",
      "Iteration 11, loss = 0.00074208\n",
      "Iteration 12, loss = 0.00074452\n",
      "Iteration 13, loss = 0.00073842\n",
      "Iteration 14, loss = 0.00073564\n",
      "Iteration 15, loss = 0.00073210\n",
      "Iteration 16, loss = 0.00073125\n",
      "Iteration 17, loss = 0.00073092\n",
      "Iteration 18, loss = 0.00073224\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.003185871948548729\n",
      "Iteration 1, loss = 4.70921498\n",
      "Iteration 2, loss = 2.21631466\n",
      "Iteration 3, loss = 1.93240651\n",
      "Iteration 4, loss = 1.66105661\n",
      "Iteration 5, loss = 1.33158053\n",
      "Iteration 6, loss = 1.32349747\n",
      "Iteration 7, loss = 1.28423179\n",
      "Iteration 8, loss = 1.38898139\n",
      "Iteration 9, loss = 1.19183968\n",
      "Iteration 10, loss = 1.08743398\n",
      "Iteration 11, loss = 0.99157761\n",
      "Iteration 12, loss = 0.95315602\n",
      "Iteration 13, loss = 1.06730782\n",
      "Iteration 14, loss = 1.08077079\n",
      "Iteration 15, loss = 1.06169846\n",
      "Iteration 16, loss = 1.17995250\n",
      "Iteration 17, loss = 1.11130878\n",
      "Iteration 18, loss = 0.95157794\n",
      "Iteration 19, loss = 1.13223875\n",
      "Iteration 20, loss = 1.00703351\n",
      "Iteration 21, loss = 0.89929768\n",
      "Iteration 22, loss = 0.96727654\n",
      "Iteration 23, loss = 0.80910161\n",
      "Iteration 24, loss = 0.81512622\n",
      "Iteration 25, loss = 0.78584314\n",
      "Iteration 26, loss = 0.87245998\n",
      "Iteration 27, loss = 0.93856889\n",
      "Iteration 28, loss = 0.93019775\n",
      "Iteration 29, loss = 0.86918720\n",
      "Iteration 30, loss = 1.00070925\n",
      "Iteration 31, loss = 1.00422592\n",
      "Iteration 32, loss = 0.90804426\n",
      "Iteration 33, loss = 0.99843707\n",
      "Iteration 34, loss = 0.88361106\n",
      "Iteration 35, loss = 0.86708936\n",
      "Iteration 36, loss = 0.98761461\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 5.8562986530628995\n",
      "Iteration 1, loss = 38.21168187\n",
      "Iteration 2, loss = 10.75356650\n",
      "Iteration 3, loss = 7.41940690\n",
      "Iteration 4, loss = 5.83332118\n",
      "Iteration 5, loss = 5.14063835\n",
      "Iteration 6, loss = 4.74928161\n",
      "Iteration 7, loss = 4.11687795\n",
      "Iteration 8, loss = 3.84030048\n",
      "Iteration 9, loss = 3.54693894\n",
      "Iteration 10, loss = 3.47818270\n",
      "Iteration 11, loss = 3.36598772\n",
      "Iteration 12, loss = 3.72446412\n",
      "Iteration 13, loss = 3.39828152\n",
      "Iteration 14, loss = 3.24078357\n",
      "Iteration 15, loss = 2.88993103\n",
      "Iteration 16, loss = 2.86317478\n",
      "Iteration 17, loss = 2.77762545\n",
      "Iteration 18, loss = 2.68358836\n",
      "Iteration 19, loss = 2.57777816\n",
      "Iteration 20, loss = 2.65271348\n",
      "Iteration 21, loss = 2.54595301\n",
      "Iteration 22, loss = 2.50216362\n",
      "Iteration 23, loss = 2.56574666\n",
      "Iteration 24, loss = 2.39360814\n",
      "Iteration 25, loss = 2.34278671\n",
      "Iteration 26, loss = 2.76009333\n",
      "Iteration 27, loss = 2.47074319\n",
      "Iteration 28, loss = 2.43719352\n",
      "Iteration 29, loss = 2.66749299\n",
      "Iteration 30, loss = 2.39115171\n",
      "Iteration 31, loss = 2.27085086\n",
      "Iteration 32, loss = 2.16405053\n",
      "Iteration 33, loss = 2.19205871\n",
      "Iteration 34, loss = 2.17665932\n",
      "Iteration 35, loss = 2.15777356\n",
      "Iteration 36, loss = 2.06289573\n",
      "Iteration 37, loss = 2.12946824\n",
      "Iteration 38, loss = 2.20396392\n",
      "Iteration 39, loss = 2.22470915\n",
      "Iteration 40, loss = 2.06439756\n",
      "Iteration 41, loss = 1.98999010\n",
      "Iteration 42, loss = 2.07121155\n",
      "Iteration 43, loss = 1.98007343\n",
      "Iteration 44, loss = 1.82523686\n",
      "Iteration 45, loss = 2.68721016\n",
      "Iteration 46, loss = 2.00649349\n",
      "Iteration 47, loss = 1.90709953\n",
      "Iteration 48, loss = 1.94288425\n",
      "Iteration 49, loss = 1.93934888\n",
      "Iteration 50, loss = 1.88160443\n",
      "Iteration 51, loss = 1.85815164\n",
      "Iteration 52, loss = 1.91194009\n",
      "Iteration 53, loss = 1.90902881\n",
      "Iteration 54, loss = 1.77510573\n",
      "Iteration 55, loss = 1.63995887\n",
      "Iteration 56, loss = 1.65713029\n",
      "Iteration 57, loss = 1.64972229\n",
      "Iteration 58, loss = 1.76049077\n",
      "Iteration 59, loss = 1.92193936\n",
      "Iteration 60, loss = 1.82076371\n",
      "Iteration 61, loss = 1.73374604\n",
      "Iteration 62, loss = 1.67425938\n",
      "Iteration 63, loss = 1.64475357\n",
      "Iteration 64, loss = 1.66550695\n",
      "Iteration 65, loss = 1.59582620\n",
      "Iteration 66, loss = 1.72339589\n",
      "Iteration 67, loss = 1.91989138\n",
      "Iteration 68, loss = 2.05444186\n",
      "Iteration 69, loss = 2.04237120\n",
      "Iteration 70, loss = 1.95434566\n",
      "Iteration 71, loss = 1.99615238\n",
      "Iteration 72, loss = 2.04199025\n",
      "Iteration 73, loss = 1.71990243\n",
      "Iteration 74, loss = 1.79397365\n",
      "Iteration 75, loss = 1.78702398\n",
      "Iteration 76, loss = 1.89980042\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 14.303271583275443\n",
      "Iteration 1, loss = 25.38877245\n",
      "Iteration 2, loss = 11.21991697\n",
      "Iteration 3, loss = 8.18491960\n",
      "Iteration 4, loss = 7.08787908\n",
      "Iteration 5, loss = 6.02487435\n",
      "Iteration 6, loss = 5.44246131\n",
      "Iteration 7, loss = 4.85007333\n",
      "Iteration 8, loss = 4.72385741\n",
      "Iteration 9, loss = 4.51160151\n",
      "Iteration 10, loss = 4.45873104\n",
      "Iteration 11, loss = 4.59957580\n",
      "Iteration 12, loss = 4.14546910\n",
      "Iteration 13, loss = 4.28743798\n",
      "Iteration 14, loss = 4.07299533\n",
      "Iteration 15, loss = 3.87666959\n",
      "Iteration 16, loss = 3.81242554\n",
      "Iteration 17, loss = 3.83804843\n",
      "Iteration 18, loss = 4.27334006\n",
      "Iteration 19, loss = 3.75270137\n",
      "Iteration 20, loss = 3.52723184\n",
      "Iteration 21, loss = 3.26803040\n",
      "Iteration 22, loss = 3.35106625\n",
      "Iteration 23, loss = 3.44258616\n",
      "Iteration 24, loss = 3.28332743\n",
      "Iteration 25, loss = 3.40862579\n",
      "Iteration 26, loss = 3.22080087\n",
      "Iteration 27, loss = 3.35421558\n",
      "Iteration 28, loss = 3.31770275\n",
      "Iteration 29, loss = 3.58540764\n",
      "Iteration 30, loss = 3.32425147\n",
      "Iteration 31, loss = 3.17932770\n",
      "Iteration 32, loss = 3.00562072\n",
      "Iteration 33, loss = 2.93730938\n",
      "Iteration 34, loss = 2.80456748\n",
      "Iteration 35, loss = 2.89989153\n",
      "Iteration 36, loss = 3.07689452\n",
      "Iteration 37, loss = 3.30302822\n",
      "Iteration 38, loss = 3.02116552\n",
      "Iteration 39, loss = 3.01442471\n",
      "Iteration 40, loss = 2.89564157\n",
      "Iteration 41, loss = 2.94080428\n",
      "Iteration 42, loss = 2.89414090\n",
      "Iteration 43, loss = 2.85575487\n",
      "Iteration 44, loss = 2.60572698\n",
      "Iteration 45, loss = 2.65094543\n",
      "Iteration 46, loss = 2.61238812\n",
      "Iteration 47, loss = 2.64895098\n",
      "Iteration 48, loss = 2.48792521\n",
      "Iteration 49, loss = 2.52203786\n",
      "Iteration 50, loss = 2.56704774\n",
      "Iteration 51, loss = 3.01213435\n",
      "Iteration 52, loss = 2.84334844\n",
      "Iteration 53, loss = 2.73937128\n",
      "Iteration 54, loss = 3.42343100\n",
      "Iteration 55, loss = 2.99474190\n",
      "Iteration 56, loss = 2.63048157\n",
      "Iteration 57, loss = 2.88993663\n",
      "Iteration 58, loss = 2.71778221\n",
      "Iteration 59, loss = 2.80132598\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.4460816195514913\n",
      "Iteration 1, loss = 40.12235349\n",
      "Iteration 2, loss = 13.89515790\n",
      "Iteration 3, loss = 8.39471718\n",
      "Iteration 4, loss = 7.25147260\n",
      "Iteration 5, loss = 5.46818223\n",
      "Iteration 6, loss = 4.91259161\n",
      "Iteration 7, loss = 4.54856005\n",
      "Iteration 8, loss = 4.39739889\n",
      "Iteration 9, loss = 3.55848641\n",
      "Iteration 10, loss = 3.33147072\n",
      "Iteration 11, loss = 3.16383772\n",
      "Iteration 12, loss = 2.88621004\n",
      "Iteration 13, loss = 2.76737556\n",
      "Iteration 14, loss = 2.84722956\n",
      "Iteration 15, loss = 2.68560668\n",
      "Iteration 16, loss = 2.58270532\n",
      "Iteration 17, loss = 2.59264193\n",
      "Iteration 18, loss = 2.35740288\n",
      "Iteration 19, loss = 2.30693784\n",
      "Iteration 20, loss = 2.41870536\n",
      "Iteration 21, loss = 2.28085264\n",
      "Iteration 22, loss = 2.34002103\n",
      "Iteration 23, loss = 2.22099530\n",
      "Iteration 24, loss = 2.07776822\n",
      "Iteration 25, loss = 2.15897766\n",
      "Iteration 26, loss = 1.95289478\n",
      "Iteration 27, loss = 1.94056261\n",
      "Iteration 28, loss = 2.13795270\n",
      "Iteration 29, loss = 1.95286527\n",
      "Iteration 30, loss = 1.97043619\n",
      "Iteration 31, loss = 1.91917524\n",
      "Iteration 32, loss = 1.72869428\n",
      "Iteration 33, loss = 1.82523662\n",
      "Iteration 34, loss = 2.42529929\n",
      "Iteration 35, loss = 3.22801198\n",
      "Iteration 36, loss = 3.08993307\n",
      "Iteration 37, loss = 2.38139986\n",
      "Iteration 38, loss = 2.03620145\n",
      "Iteration 39, loss = 2.20825679\n",
      "Iteration 40, loss = 1.86251160\n",
      "Iteration 41, loss = 1.81688186\n",
      "Iteration 42, loss = 1.94051805\n",
      "Iteration 43, loss = 1.67299402\n",
      "Iteration 44, loss = 1.88440858\n",
      "Iteration 45, loss = 1.97566962\n",
      "Iteration 46, loss = 1.88876604\n",
      "Iteration 47, loss = 1.76586763\n",
      "Iteration 48, loss = 1.70229939\n",
      "Iteration 49, loss = 1.82906799\n",
      "Iteration 50, loss = 1.61288573\n",
      "Iteration 51, loss = 1.69585845\n",
      "Iteration 52, loss = 1.59265399\n",
      "Iteration 53, loss = 1.57137464\n",
      "Iteration 54, loss = 1.88782188\n",
      "Iteration 55, loss = 1.69321284\n",
      "Iteration 56, loss = 1.68811162\n",
      "Iteration 57, loss = 2.24169137\n",
      "Iteration 58, loss = 1.76413923\n",
      "Iteration 59, loss = 1.65613378\n",
      "Iteration 60, loss = 1.69958908\n",
      "Iteration 61, loss = 1.71205864\n",
      "Iteration 62, loss = 1.60417577\n",
      "Iteration 63, loss = 1.70746471\n",
      "Iteration 64, loss = 1.77924467\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 12.900906330615417\n",
      "Iteration 1, loss = 18.25054451\n",
      "Iteration 2, loss = 7.42262905\n",
      "Iteration 3, loss = 4.94015999\n",
      "Iteration 4, loss = 3.81055126\n",
      "Iteration 5, loss = 3.49002745\n",
      "Iteration 6, loss = 3.26954595\n",
      "Iteration 7, loss = 3.15305407\n",
      "Iteration 8, loss = 3.04369478\n",
      "Iteration 9, loss = 2.77748777\n",
      "Iteration 10, loss = 2.89285071\n",
      "Iteration 11, loss = 2.74121947\n",
      "Iteration 12, loss = 2.35285932\n",
      "Iteration 13, loss = 2.36677211\n",
      "Iteration 14, loss = 2.20318460\n",
      "Iteration 15, loss = 2.21947747\n",
      "Iteration 16, loss = 2.33445851\n",
      "Iteration 17, loss = 2.52659262\n",
      "Iteration 18, loss = 2.26888996\n",
      "Iteration 19, loss = 2.41133870\n",
      "Iteration 20, loss = 2.42505154\n",
      "Iteration 21, loss = 2.52263828\n",
      "Iteration 22, loss = 2.57151335\n",
      "Iteration 23, loss = 2.64574057\n",
      "Iteration 24, loss = 2.55425366\n",
      "Iteration 25, loss = 2.44297132\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 13.292697246080362\n",
      "Iteration 1, loss = 4.25723177\n",
      "Iteration 2, loss = 2.37685309\n",
      "Iteration 3, loss = 2.05930242\n",
      "Iteration 4, loss = 2.16510544\n",
      "Iteration 5, loss = 1.99697886\n",
      "Iteration 6, loss = 1.50702706\n",
      "Iteration 7, loss = 2.07597003\n",
      "Iteration 8, loss = 1.71258497\n",
      "Iteration 9, loss = 1.60080539\n",
      "Iteration 10, loss = 1.44355113\n",
      "Iteration 11, loss = 1.38370844\n",
      "Iteration 12, loss = 1.46128109\n",
      "Iteration 13, loss = 1.27765437\n",
      "Iteration 14, loss = 1.21936876\n",
      "Iteration 15, loss = 1.07236978\n",
      "Iteration 16, loss = 1.13506891\n",
      "Iteration 17, loss = 1.05400071\n",
      "Iteration 18, loss = 1.05789493\n",
      "Iteration 19, loss = 1.08968278\n",
      "Iteration 20, loss = 1.09812287\n",
      "Iteration 21, loss = 0.99771016\n",
      "Iteration 22, loss = 0.93365654\n",
      "Iteration 23, loss = 0.86609820\n",
      "Iteration 24, loss = 0.89836477\n",
      "Iteration 25, loss = 0.92202150\n",
      "Iteration 26, loss = 0.96888372\n",
      "Iteration 27, loss = 0.88727170\n",
      "Iteration 28, loss = 0.79101239\n",
      "Iteration 29, loss = 0.78972594\n",
      "Iteration 30, loss = 0.82551191\n",
      "Iteration 31, loss = 1.02430745\n",
      "Iteration 32, loss = 1.06323752\n",
      "Iteration 33, loss = 0.83839133\n",
      "Iteration 34, loss = 0.84191089\n",
      "Iteration 35, loss = 0.85107353\n",
      "Iteration 36, loss = 0.74649200\n",
      "Iteration 37, loss = 0.81950783\n",
      "Iteration 38, loss = 0.81117626\n",
      "Iteration 39, loss = 0.74669889\n",
      "Iteration 40, loss = 0.80648250\n",
      "Iteration 41, loss = 0.83280029\n",
      "Iteration 42, loss = 0.70359382\n",
      "Iteration 43, loss = 0.63378174\n",
      "Iteration 44, loss = 0.63746979\n",
      "Iteration 45, loss = 0.60460173\n",
      "Iteration 46, loss = 0.60340480\n",
      "Iteration 47, loss = 0.59919190\n",
      "Iteration 48, loss = 0.54869997\n",
      "Iteration 49, loss = 0.55167565\n",
      "Iteration 50, loss = 0.58472886\n",
      "Iteration 51, loss = 1.54648277\n",
      "Iteration 52, loss = 1.01030095\n",
      "Iteration 53, loss = 1.03018039\n",
      "Iteration 54, loss = 0.98493546\n",
      "Iteration 55, loss = 0.75901046\n",
      "Iteration 56, loss = 1.06490971\n",
      "Iteration 57, loss = 1.31444077\n",
      "Iteration 58, loss = 0.98843570\n",
      "Iteration 59, loss = 0.89353663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 3.2084745285694907\n",
      "Iteration 1, loss = 0.11494375\n",
      "Iteration 2, loss = 0.01191319\n",
      "Iteration 3, loss = 0.00277771\n",
      "Iteration 4, loss = 0.00188164\n",
      "Iteration 5, loss = 0.00263522\n",
      "Iteration 6, loss = 0.00138462\n",
      "Iteration 7, loss = 0.00092872\n",
      "Iteration 8, loss = 0.00078699\n",
      "Iteration 9, loss = 0.00077323\n",
      "Iteration 10, loss = 0.00077730\n",
      "Iteration 11, loss = 0.00073748\n",
      "Iteration 12, loss = 0.00072210\n",
      "Iteration 13, loss = 0.00067854\n",
      "Iteration 14, loss = 0.00061200\n",
      "Iteration 15, loss = 0.00058831\n",
      "Iteration 16, loss = 0.00052436\n",
      "Iteration 17, loss = 0.00048248\n",
      "Iteration 18, loss = 0.00044220\n",
      "Iteration 19, loss = 0.00043418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: -0.007303031419671111\n",
      "Iteration 1, loss = 0.38493365\n",
      "Iteration 2, loss = 0.05740704\n",
      "Iteration 3, loss = 0.00904741\n",
      "Iteration 4, loss = 0.00411648\n",
      "Iteration 5, loss = 0.00310243\n",
      "Iteration 6, loss = 0.00080078\n",
      "Iteration 7, loss = 0.00065709\n",
      "Iteration 8, loss = 0.00015761\n",
      "Iteration 9, loss = 0.00016014\n",
      "Iteration 10, loss = 0.00009129\n",
      "Iteration 11, loss = 0.00008640\n",
      "Iteration 12, loss = 0.00006948\n",
      "Iteration 13, loss = 0.00006467\n",
      "Iteration 14, loss = 0.00006058\n",
      "Iteration 15, loss = 0.00005818\n",
      "Iteration 16, loss = 0.00005779\n",
      "Iteration 17, loss = 0.00005586\n",
      "Iteration 18, loss = 0.00006305\n",
      "Iteration 19, loss = 0.00005522\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.0012757254101359858\n",
      "Iteration 1, loss = 4.53112547\n",
      "Iteration 2, loss = 2.38636170\n",
      "Iteration 3, loss = 1.53833877\n",
      "Iteration 4, loss = 1.49188000\n",
      "Iteration 5, loss = 1.48113985\n",
      "Iteration 6, loss = 1.26973471\n",
      "Iteration 7, loss = 1.18176401\n",
      "Iteration 8, loss = 1.11972893\n",
      "Iteration 9, loss = 1.03436855\n",
      "Iteration 10, loss = 0.98525983\n",
      "Iteration 11, loss = 0.98815926\n",
      "Iteration 12, loss = 1.06730432\n",
      "Iteration 13, loss = 0.94940053\n",
      "Iteration 14, loss = 0.97203070\n",
      "Iteration 15, loss = 0.90255060\n",
      "Iteration 16, loss = 1.00430178\n",
      "Iteration 17, loss = 0.83279162\n",
      "Iteration 18, loss = 0.87568042\n",
      "Iteration 19, loss = 0.84941729\n",
      "Iteration 20, loss = 0.78136048\n",
      "Iteration 21, loss = 0.90566317\n",
      "Iteration 22, loss = 0.87394376\n",
      "Iteration 23, loss = 0.92917249\n",
      "Iteration 24, loss = 0.97768737\n",
      "Iteration 25, loss = 1.06927390\n",
      "Iteration 26, loss = 0.98884960\n",
      "Iteration 27, loss = 0.90608268\n",
      "Iteration 28, loss = 0.93623768\n",
      "Iteration 29, loss = 0.86954553\n",
      "Iteration 30, loss = 0.93356145\n",
      "Iteration 31, loss = 0.78002534\n",
      "Iteration 32, loss = 0.80952702\n",
      "Iteration 33, loss = 0.94728029\n",
      "Iteration 34, loss = 1.05074915\n",
      "Iteration 35, loss = 0.82475723\n",
      "Iteration 36, loss = 0.72351084\n",
      "Iteration 37, loss = 0.77431871\n",
      "Iteration 38, loss = 0.75661923\n",
      "Iteration 39, loss = 0.69806642\n",
      "Iteration 40, loss = 0.62962111\n",
      "Iteration 41, loss = 0.67194069\n",
      "Iteration 42, loss = 0.65892866\n",
      "Iteration 43, loss = 0.74691687\n",
      "Iteration 44, loss = 0.62287784\n",
      "Iteration 45, loss = 0.60581866\n",
      "Iteration 46, loss = 0.57640899\n",
      "Iteration 47, loss = 0.58814537\n",
      "Iteration 48, loss = 0.63887338\n",
      "Iteration 49, loss = 0.65578966\n",
      "Iteration 50, loss = 0.65403850\n",
      "Iteration 51, loss = 0.68278168\n",
      "Iteration 52, loss = 0.64802974\n",
      "Iteration 53, loss = 0.60266568\n",
      "Iteration 54, loss = 0.56428386\n",
      "Iteration 55, loss = 0.55992650\n",
      "Iteration 56, loss = 0.54594279\n",
      "Iteration 57, loss = 0.52179346\n",
      "Iteration 58, loss = 0.62766641\n",
      "Iteration 59, loss = 0.58492179\n",
      "Iteration 60, loss = 0.88309692\n",
      "Iteration 61, loss = 0.80827629\n",
      "Iteration 62, loss = 0.72161122\n",
      "Iteration 63, loss = 0.70314187\n",
      "Iteration 64, loss = 0.67614190\n",
      "Iteration 65, loss = 0.65025752\n",
      "Iteration 66, loss = 0.64274028\n",
      "Iteration 67, loss = 0.59539905\n",
      "Iteration 68, loss = 0.59506544\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.1357258923639227\n",
      "Iteration 1, loss = 24.95406416\n",
      "Iteration 2, loss = 10.17300568\n",
      "Iteration 3, loss = 6.61184997\n",
      "Iteration 4, loss = 4.91433000\n",
      "Iteration 5, loss = 4.03578021\n",
      "Iteration 6, loss = 3.58449505\n",
      "Iteration 7, loss = 3.30912975\n",
      "Iteration 8, loss = 3.10388109\n",
      "Iteration 9, loss = 2.86736222\n",
      "Iteration 10, loss = 2.83096777\n",
      "Iteration 11, loss = 2.64729709\n",
      "Iteration 12, loss = 2.58972292\n",
      "Iteration 13, loss = 2.95504811\n",
      "Iteration 14, loss = 2.70032595\n",
      "Iteration 15, loss = 2.72202933\n",
      "Iteration 16, loss = 2.67799806\n",
      "Iteration 17, loss = 2.37651402\n",
      "Iteration 18, loss = 2.26335001\n",
      "Iteration 19, loss = 2.03590430\n",
      "Iteration 20, loss = 1.95929430\n",
      "Iteration 21, loss = 2.06417568\n",
      "Iteration 22, loss = 1.98439106\n",
      "Iteration 23, loss = 1.97033985\n",
      "Iteration 24, loss = 2.01375542\n",
      "Iteration 25, loss = 1.92679032\n",
      "Iteration 26, loss = 1.96916349\n",
      "Iteration 27, loss = 1.97773922\n",
      "Iteration 28, loss = 1.86118785\n",
      "Iteration 29, loss = 1.76122914\n",
      "Iteration 30, loss = 1.80971087\n",
      "Iteration 31, loss = 1.90030805\n",
      "Iteration 32, loss = 1.66458394\n",
      "Iteration 33, loss = 1.65714257\n",
      "Iteration 34, loss = 1.66317698\n",
      "Iteration 35, loss = 1.82193129\n",
      "Iteration 36, loss = 2.06421007\n",
      "Iteration 37, loss = 2.06187331\n",
      "Iteration 38, loss = 1.76263140\n",
      "Iteration 39, loss = 1.86277359\n",
      "Iteration 40, loss = 1.80628701\n",
      "Iteration 41, loss = 1.89693778\n",
      "Iteration 42, loss = 1.80907619\n",
      "Iteration 43, loss = 1.78973384\n",
      "Iteration 44, loss = 1.88751499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 15.473939967559271\n",
      "Iteration 1, loss = 27.91603788\n",
      "Iteration 2, loss = 11.06022072\n",
      "Iteration 3, loss = 7.75952134\n",
      "Iteration 4, loss = 6.06158141\n",
      "Iteration 5, loss = 5.29752455\n",
      "Iteration 6, loss = 4.70925884\n",
      "Iteration 7, loss = 4.37232784\n",
      "Iteration 8, loss = 4.18150379\n",
      "Iteration 9, loss = 3.79675600\n",
      "Iteration 10, loss = 3.69265832\n",
      "Iteration 11, loss = 3.57259924\n",
      "Iteration 12, loss = 3.68877538\n",
      "Iteration 13, loss = 3.39497019\n",
      "Iteration 14, loss = 3.19884516\n",
      "Iteration 15, loss = 3.26508934\n",
      "Iteration 16, loss = 3.25674731\n",
      "Iteration 17, loss = 3.26473208\n",
      "Iteration 18, loss = 3.40644023\n",
      "Iteration 19, loss = 3.15980264\n",
      "Iteration 20, loss = 3.16068518\n",
      "Iteration 21, loss = 2.89341502\n",
      "Iteration 22, loss = 2.94924542\n",
      "Iteration 23, loss = 3.14395601\n",
      "Iteration 24, loss = 2.92134845\n",
      "Iteration 25, loss = 2.84367502\n",
      "Iteration 26, loss = 2.74346681\n",
      "Iteration 27, loss = 2.60755315\n",
      "Iteration 28, loss = 2.78530067\n",
      "Iteration 29, loss = 2.54217010\n",
      "Iteration 30, loss = 2.59720377\n",
      "Iteration 31, loss = 2.74926329\n",
      "Iteration 32, loss = 2.86264666\n",
      "Iteration 33, loss = 2.95873944\n",
      "Iteration 34, loss = 2.74062591\n",
      "Iteration 35, loss = 3.42124127\n",
      "Iteration 36, loss = 3.52521457\n",
      "Iteration 37, loss = 3.00467550\n",
      "Iteration 38, loss = 2.85285466\n",
      "Iteration 39, loss = 2.59298833\n",
      "Iteration 40, loss = 3.00853749\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 12.256566339669224\n",
      "Iteration 1, loss = 38.17144798\n",
      "Iteration 2, loss = 9.21382171\n",
      "Iteration 3, loss = 6.07101637\n",
      "Iteration 4, loss = 4.57600837\n",
      "Iteration 5, loss = 4.11430249\n",
      "Iteration 6, loss = 3.80529737\n",
      "Iteration 7, loss = 3.72763992\n",
      "Iteration 8, loss = 3.65986672\n",
      "Iteration 9, loss = 3.33723481\n",
      "Iteration 10, loss = 3.13075594\n",
      "Iteration 11, loss = 2.93619329\n",
      "Iteration 12, loss = 2.69132689\n",
      "Iteration 13, loss = 2.55780615\n",
      "Iteration 14, loss = 2.68451855\n",
      "Iteration 15, loss = 2.56895683\n",
      "Iteration 16, loss = 2.32897161\n",
      "Iteration 17, loss = 2.28065427\n",
      "Iteration 18, loss = 2.08145324\n",
      "Iteration 19, loss = 2.27481697\n",
      "Iteration 20, loss = 1.95534621\n",
      "Iteration 21, loss = 1.92856269\n",
      "Iteration 22, loss = 1.86022224\n",
      "Iteration 23, loss = 1.95510820\n",
      "Iteration 24, loss = 2.14971131\n",
      "Iteration 25, loss = 2.48464715\n",
      "Iteration 26, loss = 2.25011844\n",
      "Iteration 27, loss = 2.11427490\n",
      "Iteration 28, loss = 1.92098375\n",
      "Iteration 29, loss = 1.79600071\n",
      "Iteration 30, loss = 1.81712229\n",
      "Iteration 31, loss = 1.84289259\n",
      "Iteration 32, loss = 1.88126520\n",
      "Iteration 33, loss = 1.85995068\n",
      "Iteration 34, loss = 1.76302755\n",
      "Iteration 35, loss = 1.83802367\n",
      "Iteration 36, loss = 1.91619524\n",
      "Iteration 37, loss = 2.06549363\n",
      "Iteration 38, loss = 1.97976801\n",
      "Iteration 39, loss = 1.89054463\n",
      "Iteration 40, loss = 1.76640732\n",
      "Iteration 41, loss = 1.64361121\n",
      "Iteration 42, loss = 1.68132536\n",
      "Iteration 43, loss = 1.74426390\n",
      "Iteration 44, loss = 1.67818483\n",
      "Iteration 45, loss = 1.74560387\n",
      "Iteration 46, loss = 1.61683902\n",
      "Iteration 47, loss = 1.68166275\n",
      "Iteration 48, loss = 1.65772310\n",
      "Iteration 49, loss = 1.57131843\n",
      "Iteration 50, loss = 1.51869547\n",
      "Iteration 51, loss = 1.64555647\n",
      "Iteration 52, loss = 1.58440388\n",
      "Iteration 53, loss = 1.60431458\n",
      "Iteration 54, loss = 1.68945271\n",
      "Iteration 55, loss = 1.66874774\n",
      "Iteration 56, loss = 1.64538636\n",
      "Iteration 57, loss = 2.04539687\n",
      "Iteration 58, loss = 2.05001420\n",
      "Iteration 59, loss = 1.77307022\n",
      "Iteration 60, loss = 1.57910557\n",
      "Iteration 61, loss = 1.47367225\n",
      "Iteration 62, loss = 1.42278935\n",
      "Iteration 63, loss = 1.44724924\n",
      "Iteration 64, loss = 1.57514371\n",
      "Iteration 65, loss = 1.38329426\n",
      "Iteration 66, loss = 1.57571562\n",
      "Iteration 67, loss = 1.57505377\n",
      "Iteration 68, loss = 1.61421309\n",
      "Iteration 69, loss = 1.51914208\n",
      "Iteration 70, loss = 1.49030746\n",
      "Iteration 71, loss = 1.39309017\n",
      "Iteration 72, loss = 1.53190047\n",
      "Iteration 73, loss = 1.42939932\n",
      "Iteration 74, loss = 1.41316463\n",
      "Iteration 75, loss = 1.42875959\n",
      "Iteration 76, loss = 1.47629747\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 14.041389532088075\n",
      "Iteration 1, loss = 22.93340253\n",
      "Iteration 2, loss = 8.98100903\n",
      "Iteration 3, loss = 5.61628581\n",
      "Iteration 4, loss = 4.34476806\n",
      "Iteration 5, loss = 3.64431702\n",
      "Iteration 6, loss = 3.04771741\n",
      "Iteration 7, loss = 2.74353262\n",
      "Iteration 8, loss = 2.70284532\n",
      "Iteration 9, loss = 2.35674921\n",
      "Iteration 10, loss = 2.50212005\n",
      "Iteration 11, loss = 2.06585963\n",
      "Iteration 12, loss = 1.94940838\n",
      "Iteration 13, loss = 1.87509832\n",
      "Iteration 14, loss = 2.02671524\n",
      "Iteration 15, loss = 1.92784958\n",
      "Iteration 16, loss = 2.16136700\n",
      "Iteration 17, loss = 1.86183565\n",
      "Iteration 18, loss = 2.37627124\n",
      "Iteration 19, loss = 1.92167379\n",
      "Iteration 20, loss = 1.92505615\n",
      "Iteration 21, loss = 1.90803375\n",
      "Iteration 22, loss = 1.66783233\n",
      "Iteration 23, loss = 1.66374631\n",
      "Iteration 24, loss = 1.47828087\n",
      "Iteration 25, loss = 1.40255029\n",
      "Iteration 26, loss = 1.41483016\n",
      "Iteration 27, loss = 1.37651819\n",
      "Iteration 28, loss = 1.37675043\n",
      "Iteration 29, loss = 1.46094643\n",
      "Iteration 30, loss = 1.43700414\n",
      "Iteration 31, loss = 1.54830581\n",
      "Iteration 32, loss = 1.52058988\n",
      "Iteration 33, loss = 1.49018184\n",
      "Iteration 34, loss = 1.93370878\n",
      "Iteration 35, loss = 1.49032172\n",
      "Iteration 36, loss = 1.33382705\n",
      "Iteration 37, loss = 1.32976035\n",
      "Iteration 38, loss = 1.35124258\n",
      "Iteration 39, loss = 1.25862344\n",
      "Iteration 40, loss = 1.20755162\n",
      "Iteration 41, loss = 1.51413715\n",
      "Iteration 42, loss = 1.51004974\n",
      "Iteration 43, loss = 1.57874927\n",
      "Iteration 44, loss = 1.89233343\n",
      "Iteration 45, loss = 1.78630109\n",
      "Iteration 46, loss = 1.76523776\n",
      "Iteration 47, loss = 1.60865481\n",
      "Iteration 48, loss = 1.54064406\n",
      "Iteration 49, loss = 1.48726473\n",
      "Iteration 50, loss = 1.32835911\n",
      "Iteration 51, loss = 1.29696078\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 11.84972447711237\n",
      "Iteration 1, loss = 5.51964189\n",
      "Iteration 2, loss = 2.80255456\n",
      "Iteration 3, loss = 2.32821497\n",
      "Iteration 4, loss = 1.92346841\n",
      "Iteration 5, loss = 1.84939108\n",
      "Iteration 6, loss = 1.58063772\n",
      "Iteration 7, loss = 1.44815855\n",
      "Iteration 8, loss = 1.29374274\n",
      "Iteration 9, loss = 1.28186724\n",
      "Iteration 10, loss = 1.28089984\n",
      "Iteration 11, loss = 1.34828834\n",
      "Iteration 12, loss = 1.27323098\n",
      "Iteration 13, loss = 1.19323432\n",
      "Iteration 14, loss = 1.18425927\n",
      "Iteration 15, loss = 1.25751967\n",
      "Iteration 16, loss = 1.08203593\n",
      "Iteration 17, loss = 1.04049656\n",
      "Iteration 18, loss = 0.96483793\n",
      "Iteration 19, loss = 0.92757051\n",
      "Iteration 20, loss = 0.82727382\n",
      "Iteration 21, loss = 0.88975738\n",
      "Iteration 22, loss = 0.82524699\n",
      "Iteration 23, loss = 0.89422037\n",
      "Iteration 24, loss = 0.80723766\n",
      "Iteration 25, loss = 0.89402412\n",
      "Iteration 26, loss = 0.83473923\n",
      "Iteration 27, loss = 0.87424113\n",
      "Iteration 28, loss = 0.89885367\n",
      "Iteration 29, loss = 0.92079596\n",
      "Iteration 30, loss = 0.85707317\n",
      "Iteration 31, loss = 0.87218919\n",
      "Iteration 32, loss = 0.75040799\n",
      "Iteration 33, loss = 0.73115893\n",
      "Iteration 34, loss = 0.75141195\n",
      "Iteration 35, loss = 0.77265538\n",
      "Iteration 36, loss = 0.78121461\n",
      "Iteration 37, loss = 0.74335495\n",
      "Iteration 38, loss = 0.76169620\n",
      "Iteration 39, loss = 0.81946987\n",
      "Iteration 40, loss = 0.97553781\n",
      "Iteration 41, loss = 0.73283349\n",
      "Iteration 42, loss = 0.81258854\n",
      "Iteration 43, loss = 0.79035842\n",
      "Iteration 44, loss = 0.87806874\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 6.221553610565889\n",
      "Iteration 1, loss = 0.39428957\n",
      "Iteration 2, loss = 0.10379310\n",
      "Iteration 3, loss = 0.01615891\n",
      "Iteration 4, loss = 0.00199450\n",
      "Iteration 5, loss = 0.00055216\n",
      "Iteration 6, loss = 0.00037926\n",
      "Iteration 7, loss = 0.00018256\n",
      "Iteration 8, loss = 0.00005997\n",
      "Iteration 9, loss = 0.00005686\n",
      "Iteration 10, loss = 0.00004549\n",
      "Iteration 11, loss = 0.00003734\n",
      "Iteration 12, loss = 0.00003870\n",
      "Iteration 13, loss = 0.00003137\n",
      "Iteration 14, loss = 0.00003086\n",
      "Iteration 15, loss = 0.00002832\n",
      "Iteration 16, loss = 0.00002713\n",
      "Iteration 17, loss = 0.00002679\n",
      "Iteration 18, loss = 0.00002647\n",
      "Iteration 19, loss = 0.00002623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: -3.1934125154719877e-05\n",
      "Iteration 1, loss = 0.12103743\n",
      "Iteration 2, loss = 0.02760018\n",
      "Iteration 3, loss = 0.01932925\n",
      "Iteration 4, loss = 0.01497550\n",
      "Iteration 5, loss = 0.01412181\n",
      "Iteration 6, loss = 0.01305636\n",
      "Iteration 7, loss = 0.01296786\n",
      "Iteration 8, loss = 0.01286532\n",
      "Iteration 9, loss = 0.01287457\n",
      "Iteration 10, loss = 0.01281496\n",
      "Iteration 11, loss = 0.01281516\n",
      "Iteration 12, loss = 0.01279955\n",
      "Iteration 13, loss = 0.01275196\n",
      "Iteration 14, loss = 0.01396912\n",
      "Iteration 15, loss = 0.01342827\n",
      "Iteration 16, loss = 0.01400507\n",
      "Iteration 17, loss = 0.01348451\n",
      "Iteration 18, loss = 0.01292317\n",
      "Iteration 19, loss = 0.01277085\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.00797507673957049\n",
      "Iteration 1, loss = 3.33980957\n",
      "Iteration 2, loss = 1.96088552\n",
      "Iteration 3, loss = 1.80133337\n",
      "Iteration 4, loss = 1.52620798\n",
      "Iteration 5, loss = 1.48651625\n",
      "Iteration 6, loss = 1.19823236\n",
      "Iteration 7, loss = 1.03586015\n",
      "Iteration 8, loss = 0.93146867\n",
      "Iteration 9, loss = 0.87159282\n",
      "Iteration 10, loss = 0.83242599\n",
      "Iteration 11, loss = 0.88047755\n",
      "Iteration 12, loss = 0.79458025\n",
      "Iteration 13, loss = 1.00013865\n",
      "Iteration 14, loss = 0.87071533\n",
      "Iteration 15, loss = 0.83945619\n",
      "Iteration 16, loss = 0.81282795\n",
      "Iteration 17, loss = 0.71788040\n",
      "Iteration 18, loss = 0.72753295\n",
      "Iteration 19, loss = 0.74275656\n",
      "Iteration 20, loss = 0.78701497\n",
      "Iteration 21, loss = 0.85273568\n",
      "Iteration 22, loss = 0.78944615\n",
      "Iteration 23, loss = 0.79744979\n",
      "Iteration 24, loss = 0.76125483\n",
      "Iteration 25, loss = 0.72635452\n",
      "Iteration 26, loss = 0.66680937\n",
      "Iteration 27, loss = 0.72256634\n",
      "Iteration 28, loss = 0.75262947\n",
      "Iteration 29, loss = 0.70581627\n",
      "Iteration 30, loss = 0.62613221\n",
      "Iteration 31, loss = 0.81036775\n",
      "Iteration 32, loss = 0.84436044\n",
      "Iteration 33, loss = 0.76541025\n",
      "Iteration 34, loss = 0.73655761\n",
      "Iteration 35, loss = 0.73195181\n",
      "Iteration 36, loss = 0.72714341\n",
      "Iteration 37, loss = 0.66435540\n",
      "Iteration 38, loss = 0.69975487\n",
      "Iteration 39, loss = 0.64860359\n",
      "Iteration 40, loss = 0.67046604\n",
      "Iteration 41, loss = 0.73582614\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.03832669441076549\n",
      "Iteration 1, loss = 25.36026516\n",
      "Iteration 2, loss = 7.36338300\n",
      "Iteration 3, loss = 5.51435873\n",
      "Iteration 4, loss = 4.14638026\n",
      "Iteration 5, loss = 3.50265037\n",
      "Iteration 6, loss = 3.08727327\n",
      "Iteration 7, loss = 2.86799487\n",
      "Iteration 8, loss = 3.09797422\n",
      "Iteration 9, loss = 2.76400347\n",
      "Iteration 10, loss = 2.77192233\n",
      "Iteration 11, loss = 2.51991064\n",
      "Iteration 12, loss = 2.40202207\n",
      "Iteration 13, loss = 2.50458765\n",
      "Iteration 14, loss = 2.56703397\n",
      "Iteration 15, loss = 2.53166335\n",
      "Iteration 16, loss = 2.30382460\n",
      "Iteration 17, loss = 2.30718151\n",
      "Iteration 18, loss = 2.53096789\n",
      "Iteration 19, loss = 2.49326440\n",
      "Iteration 20, loss = 2.41253817\n",
      "Iteration 21, loss = 2.45765263\n",
      "Iteration 22, loss = 2.45926277\n",
      "Iteration 23, loss = 2.12223030\n",
      "Iteration 24, loss = 2.13852421\n",
      "Iteration 25, loss = 2.14112244\n",
      "Iteration 26, loss = 2.03345230\n",
      "Iteration 27, loss = 1.91837171\n",
      "Iteration 28, loss = 2.37764870\n",
      "Iteration 29, loss = 1.89221371\n",
      "Iteration 30, loss = 2.24534881\n",
      "Iteration 31, loss = 2.08068814\n",
      "Iteration 32, loss = 2.13557748\n",
      "Iteration 33, loss = 1.91985505\n",
      "Iteration 34, loss = 1.87263705\n",
      "Iteration 35, loss = 1.99011284\n",
      "Iteration 36, loss = 1.88603685\n",
      "Iteration 37, loss = 1.88172911\n",
      "Iteration 38, loss = 1.92190111\n",
      "Iteration 39, loss = 2.20033515\n",
      "Iteration 40, loss = 2.04891088\n",
      "Iteration 41, loss = 1.97448644\n",
      "Iteration 42, loss = 2.16676198\n",
      "Iteration 43, loss = 2.02311051\n",
      "Iteration 44, loss = 1.87782085\n",
      "Iteration 45, loss = 1.79708664\n",
      "Iteration 46, loss = 1.92116983\n",
      "Iteration 47, loss = 2.38225527\n",
      "Iteration 48, loss = 2.03729013\n",
      "Iteration 49, loss = 2.08775905\n",
      "Iteration 50, loss = 1.86419945\n",
      "Iteration 51, loss = 1.92928356\n",
      "Iteration 52, loss = 1.60379915\n",
      "Iteration 53, loss = 1.53734669\n",
      "Iteration 54, loss = 1.67461382\n",
      "Iteration 55, loss = 1.67175073\n",
      "Iteration 56, loss = 1.67149168\n",
      "Iteration 57, loss = 1.75215081\n",
      "Iteration 58, loss = 1.69980696\n",
      "Iteration 59, loss = 1.59334825\n",
      "Iteration 60, loss = 1.76968746\n",
      "Iteration 61, loss = 1.67813322\n",
      "Iteration 62, loss = 1.79048448\n",
      "Iteration 63, loss = 1.81871608\n",
      "Iteration 64, loss = 2.18629683\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 3.354109773817971\n",
      "Iteration 1, loss = 35.96210020\n",
      "Iteration 2, loss = 10.47410062\n",
      "Iteration 3, loss = 7.69847817\n",
      "Iteration 4, loss = 5.79671505\n",
      "Iteration 5, loss = 5.22893528\n",
      "Iteration 6, loss = 4.11045751\n",
      "Iteration 7, loss = 3.74510748\n",
      "Iteration 8, loss = 3.24249380\n",
      "Iteration 9, loss = 3.09719311\n",
      "Iteration 10, loss = 3.18063190\n",
      "Iteration 11, loss = 3.17866639\n",
      "Iteration 12, loss = 3.28737807\n",
      "Iteration 13, loss = 2.80533014\n",
      "Iteration 14, loss = 2.55565266\n",
      "Iteration 15, loss = 2.73535866\n",
      "Iteration 16, loss = 2.33829651\n",
      "Iteration 17, loss = 2.38515686\n",
      "Iteration 18, loss = 2.55468817\n",
      "Iteration 19, loss = 2.60394936\n",
      "Iteration 20, loss = 2.74279282\n",
      "Iteration 21, loss = 2.55116017\n",
      "Iteration 22, loss = 2.60045672\n",
      "Iteration 23, loss = 2.58263232\n",
      "Iteration 24, loss = 2.38270228\n",
      "Iteration 25, loss = 2.18770587\n",
      "Iteration 26, loss = 2.19263802\n",
      "Iteration 27, loss = 2.19017016\n",
      "Iteration 28, loss = 1.98473828\n",
      "Iteration 29, loss = 1.93318143\n",
      "Iteration 30, loss = 1.92977237\n",
      "Iteration 31, loss = 1.97517743\n",
      "Iteration 32, loss = 2.04352902\n",
      "Iteration 33, loss = 1.93697121\n",
      "Iteration 34, loss = 2.07414328\n",
      "Iteration 35, loss = 2.09676923\n",
      "Iteration 36, loss = 2.01657027\n",
      "Iteration 37, loss = 1.91831825\n",
      "Iteration 38, loss = 1.92332739\n",
      "Iteration 39, loss = 1.92660237\n",
      "Iteration 40, loss = 1.94726371\n",
      "Iteration 41, loss = 1.80856224\n",
      "Iteration 42, loss = 1.99783297\n",
      "Iteration 43, loss = 2.17326693\n",
      "Iteration 44, loss = 2.08070549\n",
      "Iteration 45, loss = 1.98259098\n",
      "Iteration 46, loss = 1.91012274\n",
      "Iteration 47, loss = 1.95125049\n",
      "Iteration 48, loss = 1.77178041\n",
      "Iteration 49, loss = 1.88705503\n",
      "Iteration 50, loss = 1.87552162\n",
      "Iteration 51, loss = 1.82755682\n",
      "Iteration 52, loss = 1.77519905\n",
      "Iteration 53, loss = 2.06333850\n",
      "Iteration 54, loss = 1.89956108\n",
      "Iteration 55, loss = 1.86603398\n",
      "Iteration 56, loss = 1.76613073\n",
      "Iteration 57, loss = 1.65620606\n",
      "Iteration 58, loss = 1.94624122\n",
      "Iteration 59, loss = 2.14461095\n",
      "Iteration 60, loss = 1.91312792\n",
      "Iteration 61, loss = 1.73420874\n",
      "Iteration 62, loss = 1.85810943\n",
      "Iteration 63, loss = 1.69504408\n",
      "Iteration 64, loss = 1.70093332\n",
      "Iteration 65, loss = 1.68069977\n",
      "Iteration 66, loss = 1.74746600\n",
      "Iteration 67, loss = 1.73458134\n",
      "Iteration 68, loss = 1.74851838\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 14.879404930271853\n",
      "Iteration 1, loss = 18.52147132\n",
      "Iteration 2, loss = 7.81829125\n",
      "Iteration 3, loss = 5.59829092\n",
      "Iteration 4, loss = 4.21029578\n",
      "Iteration 5, loss = 3.66882232\n",
      "Iteration 6, loss = 3.16512685\n",
      "Iteration 7, loss = 2.91726553\n",
      "Iteration 8, loss = 2.44869388\n",
      "Iteration 9, loss = 2.33655954\n",
      "Iteration 10, loss = 2.24744667\n",
      "Iteration 11, loss = 2.21535760\n",
      "Iteration 12, loss = 2.07102922\n",
      "Iteration 13, loss = 2.07764824\n",
      "Iteration 14, loss = 2.07586165\n",
      "Iteration 15, loss = 2.02921472\n",
      "Iteration 16, loss = 1.86649281\n",
      "Iteration 17, loss = 1.97241516\n",
      "Iteration 18, loss = 2.03479952\n",
      "Iteration 19, loss = 2.07897775\n",
      "Iteration 20, loss = 1.94371896\n",
      "Iteration 21, loss = 1.93785843\n",
      "Iteration 22, loss = 1.98556722\n",
      "Iteration 23, loss = 1.71893006\n",
      "Iteration 24, loss = 1.68084117\n",
      "Iteration 25, loss = 1.61540429\n",
      "Iteration 26, loss = 1.80073591\n",
      "Iteration 27, loss = 1.61281806\n",
      "Iteration 28, loss = 1.82281986\n",
      "Iteration 29, loss = 1.71387283\n",
      "Iteration 30, loss = 1.77627840\n",
      "Iteration 31, loss = 1.54480578\n",
      "Iteration 32, loss = 1.60101747\n",
      "Iteration 33, loss = 1.88591934\n",
      "Iteration 34, loss = 2.13256007\n",
      "Iteration 35, loss = 2.01599244\n",
      "Iteration 36, loss = 1.95839792\n",
      "Iteration 37, loss = 1.81928011\n",
      "Iteration 38, loss = 2.12527026\n",
      "Iteration 39, loss = 1.81537358\n",
      "Iteration 40, loss = 1.62385131\n",
      "Iteration 41, loss = 1.64981673\n",
      "Iteration 42, loss = 1.65956219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 5.17692757147082\n",
      "Iteration 1, loss = 22.75013709\n",
      "Iteration 2, loss = 9.16290854\n",
      "Iteration 3, loss = 6.58268638\n",
      "Iteration 4, loss = 5.01755665\n",
      "Iteration 5, loss = 4.41016008\n",
      "Iteration 6, loss = 3.64504034\n",
      "Iteration 7, loss = 3.28464034\n",
      "Iteration 8, loss = 3.02607156\n",
      "Iteration 9, loss = 2.76420098\n",
      "Iteration 10, loss = 2.73371181\n",
      "Iteration 11, loss = 2.43055711\n",
      "Iteration 12, loss = 2.30563645\n",
      "Iteration 13, loss = 2.40744760\n",
      "Iteration 14, loss = 2.04444559\n",
      "Iteration 15, loss = 2.12952489\n",
      "Iteration 16, loss = 1.97806537\n",
      "Iteration 17, loss = 2.01300286\n",
      "Iteration 18, loss = 2.17493029\n",
      "Iteration 19, loss = 1.95504777\n",
      "Iteration 20, loss = 1.82633499\n",
      "Iteration 21, loss = 2.02030100\n",
      "Iteration 22, loss = 2.17280327\n",
      "Iteration 23, loss = 1.94573712\n",
      "Iteration 24, loss = 1.92377852\n",
      "Iteration 25, loss = 1.89148302\n",
      "Iteration 26, loss = 1.73762839\n",
      "Iteration 27, loss = 1.76311041\n",
      "Iteration 28, loss = 1.91160820\n",
      "Iteration 29, loss = 1.95281263\n",
      "Iteration 30, loss = 1.70323198\n",
      "Iteration 31, loss = 1.53135631\n",
      "Iteration 32, loss = 1.56474417\n",
      "Iteration 33, loss = 1.56613450\n",
      "Iteration 34, loss = 1.58652041\n",
      "Iteration 35, loss = 1.51105504\n",
      "Iteration 36, loss = 1.61929398\n",
      "Iteration 37, loss = 1.61837647\n",
      "Iteration 38, loss = 2.29303898\n",
      "Iteration 39, loss = 2.10981439\n",
      "Iteration 40, loss = 2.02729409\n",
      "Iteration 41, loss = 1.99421032\n",
      "Iteration 42, loss = 1.81090631\n",
      "Iteration 43, loss = 1.83014123\n",
      "Iteration 44, loss = 1.50326211\n",
      "Iteration 45, loss = 1.45235439\n",
      "Iteration 46, loss = 1.58589703\n",
      "Iteration 47, loss = 1.56983088\n",
      "Iteration 48, loss = 1.34814897\n",
      "Iteration 49, loss = 1.26924717\n",
      "Iteration 50, loss = 1.24807364\n",
      "Iteration 51, loss = 1.44923153\n",
      "Iteration 52, loss = 1.57206122\n",
      "Iteration 53, loss = 1.37662567\n",
      "Iteration 54, loss = 1.38606924\n",
      "Iteration 55, loss = 1.35842090\n",
      "Iteration 56, loss = 1.59031431\n",
      "Iteration 57, loss = 1.36796504\n",
      "Iteration 58, loss = 1.25854409\n",
      "Iteration 59, loss = 1.33933350\n",
      "Iteration 60, loss = 1.43768118\n",
      "Iteration 61, loss = 1.28447818\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.36972533108422295\n",
      "Iteration 1, loss = 7.97598078\n",
      "Iteration 2, loss = 2.64860864\n",
      "Iteration 3, loss = 2.02077027\n",
      "Iteration 4, loss = 1.68906241\n",
      "Iteration 5, loss = 1.50935578\n",
      "Iteration 6, loss = 1.36290796\n",
      "Iteration 7, loss = 1.16569115\n",
      "Iteration 8, loss = 1.13080181\n",
      "Iteration 9, loss = 1.02397618\n",
      "Iteration 10, loss = 1.03567202\n",
      "Iteration 11, loss = 1.03317208\n",
      "Iteration 12, loss = 1.02650979\n",
      "Iteration 13, loss = 0.94746459\n",
      "Iteration 14, loss = 0.90746624\n",
      "Iteration 15, loss = 0.90148714\n",
      "Iteration 16, loss = 0.87093435\n",
      "Iteration 17, loss = 0.86625644\n",
      "Iteration 18, loss = 0.93594432\n",
      "Iteration 19, loss = 0.87006351\n",
      "Iteration 20, loss = 0.81971026\n",
      "Iteration 21, loss = 0.81322964\n",
      "Iteration 22, loss = 0.84146689\n",
      "Iteration 23, loss = 0.73525163\n",
      "Iteration 24, loss = 1.14280171\n",
      "Iteration 25, loss = 0.81018397\n",
      "Iteration 26, loss = 0.78909456\n",
      "Iteration 27, loss = 0.78400961\n",
      "Iteration 28, loss = 0.73824292\n",
      "Iteration 29, loss = 0.76370599\n",
      "Iteration 30, loss = 0.75846535\n",
      "Iteration 31, loss = 0.73423335\n",
      "Iteration 32, loss = 0.73032427\n",
      "Iteration 33, loss = 0.75406633\n",
      "Iteration 34, loss = 0.71680171\n",
      "Iteration 35, loss = 0.77252030\n",
      "Iteration 36, loss = 0.92589607\n",
      "Iteration 37, loss = 1.35014570\n",
      "Iteration 38, loss = 0.91424324\n",
      "Iteration 39, loss = 1.17541534\n",
      "Iteration 40, loss = 1.49245966\n",
      "Iteration 41, loss = 1.32204719\n",
      "Iteration 42, loss = 0.88391273\n",
      "Iteration 43, loss = 0.83664992\n",
      "Iteration 44, loss = 0.69704436\n",
      "Iteration 45, loss = 0.82507952\n",
      "Iteration 46, loss = 0.73987032\n",
      "Iteration 47, loss = 0.70011287\n",
      "Iteration 48, loss = 0.77793046\n",
      "Iteration 49, loss = 1.04457643\n",
      "Iteration 50, loss = 0.74880629\n",
      "Iteration 51, loss = 0.68689079\n",
      "Iteration 52, loss = 0.65180159\n",
      "Iteration 53, loss = 0.67678056\n",
      "Iteration 54, loss = 0.63895232\n",
      "Iteration 55, loss = 0.64837103\n",
      "Iteration 56, loss = 0.83097333\n",
      "Iteration 57, loss = 0.70925306\n",
      "Iteration 58, loss = 0.73487410\n",
      "Iteration 59, loss = 0.67354455\n",
      "Iteration 60, loss = 0.64070889\n",
      "Iteration 61, loss = 0.58738335\n",
      "Iteration 62, loss = 0.57306645\n",
      "Iteration 63, loss = 0.61519468\n",
      "Iteration 64, loss = 0.75842327\n",
      "Iteration 65, loss = 0.83185040\n",
      "Iteration 66, loss = 0.90082345\n",
      "Iteration 67, loss = 0.67656668\n",
      "Iteration 68, loss = 0.64475369\n",
      "Iteration 69, loss = 0.59131270\n",
      "Iteration 70, loss = 0.77320680\n",
      "Iteration 71, loss = 0.65105968\n",
      "Iteration 72, loss = 0.62138669\n",
      "Iteration 73, loss = 0.59074913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.05740934597582431\n",
      "Iteration 1, loss = 0.13438010\n",
      "Iteration 2, loss = 0.04608053\n",
      "Iteration 3, loss = 0.04352395\n",
      "Iteration 4, loss = 0.04130206\n",
      "Iteration 5, loss = 0.03318406\n",
      "Iteration 6, loss = 0.03387333\n",
      "Iteration 7, loss = 0.02697662\n",
      "Iteration 8, loss = 0.02356365\n",
      "Iteration 9, loss = 0.01467886\n",
      "Iteration 10, loss = 0.00895763\n",
      "Iteration 11, loss = 0.00972701\n",
      "Iteration 12, loss = 0.01337907\n",
      "Iteration 13, loss = 0.01556003\n",
      "Iteration 14, loss = 0.01548611\n",
      "Iteration 15, loss = 0.01355033\n",
      "Iteration 16, loss = 0.01953196\n",
      "Iteration 17, loss = 0.01224986\n",
      "Iteration 18, loss = 0.01081407\n",
      "Iteration 19, loss = 0.00916815\n",
      "Iteration 20, loss = 0.00810637\n",
      "Iteration 21, loss = 0.00784972\n",
      "Iteration 22, loss = 0.00773764\n",
      "Iteration 23, loss = 0.00777098\n",
      "Iteration 24, loss = 0.00815714\n",
      "Iteration 25, loss = 0.01455157\n",
      "Iteration 26, loss = 0.00937240\n",
      "Iteration 27, loss = 0.01039836\n",
      "Iteration 28, loss = 0.01006591\n",
      "Iteration 29, loss = 0.00827836\n",
      "Iteration 30, loss = 0.00780244\n",
      "Iteration 31, loss = 0.00750870\n",
      "Iteration 32, loss = 0.00730959\n",
      "Iteration 33, loss = 0.00757254\n",
      "Iteration 34, loss = 0.00783042\n",
      "Iteration 35, loss = 0.00773080\n",
      "Iteration 36, loss = 0.01140628\n",
      "Iteration 37, loss = 0.00990976\n",
      "Iteration 38, loss = 0.00850050\n",
      "Iteration 39, loss = 0.00911519\n",
      "Iteration 40, loss = 0.01777993\n",
      "Iteration 41, loss = 0.01563751\n",
      "Iteration 42, loss = 0.01138480\n",
      "Iteration 43, loss = 0.02151285\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.013181420950679182\n",
      "Iteration 1, loss = 0.23773466\n",
      "Iteration 2, loss = 0.05680074\n",
      "Iteration 3, loss = 0.02925462\n",
      "Iteration 4, loss = 0.02577177\n",
      "Iteration 5, loss = 0.02557827\n",
      "Iteration 6, loss = 0.02542631\n",
      "Iteration 7, loss = 0.02443860\n",
      "Iteration 8, loss = 0.02703397\n",
      "Iteration 9, loss = 0.02473791\n",
      "Iteration 10, loss = 0.02435542\n",
      "Iteration 11, loss = 0.02391358\n",
      "Iteration 12, loss = 0.02358308\n",
      "Iteration 13, loss = 0.02343998\n",
      "Iteration 14, loss = 0.02345974\n",
      "Iteration 15, loss = 0.02345413\n",
      "Iteration 16, loss = 0.02335717\n",
      "Iteration 17, loss = 0.02338042\n",
      "Iteration 18, loss = 0.02343391\n",
      "Iteration 19, loss = 0.02336708\n",
      "Iteration 20, loss = 0.02341085\n",
      "Iteration 21, loss = 0.02338066\n",
      "Iteration 22, loss = 0.02338612\n",
      "Iteration 23, loss = 0.02337207\n",
      "Iteration 24, loss = 0.02339986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.008740265927349451\n",
      "Iteration 1, loss = 2.01064564\n",
      "Iteration 2, loss = 1.31666550\n",
      "Iteration 3, loss = 0.96508250\n",
      "Iteration 4, loss = 0.92713558\n",
      "Iteration 5, loss = 0.81744860\n",
      "Iteration 6, loss = 1.03329743\n",
      "Iteration 7, loss = 0.74624292\n",
      "Iteration 8, loss = 0.74637801\n",
      "Iteration 9, loss = 0.71392906\n",
      "Iteration 10, loss = 0.65984119\n",
      "Iteration 11, loss = 0.68686949\n",
      "Iteration 12, loss = 0.87454442\n",
      "Iteration 13, loss = 0.90964057\n",
      "Iteration 14, loss = 0.72370187\n",
      "Iteration 15, loss = 0.67174964\n",
      "Iteration 16, loss = 0.55289797\n",
      "Iteration 17, loss = 0.56431259\n",
      "Iteration 18, loss = 0.52416881\n",
      "Iteration 19, loss = 0.49353087\n",
      "Iteration 20, loss = 0.50638510\n",
      "Iteration 21, loss = 0.48253378\n",
      "Iteration 22, loss = 0.47292562\n",
      "Iteration 23, loss = 0.44370662\n",
      "Iteration 24, loss = 0.47697172\n",
      "Iteration 25, loss = 0.43806978\n",
      "Iteration 26, loss = 0.42790883\n",
      "Iteration 27, loss = 0.44132078\n",
      "Iteration 28, loss = 0.47184418\n",
      "Iteration 29, loss = 0.45029469\n",
      "Iteration 30, loss = 0.42514588\n",
      "Iteration 31, loss = 0.43806832\n",
      "Iteration 32, loss = 0.41168663\n",
      "Iteration 33, loss = 0.38614186\n",
      "Iteration 34, loss = 0.37405078\n",
      "Iteration 35, loss = 0.37924014\n",
      "Iteration 36, loss = 0.52935677\n",
      "Iteration 37, loss = 0.48372781\n",
      "Iteration 38, loss = 0.51186344\n",
      "Iteration 39, loss = 0.39611035\n",
      "Iteration 40, loss = 0.38052141\n",
      "Iteration 41, loss = 0.36322672\n",
      "Iteration 42, loss = 0.33752174\n",
      "Iteration 43, loss = 0.37080643\n",
      "Iteration 44, loss = 0.32876182\n",
      "Iteration 45, loss = 0.29701672\n",
      "Iteration 46, loss = 0.33376611\n",
      "Iteration 47, loss = 0.41210256\n",
      "Iteration 48, loss = 0.33544993\n",
      "Iteration 49, loss = 0.31438251\n",
      "Iteration 50, loss = 0.30934602\n",
      "Iteration 51, loss = 0.34288508\n",
      "Iteration 52, loss = 0.61737202\n",
      "Iteration 53, loss = 0.64107504\n",
      "Iteration 54, loss = 0.57925220\n",
      "Iteration 55, loss = 0.49910495\n",
      "Iteration 56, loss = 0.46061385\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.10619852128259816\n",
      "Iteration 1, loss = 21.09978545\n",
      "Iteration 2, loss = 8.15282421\n",
      "Iteration 3, loss = 6.25664472\n",
      "Iteration 4, loss = 5.39959082\n",
      "Iteration 5, loss = 4.93345707\n",
      "Iteration 6, loss = 4.45125158\n",
      "Iteration 7, loss = 4.36051948\n",
      "Iteration 8, loss = 4.06779519\n",
      "Iteration 9, loss = 3.75343064\n",
      "Iteration 10, loss = 3.93562645\n",
      "Iteration 11, loss = 3.92581777\n",
      "Iteration 12, loss = 3.91609155\n",
      "Iteration 13, loss = 3.40590022\n",
      "Iteration 14, loss = 3.28870432\n",
      "Iteration 15, loss = 3.01699137\n",
      "Iteration 16, loss = 3.04235960\n",
      "Iteration 17, loss = 3.13526367\n",
      "Iteration 18, loss = 2.85602941\n",
      "Iteration 19, loss = 2.87662675\n",
      "Iteration 20, loss = 2.79422569\n",
      "Iteration 21, loss = 2.71922317\n",
      "Iteration 22, loss = 2.78765936\n",
      "Iteration 23, loss = 2.75941103\n",
      "Iteration 24, loss = 2.86389076\n",
      "Iteration 25, loss = 2.62040918\n",
      "Iteration 26, loss = 2.53213375\n",
      "Iteration 27, loss = 2.58876791\n",
      "Iteration 28, loss = 2.72349774\n",
      "Iteration 29, loss = 2.62736937\n",
      "Iteration 30, loss = 2.60951250\n",
      "Iteration 31, loss = 2.85897866\n",
      "Iteration 32, loss = 2.55195221\n",
      "Iteration 33, loss = 2.53027172\n",
      "Iteration 34, loss = 2.53466452\n",
      "Iteration 35, loss = 2.65477321\n",
      "Iteration 36, loss = 2.59676428\n",
      "Iteration 37, loss = 2.50576624\n",
      "Iteration 38, loss = 2.58667845\n",
      "Iteration 39, loss = 2.54403484\n",
      "Iteration 40, loss = 2.41687845\n",
      "Iteration 41, loss = 2.45995933\n",
      "Iteration 42, loss = 2.47360235\n",
      "Iteration 43, loss = 2.34640731\n",
      "Iteration 44, loss = 2.49857826\n",
      "Iteration 45, loss = 2.44461535\n",
      "Iteration 46, loss = 2.41206557\n",
      "Iteration 47, loss = 2.41115426\n",
      "Iteration 48, loss = 2.33770459\n",
      "Iteration 49, loss = 2.35246930\n",
      "Iteration 50, loss = 2.34760571\n",
      "Iteration 51, loss = 2.30415045\n",
      "Iteration 52, loss = 2.54558638\n",
      "Iteration 53, loss = 2.86250157\n",
      "Iteration 54, loss = 2.74974359\n",
      "Iteration 55, loss = 2.34933783\n",
      "Iteration 56, loss = 2.26904421\n",
      "Iteration 57, loss = 2.42839597\n",
      "Iteration 58, loss = 2.57501243\n",
      "Iteration 59, loss = 3.05268437\n",
      "Iteration 60, loss = 2.58409732\n",
      "Iteration 61, loss = 2.72192367\n",
      "Iteration 62, loss = 2.93762958\n",
      "Iteration 63, loss = 2.41567953\n",
      "Iteration 64, loss = 2.34448810\n",
      "Iteration 65, loss = 2.33681021\n",
      "Iteration 66, loss = 2.57613723\n",
      "Iteration 67, loss = 2.11527493\n",
      "Iteration 68, loss = 2.09512219\n",
      "Iteration 69, loss = 2.24361704\n",
      "Iteration 70, loss = 2.33649131\n",
      "Iteration 71, loss = 2.31906712\n",
      "Iteration 72, loss = 2.15700639\n",
      "Iteration 73, loss = 2.10515510\n",
      "Iteration 74, loss = 2.07256417\n",
      "Iteration 75, loss = 2.13898239\n",
      "Iteration 76, loss = 2.08998599\n",
      "Iteration 77, loss = 2.13776272\n",
      "Iteration 78, loss = 2.20731172\n",
      "Iteration 79, loss = 2.32802376\n",
      "Iteration 80, loss = 2.24278687\n",
      "Iteration 81, loss = 2.13585802\n",
      "Iteration 82, loss = 2.37518520\n",
      "Iteration 83, loss = 2.41742151\n",
      "Iteration 84, loss = 2.27166006\n",
      "Iteration 85, loss = 2.18706488\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.5404846601234596\n",
      "Iteration 1, loss = 31.92358651\n",
      "Iteration 2, loss = 9.25150720\n",
      "Iteration 3, loss = 6.47059357\n",
      "Iteration 4, loss = 5.30188270\n",
      "Iteration 5, loss = 4.82686438\n",
      "Iteration 6, loss = 4.15021892\n",
      "Iteration 7, loss = 3.84815126\n",
      "Iteration 8, loss = 3.58314246\n",
      "Iteration 9, loss = 3.43143235\n",
      "Iteration 10, loss = 3.15735167\n",
      "Iteration 11, loss = 3.15521106\n",
      "Iteration 12, loss = 3.04322545\n",
      "Iteration 13, loss = 2.82789352\n",
      "Iteration 14, loss = 2.76594417\n",
      "Iteration 15, loss = 2.70990394\n",
      "Iteration 16, loss = 2.79163067\n",
      "Iteration 17, loss = 3.08230373\n",
      "Iteration 18, loss = 2.82419058\n",
      "Iteration 19, loss = 2.60092223\n",
      "Iteration 20, loss = 2.55480928\n",
      "Iteration 21, loss = 2.48791420\n",
      "Iteration 22, loss = 2.40258231\n",
      "Iteration 23, loss = 2.59034894\n",
      "Iteration 24, loss = 2.73007208\n",
      "Iteration 25, loss = 2.76895930\n",
      "Iteration 26, loss = 2.64756046\n",
      "Iteration 27, loss = 2.46204370\n",
      "Iteration 28, loss = 2.34216013\n",
      "Iteration 29, loss = 2.29081359\n",
      "Iteration 30, loss = 2.34299579\n",
      "Iteration 31, loss = 2.33434864\n",
      "Iteration 32, loss = 2.39867634\n",
      "Iteration 33, loss = 2.38497876\n",
      "Iteration 34, loss = 2.59175165\n",
      "Iteration 35, loss = 2.77650405\n",
      "Iteration 36, loss = 2.54910729\n",
      "Iteration 37, loss = 2.56088688\n",
      "Iteration 38, loss = 2.31585612\n",
      "Iteration 39, loss = 2.51084677\n",
      "Iteration 40, loss = 2.35101749\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 16.300157265971052\n",
      "Iteration 1, loss = 26.44694335\n",
      "Iteration 2, loss = 9.95364872\n",
      "Iteration 3, loss = 6.68285038\n",
      "Iteration 4, loss = 5.43434359\n",
      "Iteration 5, loss = 4.59524207\n",
      "Iteration 6, loss = 3.73380711\n",
      "Iteration 7, loss = 3.50881753\n",
      "Iteration 8, loss = 3.47094266\n",
      "Iteration 9, loss = 3.04584936\n",
      "Iteration 10, loss = 3.06176138\n",
      "Iteration 11, loss = 3.08924507\n",
      "Iteration 12, loss = 2.81978192\n",
      "Iteration 13, loss = 2.71684372\n",
      "Iteration 14, loss = 2.59517418\n",
      "Iteration 15, loss = 2.53155816\n",
      "Iteration 16, loss = 2.54884247\n",
      "Iteration 17, loss = 2.39649892\n",
      "Iteration 18, loss = 2.34846801\n",
      "Iteration 19, loss = 2.48471413\n",
      "Iteration 20, loss = 2.33170572\n",
      "Iteration 21, loss = 2.19716259\n",
      "Iteration 22, loss = 2.37527842\n",
      "Iteration 23, loss = 2.53488251\n",
      "Iteration 24, loss = 2.45694667\n",
      "Iteration 25, loss = 2.35708065\n",
      "Iteration 26, loss = 2.29040748\n",
      "Iteration 27, loss = 2.14114117\n",
      "Iteration 28, loss = 2.22545037\n",
      "Iteration 29, loss = 2.22869313\n",
      "Iteration 30, loss = 2.18538597\n",
      "Iteration 31, loss = 2.58548572\n",
      "Iteration 32, loss = 2.44152254\n",
      "Iteration 33, loss = 2.06549669\n",
      "Iteration 34, loss = 2.13345542\n",
      "Iteration 35, loss = 2.10449576\n",
      "Iteration 36, loss = 2.01197444\n",
      "Iteration 37, loss = 2.12477798\n",
      "Iteration 38, loss = 2.18129053\n",
      "Iteration 39, loss = 2.46257153\n",
      "Iteration 40, loss = 2.27192236\n",
      "Iteration 41, loss = 2.17211931\n",
      "Iteration 42, loss = 2.26586784\n",
      "Iteration 43, loss = 1.98712962\n",
      "Iteration 44, loss = 1.95754228\n",
      "Iteration 45, loss = 2.13000054\n",
      "Iteration 46, loss = 1.83018549\n",
      "Iteration 47, loss = 1.74988784\n",
      "Iteration 48, loss = 1.64805976\n",
      "Iteration 49, loss = 1.76012225\n",
      "Iteration 50, loss = 1.73337932\n",
      "Iteration 51, loss = 1.72478434\n",
      "Iteration 52, loss = 1.73945905\n",
      "Iteration 53, loss = 1.81938185\n",
      "Iteration 54, loss = 1.68005909\n",
      "Iteration 55, loss = 1.90155088\n",
      "Iteration 56, loss = 1.78656785\n",
      "Iteration 57, loss = 1.69318240\n",
      "Iteration 58, loss = 1.62572536\n",
      "Iteration 59, loss = 1.70293836\n",
      "Iteration 60, loss = 1.66066849\n",
      "Iteration 61, loss = 1.61659445\n",
      "Iteration 62, loss = 1.92805599\n",
      "Iteration 63, loss = 2.21600703\n",
      "Iteration 64, loss = 2.07776641\n",
      "Iteration 65, loss = 1.81692294\n",
      "Iteration 66, loss = 1.72375655\n",
      "Iteration 67, loss = 1.77935185\n",
      "Iteration 68, loss = 1.80584746\n",
      "Iteration 69, loss = 2.02193701\n",
      "Iteration 70, loss = 2.02148812\n",
      "Iteration 71, loss = 1.99118775\n",
      "Iteration 72, loss = 2.12007945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 13.943940460558721\n",
      "Iteration 1, loss = 25.11100867\n",
      "Iteration 2, loss = 7.78396161\n",
      "Iteration 3, loss = 5.44014848\n",
      "Iteration 4, loss = 4.51342579\n",
      "Iteration 5, loss = 3.73574164\n",
      "Iteration 6, loss = 3.40982490\n",
      "Iteration 7, loss = 3.09385569\n",
      "Iteration 8, loss = 2.91990916\n",
      "Iteration 9, loss = 2.71000988\n",
      "Iteration 10, loss = 2.71352528\n",
      "Iteration 11, loss = 2.71811302\n",
      "Iteration 12, loss = 2.67736197\n",
      "Iteration 13, loss = 2.36173546\n",
      "Iteration 14, loss = 2.38340743\n",
      "Iteration 15, loss = 2.30665959\n",
      "Iteration 16, loss = 2.11985766\n",
      "Iteration 17, loss = 2.19811756\n",
      "Iteration 18, loss = 2.55285148\n",
      "Iteration 19, loss = 2.38782818\n",
      "Iteration 20, loss = 2.43781046\n",
      "Iteration 21, loss = 2.16877195\n",
      "Iteration 22, loss = 1.96402426\n",
      "Iteration 23, loss = 1.98311987\n",
      "Iteration 24, loss = 2.12020132\n",
      "Iteration 25, loss = 2.62332886\n",
      "Iteration 26, loss = 3.08688095\n",
      "Iteration 27, loss = 2.18786016\n",
      "Iteration 28, loss = 2.04547468\n",
      "Iteration 29, loss = 2.10224338\n",
      "Iteration 30, loss = 1.98543122\n",
      "Iteration 31, loss = 1.84718167\n",
      "Iteration 32, loss = 1.80671242\n",
      "Iteration 33, loss = 1.77312306\n",
      "Iteration 34, loss = 1.86578743\n",
      "Iteration 35, loss = 1.77770514\n",
      "Iteration 36, loss = 1.66667737\n",
      "Iteration 37, loss = 1.93260566\n",
      "Iteration 38, loss = 1.75094783\n",
      "Iteration 39, loss = 1.62538113\n",
      "Iteration 40, loss = 1.62075520\n",
      "Iteration 41, loss = 1.58459825\n",
      "Iteration 42, loss = 1.74581902\n",
      "Iteration 43, loss = 1.67951410\n",
      "Iteration 44, loss = 1.56364192\n",
      "Iteration 45, loss = 1.64609841\n",
      "Iteration 46, loss = 1.67065134\n",
      "Iteration 47, loss = 1.72933348\n",
      "Iteration 48, loss = 1.71909549\n",
      "Iteration 49, loss = 1.72021628\n",
      "Iteration 50, loss = 1.74399290\n",
      "Iteration 51, loss = 1.72815438\n",
      "Iteration 52, loss = 1.60232915\n",
      "Iteration 53, loss = 1.66313261\n",
      "Iteration 54, loss = 1.62571324\n",
      "Iteration 55, loss = 1.55962568\n",
      "Iteration 56, loss = 1.59138806\n",
      "Iteration 57, loss = 1.59543997\n",
      "Iteration 58, loss = 1.49955246\n",
      "Iteration 59, loss = 1.51543606\n",
      "Iteration 60, loss = 1.48064173\n",
      "Iteration 61, loss = 1.58476755\n",
      "Iteration 62, loss = 1.68293311\n",
      "Iteration 63, loss = 1.52774583\n",
      "Iteration 64, loss = 1.48767897\n",
      "Iteration 65, loss = 1.49085025\n",
      "Iteration 66, loss = 1.42246490\n",
      "Iteration 67, loss = 1.43366250\n",
      "Iteration 68, loss = 1.36456347\n",
      "Iteration 69, loss = 1.40848161\n",
      "Iteration 70, loss = 1.55902049\n",
      "Iteration 71, loss = 1.61120690\n",
      "Iteration 72, loss = 1.53655365\n",
      "Iteration 73, loss = 1.87617861\n",
      "Iteration 74, loss = 1.85565347\n",
      "Iteration 75, loss = 1.70461736\n",
      "Iteration 76, loss = 1.50113924\n",
      "Iteration 77, loss = 1.50289290\n",
      "Iteration 78, loss = 1.42999699\n",
      "Iteration 79, loss = 1.44299811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 9.467320622856292\n",
      "Iteration 1, loss = 8.46625138\n",
      "Iteration 2, loss = 3.86297960\n",
      "Iteration 3, loss = 3.13149090\n",
      "Iteration 4, loss = 2.26574825\n",
      "Iteration 5, loss = 2.10782612\n",
      "Iteration 6, loss = 2.28234713\n",
      "Iteration 7, loss = 2.14919472\n",
      "Iteration 8, loss = 1.86233348\n",
      "Iteration 9, loss = 1.70318965\n",
      "Iteration 10, loss = 1.72757358\n",
      "Iteration 11, loss = 1.59229133\n",
      "Iteration 12, loss = 1.49096904\n",
      "Iteration 13, loss = 1.51924251\n",
      "Iteration 14, loss = 1.46291951\n",
      "Iteration 15, loss = 1.50374060\n",
      "Iteration 16, loss = 1.68435617\n",
      "Iteration 17, loss = 1.36265215\n",
      "Iteration 18, loss = 1.37528847\n",
      "Iteration 19, loss = 1.34873529\n",
      "Iteration 20, loss = 1.21546093\n",
      "Iteration 21, loss = 1.20791273\n",
      "Iteration 22, loss = 1.20927121\n",
      "Iteration 23, loss = 1.35074748\n",
      "Iteration 24, loss = 1.22680905\n",
      "Iteration 25, loss = 1.23809189\n",
      "Iteration 26, loss = 1.13033965\n",
      "Iteration 27, loss = 1.15290388\n",
      "Iteration 28, loss = 1.13204033\n",
      "Iteration 29, loss = 1.34405320\n",
      "Iteration 30, loss = 1.21266301\n",
      "Iteration 31, loss = 1.14906037\n",
      "Iteration 32, loss = 1.09147643\n",
      "Iteration 33, loss = 0.99708333\n",
      "Iteration 34, loss = 1.01226330\n",
      "Iteration 35, loss = 1.05020980\n",
      "Iteration 36, loss = 1.23684748\n",
      "Iteration 37, loss = 1.26931052\n",
      "Iteration 38, loss = 1.15409787\n",
      "Iteration 39, loss = 1.06349309\n",
      "Iteration 40, loss = 1.18425373\n",
      "Iteration 41, loss = 1.52839967\n",
      "Iteration 42, loss = 1.31002469\n",
      "Iteration 43, loss = 1.14409938\n",
      "Iteration 44, loss = 1.11184636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: -0.023581061730286318\n",
      "Iteration 1, loss = 0.48674129\n",
      "Iteration 2, loss = 0.28138217\n",
      "Iteration 3, loss = 0.32125089\n",
      "Iteration 4, loss = 0.29350643\n",
      "Iteration 5, loss = 0.20418020\n",
      "Iteration 6, loss = 0.16923588\n",
      "Iteration 7, loss = 0.17058556\n",
      "Iteration 8, loss = 0.15134532\n",
      "Iteration 9, loss = 0.16555071\n",
      "Iteration 10, loss = 0.17038524\n",
      "Iteration 11, loss = 0.13379836\n",
      "Iteration 12, loss = 0.11484402\n",
      "Iteration 13, loss = 0.15272739\n",
      "Iteration 14, loss = 0.14174853\n",
      "Iteration 15, loss = 0.15093869\n",
      "Iteration 16, loss = 0.13729383\n",
      "Iteration 17, loss = 0.13140812\n",
      "Iteration 18, loss = 0.11746804\n",
      "Iteration 19, loss = 0.14284763\n",
      "Iteration 20, loss = 0.11700862\n",
      "Iteration 21, loss = 0.10010387\n",
      "Iteration 22, loss = 0.09311877\n",
      "Iteration 23, loss = 0.08239657\n",
      "Iteration 24, loss = 0.10599188\n",
      "Iteration 25, loss = 0.09919814\n",
      "Iteration 26, loss = 0.12795589\n",
      "Iteration 27, loss = 0.09137967\n",
      "Iteration 28, loss = 0.15776738\n",
      "Iteration 29, loss = 0.10380923\n",
      "Iteration 30, loss = 0.09281438\n",
      "Iteration 31, loss = 0.09046835\n",
      "Iteration 32, loss = 0.09920451\n",
      "Iteration 33, loss = 0.09071728\n",
      "Iteration 34, loss = 0.07760701\n",
      "Iteration 35, loss = 0.07964327\n",
      "Iteration 36, loss = 0.07982736\n",
      "Iteration 37, loss = 0.08163698\n",
      "Iteration 38, loss = 0.07148140\n",
      "Iteration 39, loss = 0.06632328\n",
      "Iteration 40, loss = 0.06657501\n",
      "Iteration 41, loss = 0.07526983\n",
      "Iteration 42, loss = 0.06716370\n",
      "Iteration 43, loss = 0.07064865\n",
      "Iteration 44, loss = 0.08782311\n",
      "Iteration 45, loss = 0.15570025\n",
      "Iteration 46, loss = 0.11843587\n",
      "Iteration 47, loss = 0.20439058\n",
      "Iteration 48, loss = 0.19179866\n",
      "Iteration 49, loss = 0.12319823\n",
      "Iteration 50, loss = 0.12458445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.09642435340708239\n",
      "Iteration 1, loss = 0.03836066\n",
      "Iteration 2, loss = 0.00705392\n",
      "Iteration 3, loss = 0.00232970\n",
      "Iteration 4, loss = 0.00077691\n",
      "Iteration 5, loss = 0.00059762\n",
      "Iteration 6, loss = 0.00054629\n",
      "Iteration 7, loss = 0.00045180\n",
      "Iteration 8, loss = 0.00037791\n",
      "Iteration 9, loss = 0.00035480\n",
      "Iteration 10, loss = 0.00034234\n",
      "Iteration 11, loss = 0.00032109\n",
      "Iteration 12, loss = 0.00030328\n",
      "Iteration 13, loss = 0.00028324\n",
      "Iteration 14, loss = 0.00024453\n",
      "Iteration 15, loss = 0.00021638\n",
      "Iteration 16, loss = 0.00015336\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.0020242303790038753\n",
      "Iteration 1, loss = 0.46868891\n",
      "Iteration 2, loss = 0.29501167\n",
      "Iteration 3, loss = 0.17499166\n",
      "Iteration 4, loss = 0.17808347\n",
      "Iteration 5, loss = 0.12922190\n",
      "Iteration 6, loss = 0.10385067\n",
      "Iteration 7, loss = 0.09034616\n",
      "Iteration 8, loss = 0.08334481\n",
      "Iteration 9, loss = 0.11574729\n",
      "Iteration 10, loss = 0.08316059\n",
      "Iteration 11, loss = 0.09711050\n",
      "Iteration 12, loss = 0.07607028\n",
      "Iteration 13, loss = 0.07955885\n",
      "Iteration 14, loss = 0.06393616\n",
      "Iteration 15, loss = 0.05955396\n",
      "Iteration 16, loss = 0.06254094\n",
      "Iteration 17, loss = 0.05529170\n",
      "Iteration 18, loss = 0.05842737\n",
      "Iteration 19, loss = 0.05200505\n",
      "Iteration 20, loss = 0.06001139\n",
      "Iteration 21, loss = 0.07632213\n",
      "Iteration 22, loss = 0.05416455\n",
      "Iteration 23, loss = 0.06587224\n",
      "Iteration 24, loss = 0.05902997\n",
      "Iteration 25, loss = 0.05303881\n",
      "Iteration 26, loss = 0.04912340\n",
      "Iteration 27, loss = 0.04709259\n",
      "Iteration 28, loss = 0.04883848\n",
      "Iteration 29, loss = 0.04872965\n",
      "Iteration 30, loss = 0.04627845\n",
      "Iteration 31, loss = 0.05078007\n",
      "Iteration 32, loss = 0.04644066\n",
      "Iteration 33, loss = 0.04976784\n",
      "Iteration 34, loss = 0.07641708\n",
      "Iteration 35, loss = 0.08327026\n",
      "Iteration 36, loss = 0.07054843\n",
      "Iteration 37, loss = 0.05954093\n",
      "Iteration 38, loss = 0.06564389\n",
      "Iteration 39, loss = 0.10133361\n",
      "Iteration 40, loss = 0.07806146\n",
      "Iteration 41, loss = 0.06337921\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: -0.020003744866097285\n",
      "Iteration 1, loss = 11.27186463\n",
      "Iteration 2, loss = 4.57386664\n",
      "Iteration 3, loss = 2.49746371\n",
      "Iteration 4, loss = 1.83168883\n",
      "Iteration 5, loss = 1.52912722\n",
      "Iteration 6, loss = 1.37181142\n",
      "Iteration 7, loss = 1.31215463\n",
      "Iteration 8, loss = 1.25490881\n",
      "Iteration 9, loss = 1.09375918\n",
      "Iteration 10, loss = 1.13518372\n",
      "Iteration 11, loss = 1.14291867\n",
      "Iteration 12, loss = 1.08806432\n",
      "Iteration 13, loss = 0.99550923\n",
      "Iteration 14, loss = 0.97121341\n",
      "Iteration 15, loss = 1.15560730\n",
      "Iteration 16, loss = 1.15209799\n",
      "Iteration 17, loss = 0.91558597\n",
      "Iteration 18, loss = 0.96174406\n",
      "Iteration 19, loss = 1.07122212\n",
      "Iteration 20, loss = 0.94955374\n",
      "Iteration 21, loss = 0.92162558\n",
      "Iteration 22, loss = 0.89520667\n",
      "Iteration 23, loss = 0.88267937\n",
      "Iteration 24, loss = 0.88679820\n",
      "Iteration 25, loss = 0.85748641\n",
      "Iteration 26, loss = 0.95060812\n",
      "Iteration 27, loss = 0.93201145\n",
      "Iteration 28, loss = 0.91587843\n",
      "Iteration 29, loss = 0.91243580\n",
      "Iteration 30, loss = 0.91485294\n",
      "Iteration 31, loss = 0.81331228\n",
      "Iteration 32, loss = 0.81811227\n",
      "Iteration 33, loss = 0.83986683\n",
      "Iteration 34, loss = 0.83796704\n",
      "Iteration 35, loss = 0.86798878\n",
      "Iteration 36, loss = 0.86308769\n",
      "Iteration 37, loss = 0.85970976\n",
      "Iteration 38, loss = 0.82453833\n",
      "Iteration 39, loss = 0.89491408\n",
      "Iteration 40, loss = 0.90550969\n",
      "Iteration 41, loss = 0.82116538\n",
      "Iteration 42, loss = 0.79469216\n",
      "Iteration 43, loss = 0.77907075\n",
      "Iteration 44, loss = 0.73070184\n",
      "Iteration 45, loss = 0.73766232\n",
      "Iteration 46, loss = 0.77104522\n",
      "Iteration 47, loss = 0.78312492\n",
      "Iteration 48, loss = 1.21100757\n",
      "Iteration 49, loss = 0.93053198\n",
      "Iteration 50, loss = 0.82517791\n",
      "Iteration 51, loss = 0.81456239\n",
      "Iteration 52, loss = 1.16937457\n",
      "Iteration 53, loss = 1.16074666\n",
      "Iteration 54, loss = 0.83578204\n",
      "Iteration 55, loss = 0.88760381\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 2.4716938673014894\n",
      "Iteration 1, loss = 39.57632898\n",
      "Iteration 2, loss = 9.58201008\n",
      "Iteration 3, loss = 6.42316597\n",
      "Iteration 4, loss = 4.53096214\n",
      "Iteration 5, loss = 3.42450931\n",
      "Iteration 6, loss = 3.13278852\n",
      "Iteration 7, loss = 2.56419477\n",
      "Iteration 8, loss = 2.36113817\n",
      "Iteration 9, loss = 2.32847500\n",
      "Iteration 10, loss = 2.39785175\n",
      "Iteration 11, loss = 2.23958071\n",
      "Iteration 12, loss = 2.11345022\n",
      "Iteration 13, loss = 2.09750662\n",
      "Iteration 14, loss = 1.96307103\n",
      "Iteration 15, loss = 1.88216717\n",
      "Iteration 16, loss = 1.83545834\n",
      "Iteration 17, loss = 1.83973319\n",
      "Iteration 18, loss = 1.76305262\n",
      "Iteration 19, loss = 1.89301519\n",
      "Iteration 20, loss = 1.78408094\n",
      "Iteration 21, loss = 1.81993956\n",
      "Iteration 22, loss = 1.89791364\n",
      "Iteration 23, loss = 1.84631189\n",
      "Iteration 24, loss = 1.69044938\n",
      "Iteration 25, loss = 1.68982199\n",
      "Iteration 26, loss = 1.69681692\n",
      "Iteration 27, loss = 1.66537499\n",
      "Iteration 28, loss = 1.62120663\n",
      "Iteration 29, loss = 1.64134949\n",
      "Iteration 30, loss = 1.71082001\n",
      "Iteration 31, loss = 1.68110003\n",
      "Iteration 32, loss = 1.70914351\n",
      "Iteration 33, loss = 1.84219805\n",
      "Iteration 34, loss = 2.27306142\n",
      "Iteration 35, loss = 1.93812356\n",
      "Iteration 36, loss = 1.72084804\n",
      "Iteration 37, loss = 1.77432363\n",
      "Iteration 38, loss = 1.63936867\n",
      "Iteration 39, loss = 1.69802728\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 1.589728231167962\n",
      "Iteration 1, loss = 41.88275230\n",
      "Iteration 2, loss = 12.24664983\n",
      "Iteration 3, loss = 6.26460174\n",
      "Iteration 4, loss = 4.60725633\n",
      "Iteration 5, loss = 3.98325860\n",
      "Iteration 6, loss = 3.26334883\n",
      "Iteration 7, loss = 2.91996169\n",
      "Iteration 8, loss = 2.62214515\n",
      "Iteration 9, loss = 2.43825213\n",
      "Iteration 10, loss = 2.47005993\n",
      "Iteration 11, loss = 2.24667697\n",
      "Iteration 12, loss = 2.18256630\n",
      "Iteration 13, loss = 2.10879009\n",
      "Iteration 14, loss = 2.08857234\n",
      "Iteration 15, loss = 1.95190973\n",
      "Iteration 16, loss = 1.87440327\n",
      "Iteration 17, loss = 1.77895540\n",
      "Iteration 18, loss = 1.78156886\n",
      "Iteration 19, loss = 2.35172372\n",
      "Iteration 20, loss = 2.20738580\n",
      "Iteration 21, loss = 2.12229582\n",
      "Iteration 22, loss = 1.80997923\n",
      "Iteration 23, loss = 1.70442548\n",
      "Iteration 24, loss = 1.68438770\n",
      "Iteration 25, loss = 1.71518906\n",
      "Iteration 26, loss = 1.63497968\n",
      "Iteration 27, loss = 1.65402722\n",
      "Iteration 28, loss = 1.86193362\n",
      "Iteration 29, loss = 1.71161015\n",
      "Iteration 30, loss = 1.94769171\n",
      "Iteration 31, loss = 1.67276520\n",
      "Iteration 32, loss = 1.87465086\n",
      "Iteration 33, loss = 1.97508770\n",
      "Iteration 34, loss = 1.76354321\n",
      "Iteration 35, loss = 1.63142886\n",
      "Iteration 36, loss = 1.49635853\n",
      "Iteration 37, loss = 1.41263871\n",
      "Iteration 38, loss = 1.42555423\n",
      "Iteration 39, loss = 1.50780759\n",
      "Iteration 40, loss = 1.42163100\n",
      "Iteration 41, loss = 1.58613341\n",
      "Iteration 42, loss = 1.43290212\n",
      "Iteration 43, loss = 1.36809963\n",
      "Iteration 44, loss = 1.45327005\n",
      "Iteration 45, loss = 1.40073792\n",
      "Iteration 46, loss = 1.38237349\n",
      "Iteration 47, loss = 1.48252087\n",
      "Iteration 48, loss = 1.46246196\n",
      "Iteration 49, loss = 1.53356602\n",
      "Iteration 50, loss = 1.35298361\n",
      "Iteration 51, loss = 1.33616716\n",
      "Iteration 52, loss = 1.38130685\n",
      "Iteration 53, loss = 1.50785028\n",
      "Iteration 54, loss = 1.60260261\n",
      "Iteration 55, loss = 1.45493142\n",
      "Iteration 56, loss = 1.53512716\n",
      "Iteration 57, loss = 1.45729087\n",
      "Iteration 58, loss = 1.54969446\n",
      "Iteration 59, loss = 1.48020690\n",
      "Iteration 60, loss = 1.36742208\n",
      "Iteration 61, loss = 1.30683863\n",
      "Iteration 62, loss = 1.34064101\n",
      "Iteration 63, loss = 1.24059843\n",
      "Iteration 64, loss = 1.39021779\n",
      "Iteration 65, loss = 1.34107245\n",
      "Iteration 66, loss = 1.32603906\n",
      "Iteration 67, loss = 1.26172741\n",
      "Iteration 68, loss = 1.25606871\n",
      "Iteration 69, loss = 1.34360497\n",
      "Iteration 70, loss = 1.37686830\n",
      "Iteration 71, loss = 1.50021199\n",
      "Iteration 72, loss = 1.40142467\n",
      "Iteration 73, loss = 1.26254493\n",
      "Iteration 74, loss = 1.60670407\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 10.930463478431808\n",
      "Iteration 1, loss = 20.89183632\n",
      "Iteration 2, loss = 5.62372217\n",
      "Iteration 3, loss = 3.86443956\n",
      "Iteration 4, loss = 3.24268637\n",
      "Iteration 5, loss = 2.87535466\n",
      "Iteration 6, loss = 2.64426810\n",
      "Iteration 7, loss = 2.49692495\n",
      "Iteration 8, loss = 2.33614081\n",
      "Iteration 9, loss = 2.28059184\n",
      "Iteration 10, loss = 2.14459063\n",
      "Iteration 11, loss = 1.95182948\n",
      "Iteration 12, loss = 2.06147455\n",
      "Iteration 13, loss = 2.00090211\n",
      "Iteration 14, loss = 1.85245168\n",
      "Iteration 15, loss = 1.92296910\n",
      "Iteration 16, loss = 2.11625632\n",
      "Iteration 17, loss = 1.93777634\n",
      "Iteration 18, loss = 1.90035661\n",
      "Iteration 19, loss = 2.04276682\n",
      "Iteration 20, loss = 1.81481121\n",
      "Iteration 21, loss = 1.99440012\n",
      "Iteration 22, loss = 1.84997186\n",
      "Iteration 23, loss = 1.60872810\n",
      "Iteration 24, loss = 1.74899467\n",
      "Iteration 25, loss = 1.77623199\n",
      "Iteration 26, loss = 1.79915147\n",
      "Iteration 27, loss = 1.80076741\n",
      "Iteration 28, loss = 1.71702739\n",
      "Iteration 29, loss = 1.56708551\n",
      "Iteration 30, loss = 1.66367487\n",
      "Iteration 31, loss = 1.70319806\n",
      "Iteration 32, loss = 1.71808270\n",
      "Iteration 33, loss = 1.81198615\n",
      "Iteration 34, loss = 1.54201710\n",
      "Iteration 35, loss = 1.66319520\n",
      "Iteration 36, loss = 1.64961984\n",
      "Iteration 37, loss = 1.49431064\n",
      "Iteration 38, loss = 1.58340398\n",
      "Iteration 39, loss = 1.67137240\n",
      "Iteration 40, loss = 1.64271850\n",
      "Iteration 41, loss = 1.72661691\n",
      "Iteration 42, loss = 1.71079357\n",
      "Iteration 43, loss = 1.81680335\n",
      "Iteration 44, loss = 1.89750868\n",
      "Iteration 45, loss = 1.76991505\n",
      "Iteration 46, loss = 1.71316568\n",
      "Iteration 47, loss = 1.90378921\n",
      "Iteration 48, loss = 1.84620511\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 2.815291001346899\n",
      "Iteration 1, loss = 5.85287334\n",
      "Iteration 2, loss = 2.62448137\n",
      "Iteration 3, loss = 2.04016043\n",
      "Iteration 4, loss = 1.73629947\n",
      "Iteration 5, loss = 1.47438587\n",
      "Iteration 6, loss = 1.26495038\n",
      "Iteration 7, loss = 1.19479597\n",
      "Iteration 8, loss = 1.15419713\n",
      "Iteration 9, loss = 1.20631387\n",
      "Iteration 10, loss = 1.33002703\n",
      "Iteration 11, loss = 1.03676035\n",
      "Iteration 12, loss = 0.94316643\n",
      "Iteration 13, loss = 0.90259279\n",
      "Iteration 14, loss = 1.00334558\n",
      "Iteration 15, loss = 1.02513420\n",
      "Iteration 16, loss = 0.88720890\n",
      "Iteration 17, loss = 0.85225287\n",
      "Iteration 18, loss = 0.88491822\n",
      "Iteration 19, loss = 0.89985585\n",
      "Iteration 20, loss = 0.76831887\n",
      "Iteration 21, loss = 0.75916504\n",
      "Iteration 22, loss = 1.49406053\n",
      "Iteration 23, loss = 1.18962497\n",
      "Iteration 24, loss = 1.04847328\n",
      "Iteration 25, loss = 0.86888191\n",
      "Iteration 26, loss = 0.77535541\n",
      "Iteration 27, loss = 1.15233280\n",
      "Iteration 28, loss = 0.73795106\n",
      "Iteration 29, loss = 0.78590133\n",
      "Iteration 30, loss = 0.65229249\n",
      "Iteration 31, loss = 0.73037062\n",
      "Iteration 32, loss = 1.00819622\n",
      "Iteration 33, loss = 0.82790022\n",
      "Iteration 34, loss = 0.74703521\n",
      "Iteration 35, loss = 0.81512970\n",
      "Iteration 36, loss = 0.76005292\n",
      "Iteration 37, loss = 0.90627100\n",
      "Iteration 38, loss = 0.78279269\n",
      "Iteration 39, loss = 0.80578895\n",
      "Iteration 40, loss = 0.77979283\n",
      "Iteration 41, loss = 0.72226668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: 0.9263668141707496\n",
      "Iteration 1, loss = 0.96271236\n",
      "Iteration 2, loss = 0.59690381\n",
      "Iteration 3, loss = 0.43426919\n",
      "Iteration 4, loss = 0.46965736\n",
      "Iteration 5, loss = 0.50250269\n",
      "Iteration 6, loss = 0.36045106\n",
      "Iteration 7, loss = 0.44499711\n",
      "Iteration 8, loss = 0.34813244\n",
      "Iteration 9, loss = 0.31293286\n",
      "Iteration 10, loss = 0.28829213\n",
      "Iteration 11, loss = 0.29549589\n",
      "Iteration 12, loss = 0.27773975\n",
      "Iteration 13, loss = 0.22816924\n",
      "Iteration 14, loss = 0.25070423\n",
      "Iteration 15, loss = 0.21993416\n",
      "Iteration 16, loss = 0.22112298\n",
      "Iteration 17, loss = 0.23008190\n",
      "Iteration 18, loss = 0.20760005\n",
      "Iteration 19, loss = 0.21842837\n",
      "Iteration 20, loss = 0.19379024\n",
      "Iteration 21, loss = 0.19663845\n",
      "Iteration 22, loss = 0.21041919\n",
      "Iteration 23, loss = 0.19326689\n",
      "Iteration 24, loss = 0.53041682\n",
      "Iteration 25, loss = 0.25884846\n",
      "Iteration 26, loss = 0.33071464\n",
      "Iteration 27, loss = 0.29898973\n",
      "Iteration 28, loss = 0.36663001\n",
      "Iteration 29, loss = 0.29372671\n",
      "Iteration 30, loss = 0.29301494\n",
      "Iteration 31, loss = 0.20978842\n",
      "Iteration 32, loss = 0.19043998\n",
      "Iteration 33, loss = 0.18183999\n",
      "Iteration 34, loss = 0.29863331\n",
      "Iteration 35, loss = 0.27240487\n",
      "Iteration 36, loss = 0.20272671\n",
      "Iteration 37, loss = 0.18834359\n",
      "Iteration 38, loss = 0.17622383\n",
      "Iteration 39, loss = 0.19722109\n",
      "Iteration 40, loss = 0.17509019\n",
      "Iteration 41, loss = 0.18544125\n",
      "Iteration 42, loss = 0.23876145\n",
      "Iteration 43, loss = 0.31706757\n",
      "Iteration 44, loss = 0.20853554\n",
      "Iteration 45, loss = 0.19808174\n",
      "Iteration 46, loss = 0.30645504\n",
      "Iteration 47, loss = 0.43355795\n",
      "Iteration 48, loss = 0.38078599\n",
      "Iteration 49, loss = 0.30978537\n",
      "Iteration 50, loss = 0.23354456\n",
      "Iteration 51, loss = 0.18003541\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "predicted pixel value for pixel 353: -0.0038143183664790716\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"+++ Start of data-assembly for feature-regression! +++\\n\")\n",
    "# construct the correct X_all from the columns we want\n",
    "# we use np.concatenate to combine parts of the dataset to get all-except-column 0:\n",
    "#                     exclude 0  , include 1 to the end\n",
    "\n",
    "predictedPixelValues = []\n",
    "errorList = []\n",
    "\n",
    "for i in range(0,64): # or just for i in [42] for the first iteration of this\n",
    "\n",
    "    # when i = 0: sepallen, i = 1: sepalwid, i = 2: petallen, i = 3: petalwid\n",
    "\n",
    "    X_all = np.concatenate( (A[:,0:i], A[:,i+1:]), axis=1)   \n",
    "    # X_all = np.concatenate( (A[:,0:i], A[:,i+1:]), axis=1)    # if i == 42, we should be excluding that pixel\n",
    "\n",
    "\n",
    "    y_all = A[:,i]                    # y (labels) ... is all of column 0, sepallen (sepal length) \n",
    "    #                                 # change the line above to make other columns the target (y_all)\n",
    "\n",
    "\n",
    "    # we scramble the data, to give a different TRAIN/TEST split each time...\n",
    "    # \n",
    "    indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "    # we scramble both X and y, necessarily with the same permutation\n",
    "    X_all = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "    y_all = y_all[indices]              # again...\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
    "\n",
    "    # for NNets, it's important to keep the feature values near 0, say -1. to 1. or so\n",
    "    #    This is done through the \"StandardScaler\" in scikit-learn\n",
    "    #\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    USE_SCALER = True   # this variable is important! It tracks if we need to use the scaler...\n",
    "\n",
    "    # we \"train the scaler\"  (computes the mean and standard deviation)\n",
    "    if USE_SCALER == True:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)  # Scale with the training data! ave becomes 0; stdev becomes 1\n",
    "    else:\n",
    "        # this one does no scaling!  We still create it to be consistent:\n",
    "        scaler = StandardScaler(copy=True, with_mean=False, with_std=False) # no scaling\n",
    "        scaler.fit(X_train)  # still need to fit, though it does not change...\n",
    "\n",
    "    scaler   # is now defined and ready to use...\n",
    "\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "    # Here are our scaled training and testing sets:\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train) # scale!\n",
    "    X_test_scaled = scaler.transform(X_test) # scale!\n",
    "\n",
    "    y_train_scaled = y_train  # the predicted/desired labels are not scaled\n",
    "    y_test_scaled = y_test  # not using the scaler\n",
    "\n",
    "    # reused from above - seeing the scaled data \n",
    "    # ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],None)\n",
    "\n",
    "    # reused from above - seeing the unscaled data (inverting the scaler)\n",
    "    # ascii_table(X_train_scaled[0:5,:],y_train_scaled[0:5],scaler)\n",
    "\n",
    "    # MLPRegressor predicts _floating-point_ outputs\n",
    "\n",
    "\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "    nn_regressor = MLPRegressor(hidden_layer_sizes=(6,7), \n",
    "                        max_iter=500,          # how many training epochs\n",
    "                        verbose=True,          # do we want to watch as it trains?\n",
    "                        shuffle=True,          # shuffle each epoch?\n",
    "                        random_state=None,     # use for reproducibility\n",
    "                        learning_rate_init=.1, # how much of each error to back-propagate\n",
    "                        learning_rate = 'adaptive')  # how to handle the learning_rate\n",
    "\n",
    "    nn_regressor.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "    def ascii_table_for_regressor(Xsc,y,nn,scaler):\n",
    "        \"\"\" a table including predictions using nn.predict \"\"\"\n",
    "        predictions = nn.predict(Xsc) # all predictions\n",
    "        Xpr = scaler.inverse_transform(Xsc)  # Xpr is the \"X to print\": unscaled data!\n",
    "        # measure error\n",
    "        error = 0.0\n",
    "        # printing\n",
    "        # print(f\"{'input ':>35s} ->  {'pred':^6s}  {'des.':^6s}   {'absdiff':^10s}\") \n",
    "        for i in range(len(y)):\n",
    "            pred = predictions[i]\n",
    "            desired = y[i]\n",
    "            result = abs(desired - pred)\n",
    "            error += result\n",
    "            # Xpr = Xsc   # if you'd like to see the scaled values\n",
    "            # print(f\"{Xpr[i,:]!s:>35s} ->  {pred:<+6.2f}  {desired:<+6.2f}   {result:^10.2f}\") \n",
    "        # print()\n",
    "        # print()\n",
    "        print(f\"predicted pixel value for pixel {i}: {pred}\")\n",
    "        predictedPixelValues.append(float(pred))\n",
    "        errorList.append(float(error/len(y)))\n",
    "\n",
    "        # print(\"\\n\" + \"+++++   +++++   +++++           \")\n",
    "        # print(f\"average abs diff error:   {error/len(y):<6.3f}\")\n",
    "        # print(\"+++++   +++++   +++++           \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    # let's see how it did on the test data (also the training data!)\n",
    "    #\n",
    "    ascii_table_for_regressor(X_test_scaled,\n",
    "                            y_test_scaled,\n",
    "                            nn_regressor,\n",
    "                            scaler)   # this is our own f'n, above\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "[-0.00016866739174050495, -0.0043410194856147635, -0.5681216208990802, 11.224713137849418, 11.66790157775046, 3.133304239933795, 0.19000875498128608, -0.14244194549084963, 0.00920292945461141, 0.1554184069817702, 3.81850183261704, 9.776196190943667, 15.305442147836878, 0.3903289326683899, 0.24939856922057504, 0.009899068356568607, 0.03365485256604759, 1.3822381038147384, 12.970950879367278, 6.637996873159974, 0.2958207599589147, 6.244061832345465, 4.378889659833014, 0.06604658998257662, 0.0031224860165814713, 0.2664954116404766, 13.077830395591135, 11.016628310667913, 7.242914182662861, 0.5993259078635537, 0.03441261935378091, 0.0021643543267468915, 0.0002219550193366248, 7.3362470536863285, 13.555930794455278, 15.52448979292788, 16.32945219695944, 5.063908849332606, 0.02396269970289587, -0.0010649936597877238, 0.014292517132655705, 0.20133070718205315, 15.59331106455795, 0.30129042990691884, 13.994986803833484, 8.287508612194884, 0.11427642738644436, 0.0389513019607344, 0.0006587663318047129, 0.35063114102482845, 0.534055466594589, 5.131877261997947, 6.429965854838601, -0.19474473874968934, 3.291643159564687, 0.0601102981425786, 0.005550909303732013, -0.02932601166134655, 8.709191428057478, 15.538446744897264, 14.187409384955412, 5.744753276862509, 2.3417695622181407, 0.03417785635025798]\n",
      "0.20133070718205315\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "\n",
    "print(len(predictedPixelValues))\n",
    "\n",
    "print(predictedPixelValues)\n",
    "\n",
    "print(predictedPixelValues[41])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous cell outputted predicted pixel value for each pixel. \n",
    "Prediction for pixel 42 was 0.201."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixels as 1d numpy array (row):\n",
      " [-0.00016866739174050495, -0.0043410194856147635, -0.5681216208990802, 11.224713137849418, 11.66790157775046, 3.133304239933795, 0.19000875498128608, -0.14244194549084963, 0.00920292945461141, 0.1554184069817702, 3.81850183261704, 9.776196190943667, 15.305442147836878, 0.3903289326683899, 0.24939856922057504, 0.009899068356568607, 0.03365485256604759, 1.3822381038147384, 12.970950879367278, 6.637996873159974, 0.2958207599589147, 6.244061832345465, 4.378889659833014, 0.06604658998257662, 0.0031224860165814713, 0.2664954116404766, 13.077830395591135, 11.016628310667913, 7.242914182662861, 0.5993259078635537, 0.03441261935378091, 0.0021643543267468915, 0.0002219550193366248, 7.3362470536863285, 13.555930794455278, 15.52448979292788, 16.32945219695944, 5.063908849332606, 0.02396269970289587, -0.0010649936597877238, 0.014292517132655705, 0.20133070718205315, 15.59331106455795, 0.30129042990691884, 13.994986803833484, 8.287508612194884, 0.11427642738644436, 0.0389513019607344, 0.0006587663318047129, 0.35063114102482845, 0.534055466594589, 5.131877261997947, 6.429965854838601, -0.19474473874968934, 3.291643159564687, 0.0601102981425786, 0.005550909303732013, -0.02932601166134655, 8.709191428057478, 15.538446744897264, 14.187409384955412, 5.744753276862509, 2.3417695622181407, 0.03417785635025798]\n",
      "\n",
      "pixels as 2d numpy array (image):\n",
      " [[-1.69e-04 -4.34e-03 -5.68e-01  1.12e+01  1.17e+01  3.13e+00  1.90e-01\n",
      "  -1.42e-01]\n",
      " [ 9.20e-03  1.55e-01  3.82e+00  9.78e+00  1.53e+01  3.90e-01  2.49e-01\n",
      "   9.90e-03]\n",
      " [ 3.37e-02  1.38e+00  1.30e+01  6.64e+00  2.96e-01  6.24e+00  4.38e+00\n",
      "   6.60e-02]\n",
      " [ 3.12e-03  2.66e-01  1.31e+01  1.10e+01  7.24e+00  5.99e-01  3.44e-02\n",
      "   2.16e-03]\n",
      " [ 2.22e-04  7.34e+00  1.36e+01  1.55e+01  1.63e+01  5.06e+00  2.40e-02\n",
      "  -1.06e-03]\n",
      " [ 1.43e-02  2.01e-01  1.56e+01  3.01e-01  1.40e+01  8.29e+00  1.14e-01\n",
      "   3.90e-02]\n",
      " [ 6.59e-04  3.51e-01  5.34e-01  5.13e+00  6.43e+00 -1.95e-01  3.29e+00\n",
      "   6.01e-02]\n",
      " [ 5.55e-03 -2.93e-02  8.71e+00  1.55e+01  1.42e+01  5.74e+00  2.34e+00\n",
      "   3.42e-02]]\n"
     ]
    }
   ],
   "source": [
    "# plotting heat map of each pixel prediction (will do error next)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "pixels_as_row = predictedPixelValues\n",
    "print(\"pixels as 1d numpy array (row):\\n\", pixels_as_row)\n",
    "\n",
    "pixels_as_image = np.reshape(pixels_as_row, (8,8))   # reshape into a 2d 8x8 array (image)\n",
    "print(\"\\npixels as 2d numpy array (image):\\n\", pixels_as_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create the figure, f, and the axes, ax:\n",
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "# colormap choice! Fun!   www.practicalpythonfordatascience.com/ap_seaborn_palette or seaborn.pydata.org/tutorial/color_palettes.html\n",
    "our_colormap = sns.color_palette(\"light:b\", as_cmap=True) \n",
    "\n",
    "# Draw a heatmap with the numeric values in each cell (make annot=False to remove the values)\n",
    "sns.heatmap(pixels_as_image, annot=True, linewidths=.5, ax=ax, cmap=our_colormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001103228558869269,\n",
       " 0.14579400095057835,\n",
       " 1.1996499225059367,\n",
       " 1.6417108784410612,\n",
       " 1.9407355471079408,\n",
       " 1.708830361269092,\n",
       " 0.7749759532302597,\n",
       " 0.04954335241808867,\n",
       " 0.015155002600291623,\n",
       " 1.1362495955258545,\n",
       " 2.17594195660636,\n",
       " 1.94633020264392,\n",
       " 2.2096456503135844,\n",
       " 1.677698432802143,\n",
       " 0.9258368440630341,\n",
       " 0.11813502863495344,\n",
       " 0.0022315954297990334,\n",
       " 1.303985502305635,\n",
       " 2.1489612759125896,\n",
       " 2.4453205619215472,\n",
       " 1.8351820972230097,\n",
       " 2.0497879276179116,\n",
       " 0.9297954088719598,\n",
       " 0.10310049234635725,\n",
       " 0.0033203813875704318,\n",
       " 1.1284387833720515,\n",
       " 1.8543127862726514,\n",
       " 2.2453300402933265,\n",
       " 1.7194032025144752,\n",
       " 1.8102449851975762,\n",
       " 1.0155940573838886,\n",
       " 0.01003166395818915,\n",
       " 0.002723956905510458,\n",
       " 1.0893264925081976,\n",
       " 1.8482507290742838,\n",
       " 2.1559862472751505,\n",
       " 1.5426532386256606,\n",
       " 1.6293162934189458,\n",
       " 1.0277694337605505,\n",
       " 0.0006254676615969624,\n",
       " 0.019184649173164897,\n",
       " 0.709428682065293,\n",
       " 1.7858671005049531,\n",
       " 1.7552181588944051,\n",
       " 1.6176540773302395,\n",
       " 1.7989550565537062,\n",
       " 0.9279749182950121,\n",
       " 0.05628657950099905,\n",
       " 0.008932874964551024,\n",
       " 0.5163161592517697,\n",
       " 2.2306645077277465,\n",
       " 2.0285552444867263,\n",
       " 1.8815454614658873,\n",
       " 1.5146755716400664,\n",
       " 1.1996252358410688,\n",
       " 0.25124258161828494,\n",
       " 0.0021528579689725036,\n",
       " 0.16666851337211136,\n",
       " 1.236273713488171,\n",
       " 1.5352813821984646,\n",
       " 1.8483217978467625,\n",
       " 1.5446302765063016,\n",
       " 0.9107243135798777,\n",
       " 0.25350725353329623]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now looking at error for each pixel\n",
    "\n",
    "errorList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixels as 1d numpy array (row):\n",
      " [0.001103228558869269, 0.14579400095057835, 1.1996499225059367, 1.6417108784410612, 1.9407355471079408, 1.708830361269092, 0.7749759532302597, 0.04954335241808867, 0.015155002600291623, 1.1362495955258545, 2.17594195660636, 1.94633020264392, 2.2096456503135844, 1.677698432802143, 0.9258368440630341, 0.11813502863495344, 0.0022315954297990334, 1.303985502305635, 2.1489612759125896, 2.4453205619215472, 1.8351820972230097, 2.0497879276179116, 0.9297954088719598, 0.10310049234635725, 0.0033203813875704318, 1.1284387833720515, 1.8543127862726514, 2.2453300402933265, 1.7194032025144752, 1.8102449851975762, 1.0155940573838886, 0.01003166395818915, 0.002723956905510458, 1.0893264925081976, 1.8482507290742838, 2.1559862472751505, 1.5426532386256606, 1.6293162934189458, 1.0277694337605505, 0.0006254676615969624, 0.019184649173164897, 0.709428682065293, 1.7858671005049531, 1.7552181588944051, 1.6176540773302395, 1.7989550565537062, 0.9279749182950121, 0.05628657950099905, 0.008932874964551024, 0.5163161592517697, 2.2306645077277465, 2.0285552444867263, 1.8815454614658873, 1.5146755716400664, 1.1996252358410688, 0.25124258161828494, 0.0021528579689725036, 0.16666851337211136, 1.236273713488171, 1.5352813821984646, 1.8483217978467625, 1.5446302765063016, 0.9107243135798777, 0.25350725353329623]\n",
      "\n",
      "pixels as 2d numpy array (image):\n",
      " [[1.10e-03 1.46e-01 1.20e+00 1.64e+00 1.94e+00 1.71e+00 7.75e-01 4.95e-02]\n",
      " [1.52e-02 1.14e+00 2.18e+00 1.95e+00 2.21e+00 1.68e+00 9.26e-01 1.18e-01]\n",
      " [2.23e-03 1.30e+00 2.15e+00 2.45e+00 1.84e+00 2.05e+00 9.30e-01 1.03e-01]\n",
      " [3.32e-03 1.13e+00 1.85e+00 2.25e+00 1.72e+00 1.81e+00 1.02e+00 1.00e-02]\n",
      " [2.72e-03 1.09e+00 1.85e+00 2.16e+00 1.54e+00 1.63e+00 1.03e+00 6.25e-04]\n",
      " [1.92e-02 7.09e-01 1.79e+00 1.76e+00 1.62e+00 1.80e+00 9.28e-01 5.63e-02]\n",
      " [8.93e-03 5.16e-01 2.23e+00 2.03e+00 1.88e+00 1.51e+00 1.20e+00 2.51e-01]\n",
      " [2.15e-03 1.67e-01 1.24e+00 1.54e+00 1.85e+00 1.54e+00 9.11e-01 2.54e-01]]\n"
     ]
    }
   ],
   "source": [
    "#plotting error (ie predictability map)\n",
    "\n",
    "pixels_as_row = errorList\n",
    "print(\"pixels as 1d numpy array (row):\\n\", pixels_as_row)\n",
    "\n",
    "pixels_as_image = np.reshape(pixels_as_row, (8,8))   # reshape into a 2d 8x8 array (image)\n",
    "print(\"\\npixels as 2d numpy array (image):\\n\", pixels_as_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create the figure, f, and the axes, ax:\n",
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "# colormap choice! Fun!   www.practicalpythonfordatascience.com/ap_seaborn_palette or seaborn.pydata.org/tutorial/color_palettes.html\n",
    "our_colormap = sns.color_palette(\"light:b\", as_cmap=True) \n",
    "\n",
    "# Draw a heatmap with the numeric values in each cell (make annot=False to remove the values)\n",
    "sns.heatmap(pixels_as_image, annot=True, linewidths=.5, ax=ax, cmap=our_colormap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which pixel is most predictable and which is least predictable? \n",
    "+ Lowest pixel error overall is center far right side, error 0.00063.\n",
    "+ Highest error is 2.4, row 2 col 3. \n",
    "\n",
    "Why do the answers above make sense?\n",
    "+ Greatest predictability, aka lowest error, is on the edges which makes sense, they are almost never shaded in. The central pixels switch between digits and the different training data images, so they are much harder to predict correctly consistently and therefore have higher error. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
