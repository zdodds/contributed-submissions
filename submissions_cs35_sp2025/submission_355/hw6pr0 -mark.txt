<br>

### cs35 Week6: Reading and response 

_On the Uses and Misuses of Models_   &nbsp;&nbsp; (hw6pr0.ipynb)

<hr><br>

#### Reading for hw6...     (hw6pr0.ipynb)

This week's reading has two options:
+ [Option 1] Georgia Meyer's review of <u>Escape from Model Land</u>, which addresses the troubles of over-trusting models -- and provides a path for balancing the skepticism and promise of models' _"knowledge"_  
  + [Here is the link to the original review](https://www.lse.ac.uk/DSI/Research/Blog-posts/Book-review-Escape-from-Model-Land) &nbsp;&nbsp; and &nbsp;&nbsp; [here is a local copy](https://drive.google.com/file/d/1SCuPWPyHEQ2N5eycg48pcV6Rkg8CLzSe/view?usp=drive_link), &nbsp;&nbsp; just in case <br><br>
+ [Option 2] Kate Harbath's short history of Cambridge Analytica, perhaps the most costly - and expensive - example of _modeling misuse_
  + [Here is the link to the original article](https://bipartisanpolicy.org/blog/cambridge-analytica-controversy/#) &nbsp;&nbsp; and &nbsp;&nbsp; [here is a local copy](https://drive.google.com/file/d/1k0DeDBH0EBdVfApY1O205FXMDbsashzj/view?usp=drive_link), &nbsp;&nbsp; just in case <br><br>
+ [Option 1+2] Feel free to read and respond to both (optional and ec, up to +10)


#### The prompt(s)

Using the article you choose - and your own experience - what are your thoughts on the ***trustworthiness*** of the models that modern approaches can and have created?   

Possible jumping-off points include your thoughts on ...
+ (1) the responsibility (and accountability) of the humans who **design and create** models. How should the source data affect models' scope and use?  For example, CA's models used _social media scraping_ ... <br><br>
+ (2) the responsibility (and accountability) of the humans who **deploy and use** models. To what extent does it matter what the model is a model ***of*** ? For example, CA's models were models _of people_ ... <br><br>
+ (3) an example you've encountered where an artificially-learned model was mis-deployed. This could be a non-artificially-learned model, for that matter! Was the responsibility for mis-deployment focused/individually-based? or was it diffuse/community-based? 

<br>

Alternative directions on the tensions between model-trust and human-trust are more than welcome!  

As with each week's reading, responses should be thoughtful, but need not be CS35_Participant_2: a 4-5 sentence paragraph is wonderful.

<br>
<br>
<hr>
<br>
<br>#### Reading response

(1+2)

Regarding the responsibilities of the individuals creating a model-- Meyer's article calls out a few of the really key obligations, in particular addressing incompleteness/uncertainty in the model and paying attention to who is making the model and what they hope to get out of it. Any model built from data on social media is incomplete on a number of levels: some people aren't on social media, with some demographics represented much more heavily than others; people may be very conscientious about what they share, such that their social media personality is substantially unlike their true self, social media feeds may bias someone's interactions to go in an 'unnatural' direction etc. As such, a model's use scope should take into account the scope of its input data-- in particular with modeling something as complex as human behavior, I don't find it reasonable to assume that you can linearly extrapolate or interpolate to capture the behavior of people who were not well represented in the model's training data. I'd also love to see more openness in terms of communicating a model's uncertainty upfront, without that being seen as a disqualifying thing. For example if a model's creator has found a questionable trend or bias in the output, and is unsure if that trend is legitimate or not, this is really valuable and necessary context for someone using the model to have. This puts an obligation on the model's creator to screen for suspicious or problematic outputs, try to address them, and then communicate their findings to a much greater extent then they may have done previously. 

However, this increased diligence is not meant to detract from the responsibiity of the model's end users to apply it within ethically acceptable bounds and to thoroughly think through the outputs. Ideally they will have better tools to evaluate and discover problems with their outputs. I think there needs to be a much higher bar set than there is right now for both model creators and users, with legal room for both to be considered liable for damaging outcomes. Even so, there will likely always be loopholes or gaps, like the one that Cambridge Analytica found, that can be exploited. Data protection laws have increased substantially since then, but without convincing precedent for legal or financial repercussions of misuse, I wouldn't expect data/model usage to get significantly better.