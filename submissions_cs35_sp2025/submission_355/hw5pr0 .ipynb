{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### cs35 Week5: Reading and response \n",
    "\n",
    "_More data!_   &nbsp;&nbsp; (hw5pr0.ipynb)\n",
    "\n",
    "In fact, can we _generate_ our own?!\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Reading for hw5...     (hw5pr0.ipynb)\n",
    "\n",
    "This week's reading is an [Economist article](https://www.economist.com/technology-quarterly/2020/06/11/for-ai-data-are-harder-to-come-by-than-you-think) (here is a [pdf version](https://drive.google.com/file/d/1tJC3jLjk_ZNzA1UTxREJGqzZg5pg2N24/view)) on the pitfalls and promise of the data-driven era we now inhabit.  \n",
    "\n",
    "The article takes a \"data-based\" view on the developments and concerns in AI. It's from about 5 years ago, and you'll notice that this is a _long_ time ago, AI-wise!\n",
    "\n",
    "One of the more durable ideas in this article is the possibility -- and possible importance -- of ***generating*** data to improve model-training, when available data is inequitable, inflexible, or insufficient in another way. (Amazon Go, on the other hand? [Gone.](https://foodinstitute.com/focus/rise-and-stall-of-amazon-go-illustrates-limits-of-ai/))   \n",
    "\n",
    "Using the article and your own experience, what are <font color=\"Coral\"><b>your thoughts on artificially generating data to assist AI/ML training</b></font>?  Possible jumping-off points include \n",
    "+ (1) echo-chamber effects: &nbsp; Can generated data yield more fairness -- or only reinforce existing biases?, or \n",
    "+ (2) implementation concerns: &nbsp; What process would artificially generate the data?, or \n",
    "+ (3) a specific example you've encountered, &nbsp; where a computational system generated data, but \"got things obviously wrong\" (you may have experiences several of these examples!) \n",
    "\n",
    "In the last case, the automatically-generated data may have made the world's data-landscape worse, not better.  \n",
    "\n",
    "Alternative and additional perspectives about artificially-generated data are more than welcome... &nbsp; .\n",
    "\n",
    "As with each week's reading, responses should be thoughtful, but need not be CS35_Participant_2: a 4-5 sentence paragraph is wonderful.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading response\n",
    "\n",
    "(Feel free to use this cell for your response.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)\n",
    "I think just generating supplemental data without any supervision or further inputs will reinforce existing biases and lead to less 'fair' data overall. \n",
    "However, I think there is potential for generating data that can be used to counteract some of the implicit bias present in human-bias-influenced training data--I think we're far away from being able to do this reliably, but that it is very worthwhile to pursue. For example, in the context of AI systems used in the parole process (\"Ethics of Artificial Intelligence\", pgs 12-23, https://link.springer.com/book/10.1007/978-3-031-17040-9) I think there is (in theory) an opportunity to have AI reduce the impact of racial prejudice of correctional officers. The system referenced in the case study, \"COMPAS\", took in subjective assessments of prison guards that allowed for their biases to become amplified and codified in Rodriguez's 'official' risk assessment. The resulting high risk evaluation prevented him from getting paroled. However, rather than just feed the system biased information and get inevitably biased results out, I think it's well within AI's potential to be trained to identify patterns of racial (or other) forms of prejudice, and at least be able to flag them, and maybe quantify them in the context of things like the parole risk assessmment. Maybe there is a way for the system to not amplify, but instead reduce or eliminate the effect of a parole board's racial prejudices in making their decisions. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ccb4bb6bd67730c9185e6c24c983362cd7b4575b595bfae100d8d91e48f4f1e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
