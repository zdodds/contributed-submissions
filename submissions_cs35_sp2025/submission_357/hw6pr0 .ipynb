{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### cs35 Week6: Reading and response \n",
    "\n",
    "_On the Uses and Misuses of Models_   &nbsp;&nbsp; (hw6pr0.ipynb)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Reading for hw6...     (hw6pr0.ipynb)\n",
    "\n",
    "This week's reading has two options:\n",
    "+ [Option 1] Georgia Meyer's review of <u>Escape from Model Land</u>, which addresses the troubles of over-trusting models -- and provides a path for balancing the skepticism and promise of models' _\"knowledge\"_  \n",
    "  + [Here is the link to the original review](https://www.lse.ac.uk/DSI/Research/Blog-posts/Book-review-Escape-from-Model-Land) &nbsp;&nbsp; and &nbsp;&nbsp; [here is a local copy](https://drive.google.com/file/d/1SCuPWPyHEQ2N5eycg48pcV6Rkg8CLzSe/view?usp=drive_link), &nbsp;&nbsp; just in case <br><br>\n",
    "+ [Option 2] Kate Harbath's short history of Cambridge Analytica, perhaps the most costly - and expensive - example of _modeling misuse_\n",
    "  + [Here is the link to the original article](https://bipartisanpolicy.org/blog/cambridge-analytica-controversy/#) &nbsp;&nbsp; and &nbsp;&nbsp; [here is a local copy](https://drive.google.com/file/d/1k0DeDBH0EBdVfApY1O205FXMDbsashzj/view?usp=drive_link), &nbsp;&nbsp; just in case <br><br>\n",
    "+ [Option 1+2] Feel free to read and respond to both (optional and ec, up to +10)\n",
    "\n",
    "\n",
    "#### The prompt(s)\n",
    "\n",
    "Using the article you choose - and your own experience - what are your thoughts on the ***trustworthiness*** of the models that modern approaches can and have created?   \n",
    "\n",
    "Possible jumping-off points include your thoughts on ...\n",
    "+ (1) the responsibility (and accountability) of the humans who **design and create** models. How should the source data affect models' scope and use?  For example, CA's models used _social media scraping_ ... <br><br>\n",
    "+ (2) the responsibility (and accountability) of the humans who **deploy and use** models. To what extent does it matter what the model is a model ***of*** ? For example, CA's models were models _of people_ ... <br><br>\n",
    "+ (3) an example you've encountered where an artificially-learned model was mis-deployed. This could be a non-artificially-learned model, for that matter! Was the responsibility for mis-deployment focused/individually-based? or was it diffuse/community-based? \n",
    "\n",
    "<br>\n",
    "\n",
    "Alternative directions on the tensions between model-trust and human-trust are more than welcome!  \n",
    "\n",
    "As with each week's reading, responses should be thoughtful, but need not be CS35_Participant_2: a 4-5 sentence paragraph is wonderful.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading response\n",
    "\n",
    "Feel free to use this cell for your response(s).\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<h> Responsibility of the humans who design and create models</h1>\n",
    "   \n",
    "    Creators of models have a serious responsibility because the quality and nature of their source data directly impact how trustworthy the models can be. For instance, Cambridge Analytica’s use of personal data scraped from social media was ethically problematic because the users did not explicitly consent, making their model untrustworthy​. Thus, creators should always consider ethical boundaries, transparency, and consent when collecting data to ensure their models are used responsibly.\n",
    "\n",
    "<h>Responsibility of the humans who deply and use models</h1>\n",
    "\n",
    "    Those who deploy models also carry significant accountability, especially when the models represent or impact people directly. For example, Cambridge Analytica built models predicting and manipulating human behavior, which was morally concerning because users of those models exploited people’s personal information to influence political behavior without their clear knowledge​. It matters greatly what the model represents and models involving people require heightened caution and ethical responsibility to avoid harming or manipulating individuals. \n",
    "\n",
    "<h>Example</h1>\n",
    "\n",
    "    An example of misusing models that I've personally seen involves social media algorithms, like those on TikTok or Instagram. These models are supposed to suggest content you'll like, but sometimes they end up promoting harmful or misleading things, like extreme diets or fake news. In this case, the responsibility for the mis-deployment feels diffuse, meaning it’s not just one person’s fault, but it's the fault of both the companies who create these models and us as users who accept them without questioning. I've realized that we need to be more careful and thoughtful about trusting these models blindly, especially since they have a influence on our daily lives and beliefs.\n",
    "\n",
    "<h>Additional thoughts</h1>\n",
    "\n",
    "    Balancing human trust with model trust requires transparency, continuous scrutiny, and humility about the limits of models, as Erica Thompson emphasizes in \"Escape from Model Land.\" Models should not be treated as perfectly accurate representations of reality, nor completely disregarded. Instead, they should be constantly questioned, critically analyzed, and improved with careful consideration of ethical implications and social values. \n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ccb4bb6bd67730c9185e6c24c983362cd7b4575b595bfae100d8d91e48f4f1e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
