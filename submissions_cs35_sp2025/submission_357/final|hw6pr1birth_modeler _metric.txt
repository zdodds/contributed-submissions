#
# hw6pr1births_modeler:  bday popularity modeling by month + day    
#                                                 (above/below median: 190942)
#

#
# suggestion:  
# 
# +++ copy-paste-and-alter from the week6 iris-modeling notebook into here +++
#
# libraries!
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split



# let's read in our flower data...
# 
# iris_cleaned.csv should be in this folder
# 

# read cleaned data
cleaned_filename = "births_cleaned.csv"
df_tidy = pd.read_csv(cleaned_filename)
print(f"{cleaned_filename} loaded into dataframe.")
median_births = df_tidy['births'].median()

df_tidy['above/below median'] = (df_tidy['births'] > median_births).astype(int)
# select relevant columns for modeling
df_model = df_tidy[['month', 'day', 'above/below median']]


# verify dataset and columns
print(f"using columns: {df_model.columns.tolist()}")
print(df_model.head())

# prepare data for ml
x_all = df_model[['month', 'day']].values  # features: month and day
y_all = df_model['above/below median'].values    # labels: above/below median

# split into training and testing
test_percent = 0.2
x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=test_percent, random_state=42)

print(f"training rows: {len(x_train)}, testing rows: {len(x_test)}")



#
# suggestion:  
# 
#   copy-paste-and-adapt from the iris-modeling notebook.
#
#
# this approach has the advantage of more deeply "digesting" the iris workflow ...
#
#      ... altering the parts that don't transfer, and taking the parts that do
#
from sklearn import tree
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import numpy as np

# use the existing training and test sets from data setup

# step 1: find best depth using cross-validation
best_d = 1
best_accuracy = 0.0

for d in range(1, 11):
    cv_model = tree.decisiontreeclassifier(max_depth=d)
    cv_scores = cross_val_score(cv_model, x_train, y_train, cv=5)
    avg_accuracy = cv_scores.mean()
    print(f"depth: {d:2d}  cv accuracy: {avg_accuracy:.4f}")

    if avg_accuracy > best_accuracy:
        best_accuracy = avg_accuracy
        best_d = d

best_depth = best_d
print(f"\nbest depth: {best_depth} with accuracy: {best_accuracy:.4f}")

# step 2: train final decision tree model with best depth
dtree_model = tree.decisiontreeclassifier(max_depth=best_depth)
dtree_model.fit(x_train, y_train)
print("\ntrained decision tree with max depth =", best_depth)

# step 3: predict and evaluate on test set
predicted_labels = dtree_model.predict(x_test)
num_correct = np.sum(predicted_labels == y_test)
total = len(y_test)
print(f"\nresults on test set: {num_correct} correct out of {total} total")

# step 4: display feature importances
feature_names = ['month', 'day']
importances = dtree_model.feature_importances_
print("\nfeature importances:")
for name, importance in zip(feature_names, importances):
    print(f"{name}: {importance:.4f}")

# step 5: plot the tree
plt.figure(figsize=(8, 5))
tree.plot_tree(dtree_model,
               feature_names=feature_names,
               class_names=['below', 'above'],
               filled=true,
               rounded=true,
               fontsize=10)
plt.title("decision tree for births data")
plt.show()



#
# here, use this week's iris-modeler to create, cross-validate, and model
#       bday popularity using random forests
#

# be sure to cross-validate on the two parameters: depth and number-of-estimators
#    (the number-of-estimators is the number of trees used)


from sklearn.ensemble import randomforestclassifier
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import numpy as np

# step 1: find best depth and number of trees using cross-validation
best_d = 1
best_ntrees = 50
best_accuracy = 0.0

for d in range(1, 6):
    for ntrees in [50, 150, 250]:
        rf_model = randomforestclassifier(max_depth=d,
                                          n_estimators=ntrees,
                                          max_samples=0.5,
                                          random_state=42)
        scores = cross_val_score(rf_model, x_train, y_train, cv=5)
        avg_accuracy = scores.mean()
        print(f"depth: {d}, trees: {ntrees}, cv accuracy: {avg_accuracy:.4f}")

        if avg_accuracy > best_accuracy:
            best_accuracy = avg_accuracy
            best_d = d
            best_ntrees = ntrees

print(f"\nbest depth: {best_d}, best num trees: {best_ntrees}, accuracy: {best_accuracy:.4f}")

# step 2: train random forest with best hyperparameters
rf_model = randomforestclassifier(max_depth=best_d,
                                  n_estimators=best_ntrees,
                                  max_samples=0.5,
                                  random_state=42)
rf_model.fit(x_train, y_train)
print("\ntrained random forest model")

# step 3: predict and evaluate
predicted_labels = rf_model.predict(x_test)
num_correct = np.sum(predicted_labels == y_test)
total = len(y_test)
print(f"\nresults on test set: {num_correct} correct out of {total} total")

# step 4: display feature importances
feature_names = ['month', 'day']
importances = rf_model.feature_importances_
print("\nfeature importances:")
for name, importance in zip(feature_names, importances):
    print(f"{name}: {importance:.4f}")


#
# your thoughts... / why...
#the feature importances showed that month had a slightly higher impact on predicting birthday popularity than day, but both played a role. this kind of makes sense, since some months probably have more popular birthdays (like september), while the exact day matters less overall.
# outside of “model land,” i wouldn’t trust the model too much to explain real-world causes. it’s cool to see patterns, but people aren’t born just based on months and days. it’s way more complex. so i’d say it’s helpful for spotting trends, but not for making big decisions.


#
# welcome to the world of model-building workflows!!    
#

#
# in fact, the next task on this hw is to run at least one more ml workflow:   
#          (2) digits, (ec) your-own-data, ...
#


