<br>

#### week4 ~ <i>... and into the data we dive ...</i>  &nbsp;&nbsp; (hw4pr0.ipynb)

<a href="https://docs.google.com/document/d/1OdQ-01Gp7XAk9vbZ61zM3CaTdGsZVWEvZxgu1Z0toSY/edit?tab=t.0">This week's hw page</a>

<hr><br>

#### Reading for hw4...     (hw4pr0.ipynb)

As we transition into AI/Machine Learning for the next few weeks, this week's reading takes a more <u>policy-based</u>, rather than socially-normed, view:
+ it's a NYTimes article from a few years ago -- nice, because it has a definite, wide-angle stance on regulating AI:
+ [How to Regulate Artificial Intelligence](https://www.nytimes.com/2017/09/01/opinion/artificial-intelligence-regulations-rules.html)
+ [locally-hosted pdf copy](https://drive.google.com/file/d/1EZcygQdk40J0dJTZ1vp0F20rIoQU27nH/view)

Consider the author's three proposed principles, which elaborate Isaac Asimov's "laws":
+ AIs must obey human society's laws
+ AIs must disclose themselves as such
+ AIs cannot retain confidential information (w/o permission)

Choose one (or more) of these principles with which to agree or disagree, bringing in your own experience and perspective. 

Alternative paths and balancing acts are, as always, welcome! For example -- this article was written before conversational AI was a reality. Do you feel there are differences in the appropriate principles or guidance for regulating today's AI agents and their authors?

<hr>#### Reading response

I thought this was an interesting reading and definitely agree with the author's proposed principles. However, one principle I believe is especially difficult to follow or enforce is the second, that AIs must disclose themselves as such. Lots of times, people use AI with the desire to allow them to complete tasks more quickly, and this personal disclosure is often something people are hesitant to make. In addition, lots of times disclosure is hindered by people wanting to 'take credit' themselves--kind of relating to the 'credit' discussion we had in an earlier reading, in terms of who actually deserves credit for AI work. In addition, related to what they were mentioning about AI weapons or an AI TA at Georgia Tech... I believe that at times, AI's effectiveness is diminished if its non-human identity is disclosed. An AI TA may provide as or more effective feedback than a human one, but I believe if people know it is such, its perceived helpfulness would decrease. The difficulty of trying to identify AI seems to me to make this principle one that will be impossible to be followed on a large scale as AI usage continues to grow. 