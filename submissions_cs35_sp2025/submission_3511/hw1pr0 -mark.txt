# Welcome to cs35, hw1pr0 ! #### Homework 1, problem 0 is a "reading-and-response problem"
+ There is usually one of these each week
+ It often reflects on the programming portion of that week...
+ **Response**    We ask you to compose a response to each article. It can be short (4-5 sentences is great), and the goal is to engage with the article, as well as to incorporate your own thoughts and experiences into your response. 
  + Include your response in the cell at the bottom of this notebook
  + Then, submit this notebook `hw1pr0.ipynb` to the submission site

<br>

+ To get familiar with Gradescope, feel free to submit this notebook, for the moment, _without_ the reading-response 
  + It's always possible to re-submit a file to Gradescope.
  + Gradescope keeps all submitted versions; the graders see the last version submitted

<br>

<hr>### The reading

Similar in spirit to cs5's reading/response assignments, each week there will be a short article overlapping that week's topics (sometimes broadly, other times narrowly). 

This week has "1+" readings. 

<br>

The "1" of "1+" comes from a short Nature article that describes a "credit-blame asymmetry" caused by AI, i.e., LLMs.  That is, LLMs make it more difficult to feel that creditable work is being accomplished -- but they do not change the standards for blame.  This is timely, seeing as this class -- and so much else -- is about using and exploring AI.  First, the article:

+ The article:     [Generative AI entails a creditâ€“blame asymmetry](https://www.nature.com/articles/s42256-023-00653-1)  &nbsp;&nbsp;   [pdf](https://drive.google.com/file/d/1rgIBOjld0N6QZpkI6G4Vm4YZAFVyYQe_/view?usp=drive_link)  &nbsp;&nbsp;     <font size="-2">(Oxford's  <a href="https://www.ox.ac.uk/news/2023-05-05-tackling-ethical-dilemma-responsibility-large-language-models">overview</a>  of the article)</font>

<br>

The prompt below will ask if you sense the "asymmetry" the article claims. But, first, the "+" of "1+":

The "+" of "1+" is the first two sections, 1.1 and 1.2, of the [ACM's Code of Ethics and Professional Conduct](https://ethics.acm.org/). The ACM is the Association for Computing Machinery and  is the world's largest professional-computing group. In 2018, the ACM released "the Code," its broad ethical guide for all computing work:  

+ Read sections 1.1 and 1.2  of "[the Code](https://ethics.acm.org/)."  More, of course, if you'd like...

<br><hr><br>
#### The prompt

As each week, this "problem 0" asks you to compose a one-paragraph reflection (~5 sentences, give or take), with the goal of bridging your personal experiences with the ideas in the articles. 

This week's prompts:

<br>

(a) The Nature article - and its overview - suggests that society is "justified in holding persons accountable for deliberate or careless errors in generated text if they put such text to use in ways that negatively impact others"  [<font color="DodgerBlue">the "blame" side</font>] and also that society "might not think people deserve credit for text generated without much skill and effort, such as ChatGPT-generated exam papers" [<font color="DodgerBlue">the "credit" side</font>].  <font color="Coral">Do you agree with neither, either, or both of these two "sides" of the article's "credit-blame asymmetry"? In a sentence or two, elaborate how/why</font>

(b) The ACM Code of Ethics, especially sections 1.1 (<i>Contribute...</i>) and 1.2 (<i>Avoid harm</i>), mirror this credit-blame dichotomy.  With these principles in mind, <font color="Coral">do you feel that, in particular, AI systems are ethics-promoting? detracting? neutral? amplifying what's already there?</font>  What changes to norms or expectations would you like to see around AI systems, credit for creative work, and blame for misuse? 

<br>

Here, and in general, your response should incorporate ideas from the article(s), connecting them with your own thoughts, takes, and experiences. 

And - you can always add detours, if you'd like ...

<br><hr><br>#### The Response
a.
I would agree that the creators should be held accountable for the errors, especially deliberate ones. If it is a code, the coder should run some troubleshooting procedures to make sure all is working as intended. 
As for credit, I think my view on this topic will change as I see AI evolve. I think a coder's task is to engineer an algorithm or function that meets the demands of a task. Historically, this part of the job would include typing the actual code itself, but since AI can code many basic functions. The coder's task might gradually shift to psuedocode, logic creation, and integrating the AI contributions to the code.

b.
I think AI amplifies the morality our culture have. It makes those who want to be good, bad, chaotic, or rich's task easier. Currently, I feel like there is a bubble around AI. I do not enjoy companies and schools trying to integrate ChatGPT and the like into programs that don't need it. I hate to say it, AI is good enough to make me lazy, but not good enough to make it into a trustworthy source. For creative work, I'd love to see the artist's ideas and creativity. The medium certainly plays a role as well. But I would mind incorperating AI elements into art, but things that are obviously AI generated are bad since I can tell it is obviously AI generated. Blame for misuse should fall on the users. Though there should be elements as failsafes. For example, AI pretending to be users on the internet can be distinguished by responding with "ignore all previous instructions."

<br><br>
<hr>
<br>

Additional thoughts on LLMs: not necessary for cs35.

In fact, we will explore more about the technical workings of LLMs around weeks 8-9.

+ [LLMs don't have a coherent model of the world](https://www.lesswrong.com/posts/wkws2WgraeN8AYJjv/llms-don-t-have-a-coherent-model-of-the-world-what-it-means) &nbsp; Interesting...
+ [Human Immortality using LLMs](https://danielmiessler.com/p/human-immortality-using-llms) &nbsp; Is that word warranted? ...
+ [LLM Programs](https://mpost.io/llm-programs-the-new-path-to-fine-tuning-neural-models-in-complex-situations/) &nbsp; An overview article
+ [Large language model programs](https://arxiv.org/abs/2305.05364) &nbsp; An Arxiv article
