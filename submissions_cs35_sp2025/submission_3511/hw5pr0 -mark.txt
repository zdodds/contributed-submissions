<br>

### cs35 Week5: Reading and response 

_More data!_   &nbsp;&nbsp; (hw5pr0.ipynb)

In fact, can we _generate_ our own?!

<hr><br>

#### Reading for hw5...     (hw5pr0.ipynb)

This week's reading is an [Economist article](https://www.economist.com/technology-quarterly/2020/06/11/for-ai-data-are-harder-to-come-by-than-you-think) (here is a [pdf version](https://drive.google.com/file/d/1tJC3jLjk_ZNzA1UTxREJGqzZg5pg2N24/view)) on the pitfalls and promise of the data-driven era we now inhabit.  

The article takes a "data-based" view on the developments and concerns in AI. It's from about 5 years ago, and you'll notice that this is a _long_ time ago, AI-wise!

One of the more durable ideas in this article is the possibility -- and possible importance -- of ***generating*** data to improve model-training, when available data is inequitable, inflexible, or insufficient in another way. (Amazon Go, on the other hand? [Gone.](https://foodinstitute.com/focus/rise-and-stall-of-amazon-go-illustrates-limits-of-ai/))   

Using the article and your own experience, what are <font color="Coral"><b>your thoughts on artificially generating data to assist AI/ML training</b></font>?  Possible jumping-off points include 
+ (1) echo-chamber effects: &nbsp; Can generated data yield more fairness -- or only reinforce existing biases?, or 
+ (2) implementation concerns: &nbsp; What process would artificially generate the data?, or 
+ (3) a specific example you've encountered, &nbsp; where a computational system generated data, but "got things obviously wrong" (you may have experiences several of these examples!) 

In the last case, the automatically-generated data may have made the world's data-landscape worse, not better.  

Alternative and additional perspectives about artificially-generated data are more than welcome... &nbsp; .

As with each week's reading, responses should be thoughtful, but need not be CS35_Participant_2: a 4-5 sentence paragraph is wonderful.

<hr>1. 
I think data generation algorithms can lead to both fairness and reinforcing bias. 

Let's the example of a criminal court. An data-based AI can eliminate the element of the judge falling to sweet words (or the lackthereof). One may consider this "fair" because there is a program that could make suggestions without the influence of emotions. The court may deliver an unfair sentence if the defendent is either very articulate or very not so. We enter some moral gray area here because emotions are an important part of the human existance, and one may argue that using an emotionless entity to judge humans is not "fair." In short, programs can eliminate the emotional aspect of court, whether is this is good/fair/moral or not varies by person.

Then again, I think the article brings up an interesting point of the AI can subtly reinforce our biases through information like zip code, education level, income, etc. This may be quite tricky, but I don't think is insolvable. I am under the impression that breaking down an AI algorithm is harder than breaking down the algorithm in, for example, a digital watch. However, I suggest just to simply remove all these seemingly irrelevant information from the algorithm, but I'm sure this approach also might have its own issues as well.